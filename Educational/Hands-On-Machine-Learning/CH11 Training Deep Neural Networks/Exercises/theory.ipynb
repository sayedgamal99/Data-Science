{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Chapter 11 Exercises: Training Deep Neural Networks\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 1- What is the problem that Glorot initialization and He initialization aim to fix?\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The basic initialization was **Standard Normal** distributed random values. which was a reason of vanishing/exploding gradients and using sigmoid activation function make it worse because it saturate at big, small values and it has mean of 0.5 not 0.\n",
    "\n",
    "The output variance of each layer was bigger than input variance, going forward through the network.. producing very high variance (many small and big values appears in output) which affect the learning process and make it making no improvements each forward propagation.\n",
    "\n",
    "This happens at top layers, the derivative for this horizontal line close to 0 which getting smaller and smaller going back at backpropagation.. which make no difference in weights at the end.\n",
    "\n",
    "**Glorot Initialization** and **He Initialization** aims to fix this problem through keeping the variance of outputs equal the variance of the inputs at each layer.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<br>\n",
    "<br>\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 2- Is it OK to initialize all the weights to the same value as long as that value is selected randomly using He initialization?\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "No, it will make all units works in the same way leading to duplicated units in each layer without learning different features. This will lead to very poor performance. And will not break symmetry and backpropagation will not able to break it.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<br>\n",
    "<br>\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 3- Is it OK to initialize the bias terms to 0?\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Yes, it will be learned easily and will not be a reason for symmetry problem\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<br>\n",
    "<br>\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 4- In which cases would you want to use each of the activation functions we discussed in this chapter?\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "I will use in top layer classification activation function if needed such as sigmoid or softmax.\n",
    "\n",
    "In hidden layers I would use **RELU** for simple tasks, **Swish** and **Mish** good default for more complex tasks, I will use Mish if i don't care about latency. If I do, I will use **Leaky RELU** as it improved from simple relu and give slightly better results.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<br>\n",
    "<br>\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 5- What may happen if you set the momentum hyperparameter too close to 1 (e.g., 0.99999) when using an SGD optimizer?\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "A high $\\beta$ will give alot of attention for past gradients, make it less sensitive to present gradients which may cause slower convergence, bit oscillations about optimal\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<br>\n",
    "<br>\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 6- Name three ways you can produce a sparse model\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Sparse model means most weights are set to 0, so we can use $l_1$ Regularization, manually set tiny weights to zeros.\n",
    "\n",
    "Tensorflow model Optimization ToolKit, This will break self-normalization.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<br>\n",
    "<br>\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 7- Does dropout slow down training? Does it slow down inference (i.e., making predictions on new instances)? What about MC dropout?\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "It slows down the model a bit because it increases the variance of gradients, and in inference it doesn't affect also as the model units are the same but except all of them turned on.\n",
    "\n",
    "MC dropout slows down the inference process as we increase the number of inferences process.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<br>\n",
    "<br>\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<br>\n",
    "<br>\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
