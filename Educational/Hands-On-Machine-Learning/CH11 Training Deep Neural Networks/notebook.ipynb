{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# CH11 Training Deep Neural Networks\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'2.10.1'"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tf.__version__"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Activations and he initializations\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "dense = tf.keras.layers.Dense(\n",
    "    50, activation='relu', kernel_initializer='he_normal')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'truncated_normal'"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dense.kernel_initializer.distribution"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "he_avg_init = tf.keras.initializers.VarianceScaling(scale=2., mode=\"fan_avg\",\n",
    "                                                    distribution=\"uniform\")\n",
    "dense = tf.keras.layers.Dense(50, activation=\"sigmoid\",\n",
    "                              kernel_initializer=he_avg_init)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "leaky relu\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "leaky_relu = tf.keras.layers.LeakyReLU(alpha=.2)\n",
    "dense = tf.keras.layers.Dense(\n",
    "    50, activation=leaky_relu, kernel_initializer='he_normal')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "elu\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "dense = tf.keras.layers.Dense(\n",
    "    50, activation='elu', kernel_initializer='he_normal')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "`SELU` , but it has few considerations:\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "• The input features must be standardized: mean 0 and standard deviation 1.\n",
    "\n",
    "• Every hidden layer’s weights must be initialized using LeCun normal initializa\n",
    "tion. In Keras, this means setting kernel_initializer=\"lecun_normal\".\n",
    "\n",
    "• The self-normalizing property is only guaranteed with plain MLPs.\n",
    "\n",
    "• You cannot use regularization techniques like ℓ1\n",
    "or ℓ2\n",
    "regularization, max-norm,\n",
    "batch-norm, or regular dropout.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "dense = tf.keras.layers.Dense(\n",
    "    50, activation='selu', kernel_initializer='lecun_normal')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "an example of a self-regularized network using SELU\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "tf.random.set_seed(42)\n",
    "model = tf.keras.Sequential()\n",
    "model.add(tf.keras.layers.Flatten(input_shape=[28, 28]))\n",
    "for i in range(100):\n",
    "    model.add(tf.keras.layers.Dense(100, activation='selu',\n",
    "              kernel_initializer='lecun_normal'))\n",
    "model.add(tf.keras.layers.Dense(10, activation='softmax'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'model' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[3], line 1\u001b[0m\n\u001b[1;32m----> 1\u001b[0m \u001b[43mmodel\u001b[49m\u001b[38;5;241m.\u001b[39mcompile(loss\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124msparse_categorical_crossentropy\u001b[39m\u001b[38;5;124m'\u001b[39m, optimizer\u001b[38;5;241m=\u001b[39mtf\u001b[38;5;241m.\u001b[39mkeras\u001b[38;5;241m.\u001b[39moptimizers\u001b[38;5;241m.\u001b[39mSGD(\n\u001b[0;32m      2\u001b[0m     learning_rate\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m0.001\u001b[39m), metrics\u001b[38;5;241m=\u001b[39m[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124maccuracy\u001b[39m\u001b[38;5;124m'\u001b[39m])\n\u001b[0;32m      3\u001b[0m fashion_mnist \u001b[38;5;241m=\u001b[39m tf\u001b[38;5;241m.\u001b[39mkeras\u001b[38;5;241m.\u001b[39mdatasets\u001b[38;5;241m.\u001b[39mfashion_mnist\u001b[38;5;241m.\u001b[39mload_data()\n\u001b[0;32m      4\u001b[0m (X_train_full, y_train_full), (X_test, y_test) \u001b[38;5;241m=\u001b[39m fashion_mnist\n",
      "\u001b[1;31mNameError\u001b[0m: name 'model' is not defined"
     ]
    }
   ],
   "source": [
    "model.compile(loss='sparse_categorical_crossentropy', optimizer=tf.keras.optimizers.SGD(\n",
    "    learning_rate=0.001), metrics=['accuracy'])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "fashion_mnist = tf.keras.datasets.fashion_mnist.load_data()\n",
    "(X_train_full, y_train_full), (X_test, y_test) = fashion_mnist\n",
    "X_train, y_train = X_train_full[:-5000], y_train_full[:-5000]\n",
    "X_valid, y_valid = X_train_full[-5000:], y_train_full[-5000:]\n",
    "X_train, X_valid, X_test = X_train / 255, X_valid / 255, X_test / 255\n",
    "\n",
    "class_names = [\"T-shirt/top\", \"Trouser\", \"Pullover\", \"Dress\", \"Coat\",\n",
    "               \"Sandal\", \"Shirt\", \"Sneaker\", \"Bag\", \"Ankle boot\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "pixel_means = X_train.mean(axis=0, keepdims=True)\n",
    "pixel_stds = X_train.std(axis=0, keepdims=True)\n",
    "X_train_scaled = (X_train - pixel_means)/pixel_stds\n",
    "X_valid_scaled = (X_valid - pixel_means)/pixel_stds\n",
    "X_test_scaled = (X_test - pixel_means)/pixel_stds"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/5\n",
      "1719/1719 [==============================] - 36s 18ms/step - loss: 1.0050 - accuracy: 0.6275 - val_loss: 0.7212 - val_accuracy: 0.7350\n",
      "Epoch 2/5\n",
      "1719/1719 [==============================] - 33s 19ms/step - loss: 0.6421 - accuracy: 0.7656 - val_loss: 0.5779 - val_accuracy: 0.7892\n",
      "Epoch 3/5\n",
      "1719/1719 [==============================] - 32s 19ms/step - loss: 0.5580 - accuracy: 0.7990 - val_loss: 0.5336 - val_accuracy: 0.8102\n",
      "Epoch 4/5\n",
      "1719/1719 [==============================] - 30s 18ms/step - loss: 0.5159 - accuracy: 0.8169 - val_loss: 0.4961 - val_accuracy: 0.8254\n",
      "Epoch 5/5\n",
      "1719/1719 [==============================] - 33s 19ms/step - loss: 0.4952 - accuracy: 0.8280 - val_loss: 0.4968 - val_accuracy: 0.8308\n"
     ]
    }
   ],
   "source": [
    "history = model.fit(X_train_scaled, y_train, epochs=5,\n",
    "                    validation_data=(X_valid_scaled, y_valid))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The network managed to learn, despite how deep it is. Now look at what happens if we try to use the ReLU activation function instead:\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = tf.keras.Sequential()\n",
    "model.add(tf.keras.layers.Flatten(input_shape=[28, 28]))\n",
    "for layer in range(100):\n",
    "    model.add(tf.keras.layers.Dense(100, activation=\"relu\",\n",
    "                                    kernel_initializer=\"he_normal\"))\n",
    "model.add(tf.keras.layers.Dense(10, activation=\"softmax\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.compile(loss=\"sparse_categorical_crossentropy\",\n",
    "              optimizer=tf.keras.optimizers.SGD(learning_rate=0.001),\n",
    "              metrics=[\"accuracy\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/5\n",
      "1719/1719 [==============================] - 34s 18ms/step - loss: 1.9997 - accuracy: 0.2147 - val_loss: 1.7337 - val_accuracy: 0.3048\n",
      "Epoch 2/5\n",
      "1719/1719 [==============================] - 30s 17ms/step - loss: 1.2544 - accuracy: 0.4702 - val_loss: 1.3963 - val_accuracy: 0.3952\n",
      "Epoch 3/5\n",
      "1719/1719 [==============================] - 31s 18ms/step - loss: 1.0043 - accuracy: 0.5888 - val_loss: 1.6453 - val_accuracy: 0.3570\n",
      "Epoch 4/5\n",
      "1719/1719 [==============================] - 31s 18ms/step - loss: 0.9494 - accuracy: 0.6116 - val_loss: 0.7680 - val_accuracy: 0.7068\n",
      "Epoch 5/5\n",
      "1719/1719 [==============================] - 31s 18ms/step - loss: 0.8271 - accuracy: 0.6733 - val_loss: 0.7861 - val_accuracy: 0.6828\n"
     ]
    }
   ],
   "source": [
    "history = model.fit(X_train_scaled, y_train, epochs=5,\n",
    "                    validation_data=(X_valid_scaled, y_valid))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Not great at all, we suffered from the vanishing/exploding gradients problem.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tf.keras.backend.clear_session()\n",
    "tf.random.set_seed(42)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "GELU, Swish and Mish\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "def mish(x):\n",
    "    return x * tf.math.tanh(tf.math.softplus(x))\n",
    "\n",
    "\n",
    "# Using GELU activation\n",
    "dense_gelu = tf.keras.layers.Dense(\n",
    "    50, activation=tf.keras.activations.gelu, kernel_initializer='he_normal')\n",
    "\n",
    "# Using built-in Swish activation\n",
    "dense_swish = tf.keras.layers.Dense(\n",
    "    50, activation='swish', kernel_initializer='he_normal')\n",
    "\n",
    "# Using Mish activation\n",
    "dense_mish = tf.keras.layers.Dense(\n",
    "    50, activation=mish, kernel_initializer='he_normal')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Batch Normalization\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "tf.keras.backend.clear_session()\n",
    "tf.random.set_seed(42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tensorflow.keras.layers import Flatten, Dense, BatchNormalization\n",
    "\n",
    "model = tf.keras.Sequential([\n",
    "    Flatten(input_shape=[28, 28]),\n",
    "    BatchNormalization(),\n",
    "    Dense(300, activation='relu', kernel_initializer='he_normal'),\n",
    "    BatchNormalization(),\n",
    "    Dense(100, activation='relu', kernel_initializer='he_normal'),\n",
    "    BatchNormalization(),\n",
    "    Dense(10, activation='softmax'),\n",
    "\n",
    "])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"sequential_2\"\n",
      "_________________________________________________________________\n",
      " Layer (type)                Output Shape              Param #   \n",
      "=================================================================\n",
      " flatten_2 (Flatten)         (None, 784)               0         \n",
      "                                                                 \n",
      " batch_normalization (BatchN  (None, 784)              3136      \n",
      " ormalization)                                                   \n",
      "                                                                 \n",
      " dense (Dense)               (None, 300)               235500    \n",
      "                                                                 \n",
      " batch_normalization_1 (Batc  (None, 300)              1200      \n",
      " hNormalization)                                                 \n",
      "                                                                 \n",
      " dense_1 (Dense)             (None, 100)               30100     \n",
      "                                                                 \n",
      " batch_normalization_2 (Batc  (None, 100)              400       \n",
      " hNormalization)                                                 \n",
      "                                                                 \n",
      " dense_2 (Dense)             (None, 10)                1010      \n",
      "                                                                 \n",
      "=================================================================\n",
      "Total params: 271,346\n",
      "Trainable params: 268,978\n",
      "Non-trainable params: 2,368\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('batch_normalization/gamma:0', True),\n",
       " ('batch_normalization/beta:0', True),\n",
       " ('batch_normalization/moving_mean:0', False),\n",
       " ('batch_normalization/moving_variance:0', False)]"
      ]
     },
     "execution_count": 33,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "[(var.name, var.trainable) for var in model.layers[1].variables]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/2\n",
      "1719/1719 [==============================] - 10s 5ms/step - loss: 0.5548 - accuracy: 0.8077 - val_loss: 0.3976 - val_accuracy: 0.8580\n",
      "Epoch 2/2\n",
      "1719/1719 [==============================] - 9s 5ms/step - loss: 0.4069 - accuracy: 0.8550 - val_loss: 0.3608 - val_accuracy: 0.8686\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.callbacks.History at 0x1d70fb05db0>"
      ]
     },
     "execution_count": 34,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.compile(loss=\"sparse_categorical_crossentropy\", optimizer=\"sgd\",\n",
    "              metrics=\"accuracy\")\n",
    "model.fit(X_train, y_train, epochs=2, validation_data=(X_valid, y_valid))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Sometimes applying BN before the activation function works better (there's a debate on this topic). Moreover, the layer before a BatchNormalization layer does not need to have bias terms, since the BatchNormalization layer some as well, it would be a waste of parameters, so you can set use_bias=False when creating those layers:\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1719/1719 [==============================] - 9s 5ms/step - loss: 0.6099 - accuracy: 0.7979 - val_loss: 0.4347 - val_accuracy: 0.8442\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.callbacks.History at 0x1d70fb04c40>"
      ]
     },
     "execution_count": 35,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from tensorflow.keras import Sequential\n",
    "from tensorflow.keras.layers import Activation\n",
    "tf.keras.backend.clear_session()\n",
    "tf.random.set_seed(42)\n",
    "\n",
    "\n",
    "model = Sequential([\n",
    "    Flatten(input_shape=[28, 28]),\n",
    "    Dense(300, kernel_initializer='he_normal'),\n",
    "    BatchNormalization(),\n",
    "    Activation('relu'),\n",
    "    Dense(100, kernel_initializer='he_normal'),\n",
    "    BatchNormalization(),\n",
    "    Activation('relu'),\n",
    "    Dense(10),\n",
    "    Activation('softmax')\n",
    "])\n",
    "\n",
    "model.compile(loss=\"sparse_categorical_crossentropy\", optimizer=\"sgd\",\n",
    "              metrics=\"accuracy\")\n",
    "model.fit(X_train, y_train, epochs=1, validation_data=(X_valid, y_valid))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Gradient Clipping\n",
    "\n",
    "to avoid gradients explosion\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "All tf.keras.optimizers accept clipnorm or clipvalue arguments:\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "optim = tf.keras.optimizers.SGD(clipvalue=1.0)\n",
    "model.compile(loss=\"sparse_categorical_crossentropy\", optimizer=optim)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [],
   "source": [
    "optim = tf.keras.optimizers.SGD(clipnorm=1.0)\n",
    "model.compile(loss=\"sparse_categorical_crossentropy\", optimizer=optim)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Reusing Pre-trained Layers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "split Fashion MNIST into tasks A and B, then train and save model A to \"my_model_A\"."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/20\n",
      "1376/1376 [==============================] - 6s 3ms/step - loss: 1.0987 - accuracy: 0.6467 - val_loss: 0.6847 - val_accuracy: 0.7691\n",
      "Epoch 2/20\n",
      "1376/1376 [==============================] - 4s 3ms/step - loss: 0.6033 - accuracy: 0.7994 - val_loss: 0.5222 - val_accuracy: 0.8245\n",
      "Epoch 3/20\n",
      "1376/1376 [==============================] - 4s 3ms/step - loss: 0.4917 - accuracy: 0.8429 - val_loss: 0.4489 - val_accuracy: 0.8491\n",
      "Epoch 4/20\n",
      "1376/1376 [==============================] - 4s 3ms/step - loss: 0.4316 - accuracy: 0.8615 - val_loss: 0.4049 - val_accuracy: 0.8611\n",
      "Epoch 5/20\n",
      "1376/1376 [==============================] - 4s 3ms/step - loss: 0.3926 - accuracy: 0.8731 - val_loss: 0.3749 - val_accuracy: 0.8732\n",
      "Epoch 6/20\n",
      "1376/1376 [==============================] - 4s 3ms/step - loss: 0.3649 - accuracy: 0.8803 - val_loss: 0.3534 - val_accuracy: 0.8757\n",
      "Epoch 7/20\n",
      "1376/1376 [==============================] - 4s 3ms/step - loss: 0.3452 - accuracy: 0.8847 - val_loss: 0.3380 - val_accuracy: 0.8792\n",
      "Epoch 8/20\n",
      "1376/1376 [==============================] - 5s 4ms/step - loss: 0.3296 - accuracy: 0.8887 - val_loss: 0.3296 - val_accuracy: 0.8819\n",
      "Epoch 9/20\n",
      "1376/1376 [==============================] - 5s 4ms/step - loss: 0.3176 - accuracy: 0.8925 - val_loss: 0.3167 - val_accuracy: 0.8874\n",
      "Epoch 10/20\n",
      "1376/1376 [==============================] - 5s 4ms/step - loss: 0.3073 - accuracy: 0.8961 - val_loss: 0.3117 - val_accuracy: 0.8879\n",
      "Epoch 11/20\n",
      "1376/1376 [==============================] - 5s 4ms/step - loss: 0.2986 - accuracy: 0.8988 - val_loss: 0.3041 - val_accuracy: 0.8917\n",
      "Epoch 12/20\n",
      "1376/1376 [==============================] - 4s 3ms/step - loss: 0.2915 - accuracy: 0.9008 - val_loss: 0.3028 - val_accuracy: 0.8907\n",
      "Epoch 13/20\n",
      "1376/1376 [==============================] - 4s 3ms/step - loss: 0.2856 - accuracy: 0.9028 - val_loss: 0.2937 - val_accuracy: 0.8970\n",
      "Epoch 14/20\n",
      "1376/1376 [==============================] - 4s 3ms/step - loss: 0.2795 - accuracy: 0.9045 - val_loss: 0.2875 - val_accuracy: 0.8962\n",
      "Epoch 15/20\n",
      "1376/1376 [==============================] - 4s 3ms/step - loss: 0.2744 - accuracy: 0.9062 - val_loss: 0.2830 - val_accuracy: 0.9012\n",
      "Epoch 16/20\n",
      "1376/1376 [==============================] - 4s 3ms/step - loss: 0.2696 - accuracy: 0.9082 - val_loss: 0.2807 - val_accuracy: 0.9020\n",
      "Epoch 17/20\n",
      "1376/1376 [==============================] - 4s 3ms/step - loss: 0.2654 - accuracy: 0.9082 - val_loss: 0.2766 - val_accuracy: 0.9030\n",
      "Epoch 18/20\n",
      "1376/1376 [==============================] - 4s 3ms/step - loss: 0.2609 - accuracy: 0.9107 - val_loss: 0.2737 - val_accuracy: 0.9032\n",
      "Epoch 19/20\n",
      "1376/1376 [==============================] - 4s 3ms/step - loss: 0.2574 - accuracy: 0.9120 - val_loss: 0.2717 - val_accuracy: 0.9050\n",
      "Epoch 20/20\n",
      "1376/1376 [==============================] - 4s 3ms/step - loss: 0.2538 - accuracy: 0.9131 - val_loss: 0.2720 - val_accuracy: 0.9035\n",
      "INFO:tensorflow:Assets written to: my_model_A\\assets\n"
     ]
    }
   ],
   "source": [
    "pos_class_id = class_names.index(\"Pullover\")\n",
    "neg_class_id = class_names.index(\"T-shirt/top\")\n",
    "\n",
    "\n",
    "def split_dataset(X, y):\n",
    "    y_for_B = (y == pos_class_id) | (y == neg_class_id)\n",
    "    y_A = y[~y_for_B]\n",
    "    y_B = (y[y_for_B] == pos_class_id).astype(np.float32)\n",
    "    old_class_ids = list(set(range(10)) - set([neg_class_id, pos_class_id]))\n",
    "    for old_class_id, new_class_id in zip(old_class_ids, range(8)):\n",
    "        y_A[y_A == old_class_id] = new_class_id  # reorder class ids for A\n",
    "    return ((X[~y_for_B], y_A), (X[y_for_B], y_B))\n",
    "\n",
    "\n",
    "(X_train_A, y_train_A), (X_train_B, y_train_B) = split_dataset(X_train, y_train)\n",
    "(X_valid_A, y_valid_A), (X_valid_B, y_valid_B) = split_dataset(X_valid, y_valid)\n",
    "(X_test_A, y_test_A), (X_test_B, y_test_B) = split_dataset(X_test, y_test)\n",
    "X_train_B = X_train_B[:200]\n",
    "y_train_B = y_train_B[:200]\n",
    "\n",
    "tf.random.set_seed(42)\n",
    "\n",
    "model_A = tf.keras.Sequential([\n",
    "    tf.keras.layers.Flatten(input_shape=[28, 28]),\n",
    "    tf.keras.layers.Dense(100, activation=\"relu\",\n",
    "                          kernel_initializer=\"he_normal\"),\n",
    "    tf.keras.layers.Dense(100, activation=\"relu\",\n",
    "                          kernel_initializer=\"he_normal\"),\n",
    "    tf.keras.layers.Dense(100, activation=\"relu\",\n",
    "                          kernel_initializer=\"he_normal\"),\n",
    "    tf.keras.layers.Dense(8, activation=\"softmax\")\n",
    "])\n",
    "\n",
    "model_A.compile(loss=\"sparse_categorical_crossentropy\",\n",
    "                optimizer=tf.keras.optimizers.SGD(learning_rate=0.001),\n",
    "                metrics=[\"accuracy\"])\n",
    "history = model_A.fit(X_train_A, y_train_A, epochs=20,\n",
    "                      validation_data=(X_valid_A, y_valid_A))\n",
    "model_A.save(\"my_model_A\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/20\n",
      "7/7 [==============================] - 1s 38ms/step - loss: 0.7114 - accuracy: 0.5400 - val_loss: 0.6988 - val_accuracy: 0.5440\n",
      "Epoch 2/20\n",
      "7/7 [==============================] - 0s 22ms/step - loss: 0.6740 - accuracy: 0.5900 - val_loss: 0.6658 - val_accuracy: 0.6231\n",
      "Epoch 3/20\n",
      "7/7 [==============================] - 0s 19ms/step - loss: 0.6447 - accuracy: 0.6800 - val_loss: 0.6405 - val_accuracy: 0.6756\n",
      "Epoch 4/20\n",
      "7/7 [==============================] - 0s 18ms/step - loss: 0.6185 - accuracy: 0.7300 - val_loss: 0.6170 - val_accuracy: 0.7438\n",
      "Epoch 5/20\n",
      "7/7 [==============================] - 0s 18ms/step - loss: 0.5959 - accuracy: 0.7950 - val_loss: 0.5939 - val_accuracy: 0.8160\n",
      "Epoch 6/20\n",
      "7/7 [==============================] - 0s 17ms/step - loss: 0.5735 - accuracy: 0.8450 - val_loss: 0.5732 - val_accuracy: 0.8645\n",
      "Epoch 7/20\n",
      "7/7 [==============================] - 0s 16ms/step - loss: 0.5527 - accuracy: 0.9050 - val_loss: 0.5536 - val_accuracy: 0.8961\n",
      "Epoch 8/20\n",
      "7/7 [==============================] - 0s 15ms/step - loss: 0.5345 - accuracy: 0.9200 - val_loss: 0.5364 - val_accuracy: 0.9100\n",
      "Epoch 9/20\n",
      "7/7 [==============================] - 0s 15ms/step - loss: 0.5164 - accuracy: 0.9250 - val_loss: 0.5205 - val_accuracy: 0.9209\n",
      "Epoch 10/20\n",
      "7/7 [==============================] - 0s 16ms/step - loss: 0.4998 - accuracy: 0.9350 - val_loss: 0.5052 - val_accuracy: 0.9248\n",
      "Epoch 11/20\n",
      "7/7 [==============================] - 0s 16ms/step - loss: 0.4837 - accuracy: 0.9350 - val_loss: 0.4905 - val_accuracy: 0.9278\n",
      "Epoch 12/20\n",
      "7/7 [==============================] - 0s 15ms/step - loss: 0.4683 - accuracy: 0.9450 - val_loss: 0.4765 - val_accuracy: 0.9308\n",
      "Epoch 13/20\n",
      "7/7 [==============================] - 0s 16ms/step - loss: 0.4535 - accuracy: 0.9450 - val_loss: 0.4644 - val_accuracy: 0.9357\n",
      "Epoch 14/20\n",
      "7/7 [==============================] - 0s 16ms/step - loss: 0.4405 - accuracy: 0.9450 - val_loss: 0.4522 - val_accuracy: 0.9357\n",
      "Epoch 15/20\n",
      "7/7 [==============================] - 0s 16ms/step - loss: 0.4279 - accuracy: 0.9450 - val_loss: 0.4411 - val_accuracy: 0.9387\n",
      "Epoch 16/20\n",
      "7/7 [==============================] - 0s 15ms/step - loss: 0.4163 - accuracy: 0.9450 - val_loss: 0.4306 - val_accuracy: 0.9397\n",
      "Epoch 17/20\n",
      "7/7 [==============================] - 0s 15ms/step - loss: 0.4055 - accuracy: 0.9450 - val_loss: 0.4203 - val_accuracy: 0.9407\n",
      "Epoch 18/20\n",
      "7/7 [==============================] - 0s 16ms/step - loss: 0.3949 - accuracy: 0.9450 - val_loss: 0.4102 - val_accuracy: 0.9407\n",
      "Epoch 19/20\n",
      "7/7 [==============================] - 0s 16ms/step - loss: 0.3844 - accuracy: 0.9450 - val_loss: 0.4008 - val_accuracy: 0.9416\n",
      "Epoch 20/20\n",
      "7/7 [==============================] - 0s 15ms/step - loss: 0.3752 - accuracy: 0.9500 - val_loss: 0.3925 - val_accuracy: 0.9446\n",
      "63/63 [==============================] - 0s 2ms/step - loss: 0.4044 - accuracy: 0.9235\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[0.4043788015842438, 0.9235000014305115]"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# train and evaluate model B, without reusing model A\n",
    "\n",
    "tf.random.set_seed(42)\n",
    "model_B = tf.keras.Sequential([\n",
    "    tf.keras.layers.Flatten(input_shape=[28, 28]),\n",
    "    tf.keras.layers.Dense(100, activation=\"relu\",\n",
    "                          kernel_initializer=\"he_normal\"),\n",
    "    tf.keras.layers.Dense(100, activation=\"relu\",\n",
    "                          kernel_initializer=\"he_normal\"),\n",
    "    tf.keras.layers.Dense(100, activation=\"relu\",\n",
    "                          kernel_initializer=\"he_normal\"),\n",
    "    tf.keras.layers.Dense(1, activation=\"sigmoid\")\n",
    "])\n",
    "\n",
    "model_B.compile(loss=\"binary_crossentropy\",\n",
    "                optimizer=tf.keras.optimizers.SGD(learning_rate=0.001),\n",
    "                metrics=[\"accuracy\"])\n",
    "history = model_B.fit(X_train_B, y_train_B, epochs=20,\n",
    "                      validation_data=(X_valid_B, y_valid_B))\n",
    "model_B.evaluate(X_test_B, y_test_B)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_A = tf.keras.models.load_model('my_model_A')\n",
    "model_B_on_A = tf.keras.Sequential(model_A.layers[:-1])\n",
    "model_B_on_A.add(tf.keras.layers.Dense(1, activation='sigmoid'))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Note that model_B_on_A and model_A actually share layers now, so when we train one, it will update both models. If we want to avoid that, we need to build model_B_on_A on top of a clone of model_A:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "tf.random.set_seed(42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_A_clone = tf.keras.models.clone_model(model_A)\n",
    "model_A_clone.set_weights(model_A.get_weights())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_B_on_A = tf.keras.Sequential(model_A_clone.layers[:-1])\n",
    "model_B_on_A.add(tf.keras.layers.Dense(1, activation='sigmoid'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "for layer in model_B_on_A.layers[:-1]:\n",
    "    layer.trainable = False\n",
    "\n",
    "optimizer = tf.keras.optimizers.SGD(learning_rate=0.001)\n",
    "model_B_on_A.compile(loss=\"binary_crossentropy\", optimizer=optimizer,\n",
    "                     metrics=[\"accuracy\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/4\n",
      "7/7 [==============================] - 1s 53ms/step - loss: 0.7796 - accuracy: 0.4400 - val_loss: 0.7439 - val_accuracy: 0.4491\n",
      "Epoch 2/4\n",
      "7/7 [==============================] - 0s 25ms/step - loss: 0.7383 - accuracy: 0.4350 - val_loss: 0.7202 - val_accuracy: 0.4510\n",
      "Epoch 3/4\n",
      "7/7 [==============================] - 0s 20ms/step - loss: 0.7151 - accuracy: 0.4700 - val_loss: 0.7064 - val_accuracy: 0.5045\n",
      "Epoch 4/4\n",
      "7/7 [==============================] - 0s 20ms/step - loss: 0.6902 - accuracy: 0.5200 - val_loss: 0.6872 - val_accuracy: 0.5648\n",
      "Epoch 1/16\n",
      "7/7 [==============================] - 1s 46ms/step - loss: 0.6394 - accuracy: 0.6650 - val_loss: 0.5875 - val_accuracy: 0.7280\n",
      "Epoch 2/16\n",
      "7/7 [==============================] - 0s 24ms/step - loss: 0.5482 - accuracy: 0.8450 - val_loss: 0.5116 - val_accuracy: 0.8783\n",
      "Epoch 3/16\n",
      "7/7 [==============================] - 0s 25ms/step - loss: 0.4778 - accuracy: 0.9050 - val_loss: 0.4624 - val_accuracy: 0.9001\n",
      "Epoch 4/16\n",
      "7/7 [==============================] - 0s 23ms/step - loss: 0.4216 - accuracy: 0.9350 - val_loss: 0.4160 - val_accuracy: 0.9199\n",
      "Epoch 5/16\n",
      "7/7 [==============================] - 0s 21ms/step - loss: 0.3811 - accuracy: 0.9350 - val_loss: 0.3827 - val_accuracy: 0.9268\n",
      "Epoch 6/16\n",
      "7/7 [==============================] - 0s 18ms/step - loss: 0.3489 - accuracy: 0.9450 - val_loss: 0.3570 - val_accuracy: 0.9308\n",
      "Epoch 7/16\n",
      "7/7 [==============================] - 0s 18ms/step - loss: 0.3223 - accuracy: 0.9500 - val_loss: 0.3422 - val_accuracy: 0.9347\n",
      "Epoch 8/16\n",
      "7/7 [==============================] - 0s 16ms/step - loss: 0.3074 - accuracy: 0.9500 - val_loss: 0.3189 - val_accuracy: 0.9377\n",
      "Epoch 9/16\n",
      "7/7 [==============================] - 0s 19ms/step - loss: 0.2839 - accuracy: 0.9500 - val_loss: 0.3036 - val_accuracy: 0.9377\n",
      "Epoch 10/16\n",
      "7/7 [==============================] - 0s 16ms/step - loss: 0.2691 - accuracy: 0.9550 - val_loss: 0.2913 - val_accuracy: 0.9357\n",
      "Epoch 11/16\n",
      "7/7 [==============================] - 0s 17ms/step - loss: 0.2576 - accuracy: 0.9550 - val_loss: 0.2804 - val_accuracy: 0.9357\n",
      "Epoch 12/16\n",
      "7/7 [==============================] - 0s 17ms/step - loss: 0.2461 - accuracy: 0.9600 - val_loss: 0.2706 - val_accuracy: 0.9407\n",
      "Epoch 13/16\n",
      "7/7 [==============================] - 0s 17ms/step - loss: 0.2358 - accuracy: 0.9600 - val_loss: 0.2630 - val_accuracy: 0.9377\n",
      "Epoch 14/16\n",
      "7/7 [==============================] - 0s 16ms/step - loss: 0.2283 - accuracy: 0.9600 - val_loss: 0.2555 - val_accuracy: 0.9397\n",
      "Epoch 15/16\n",
      "7/7 [==============================] - 0s 17ms/step - loss: 0.2209 - accuracy: 0.9600 - val_loss: 0.2490 - val_accuracy: 0.9436\n",
      "Epoch 16/16\n",
      "7/7 [==============================] - 0s 17ms/step - loss: 0.2137 - accuracy: 0.9600 - val_loss: 0.2432 - val_accuracy: 0.9436\n"
     ]
    }
   ],
   "source": [
    "history = model_B_on_A.fit(X_train_B, y_train_B, epochs=4,\n",
    "                           validation_data=(X_valid_B, y_valid_B))\n",
    "\n",
    "for layer in model_B_on_A.layers[:-1]:\n",
    "    layer.trainable = True\n",
    "\n",
    "optimizer = tf.keras.optimizers.SGD(learning_rate=0.001)\n",
    "model_B_on_A.compile(loss=\"binary_crossentropy\", optimizer=optimizer,\n",
    "                     metrics=[\"accuracy\"])\n",
    "history = model_B_on_A.fit(X_train_B, y_train_B, epochs=16,\n",
    "                           validation_data=(X_valid_B, y_valid_B))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "63/63 [==============================] - 0s 3ms/step - loss: 0.2540 - accuracy: 0.9390\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[0.25399863719940186, 0.9390000104904175]"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model_B_on_A.evaluate(X_test_B, y_test_B)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Optimizers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "# a little function to test an optimizer on Fashion MNIST\n",
    "\n",
    "def build_model(seed=42):\n",
    "    tf.random.set_seed(seed)\n",
    "    return tf.keras.Sequential([\n",
    "        tf.keras.layers.Flatten(input_shape=[28, 28]),\n",
    "        tf.keras.layers.Dense(100, activation=\"relu\",\n",
    "                              kernel_initializer=\"he_normal\"),\n",
    "        tf.keras.layers.Dense(100, activation=\"relu\",\n",
    "                              kernel_initializer=\"he_normal\"),\n",
    "        tf.keras.layers.Dense(100, activation=\"relu\",\n",
    "                              kernel_initializer=\"he_normal\"),\n",
    "        tf.keras.layers.Dense(10, activation=\"softmax\")\n",
    "    ])\n",
    "\n",
    "\n",
    "def build_and_train_model(optimizer):\n",
    "    model = build_model()\n",
    "    model.compile(loss=\"sparse_categorical_crossentropy\", optimizer=optimizer,\n",
    "                  metrics=[\"accuracy\"])\n",
    "    return model.fit(X_train, y_train, epochs=10,\n",
    "                     validation_data=(X_valid, y_valid))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### SGD"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/10\n",
      "1719/1719 [==============================] - 5s 3ms/step - loss: 1.4252 - accuracy: 0.5570 - val_loss: 0.8978 - val_accuracy: 0.7126\n",
      "Epoch 2/10\n",
      "1719/1719 [==============================] - 5s 3ms/step - loss: 0.7930 - accuracy: 0.7341 - val_loss: 0.6897 - val_accuracy: 0.7600\n",
      "Epoch 3/10\n",
      "1719/1719 [==============================] - 5s 3ms/step - loss: 0.6643 - accuracy: 0.7730 - val_loss: 0.6031 - val_accuracy: 0.7902\n",
      "Epoch 4/10\n",
      "1719/1719 [==============================] - 5s 3ms/step - loss: 0.6002 - accuracy: 0.7951 - val_loss: 0.5558 - val_accuracy: 0.8068\n",
      "Epoch 5/10\n",
      "1719/1719 [==============================] - 5s 3ms/step - loss: 0.5598 - accuracy: 0.8070 - val_loss: 0.5297 - val_accuracy: 0.8140\n",
      "Epoch 6/10\n",
      "1719/1719 [==============================] - 5s 3ms/step - loss: 0.5317 - accuracy: 0.8170 - val_loss: 0.5140 - val_accuracy: 0.8182\n",
      "Epoch 7/10\n",
      "1719/1719 [==============================] - 5s 3ms/step - loss: 0.5117 - accuracy: 0.8231 - val_loss: 0.4893 - val_accuracy: 0.8292\n",
      "Epoch 8/10\n",
      "1719/1719 [==============================] - 5s 3ms/step - loss: 0.4963 - accuracy: 0.8272 - val_loss: 0.4793 - val_accuracy: 0.8296\n",
      "Epoch 9/10\n",
      "1719/1719 [==============================] - 5s 3ms/step - loss: 0.4835 - accuracy: 0.8317 - val_loss: 0.4692 - val_accuracy: 0.8326\n",
      "Epoch 10/10\n",
      "1719/1719 [==============================] - 5s 3ms/step - loss: 0.4724 - accuracy: 0.8359 - val_loss: 0.4569 - val_accuracy: 0.8376\n"
     ]
    }
   ],
   "source": [
    "optimizer = tf.keras.optimizers.SGD(learning_rate=0.001)\n",
    "history_sgd = build_and_train_model(optimizer)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Momentum optimization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/10\n",
      "1719/1719 [==============================] - 5s 3ms/step - loss: 0.6785 - accuracy: 0.7709 - val_loss: 0.5065 - val_accuracy: 0.8170\n",
      "Epoch 2/10\n",
      "1719/1719 [==============================] - 5s 3ms/step - loss: 0.4638 - accuracy: 0.8367 - val_loss: 0.4400 - val_accuracy: 0.8392\n",
      "Epoch 3/10\n",
      "1719/1719 [==============================] - 5s 3ms/step - loss: 0.4183 - accuracy: 0.8517 - val_loss: 0.4157 - val_accuracy: 0.8520\n",
      "Epoch 4/10\n",
      "1719/1719 [==============================] - 5s 3ms/step - loss: 0.3893 - accuracy: 0.8616 - val_loss: 0.3781 - val_accuracy: 0.8632\n",
      "Epoch 5/10\n",
      "1719/1719 [==============================] - 6s 3ms/step - loss: 0.3678 - accuracy: 0.8687 - val_loss: 0.3699 - val_accuracy: 0.8686\n",
      "Epoch 6/10\n",
      "1719/1719 [==============================] - 5s 3ms/step - loss: 0.3548 - accuracy: 0.8731 - val_loss: 0.3692 - val_accuracy: 0.8656\n",
      "Epoch 7/10\n",
      "1719/1719 [==============================] - 5s 3ms/step - loss: 0.3393 - accuracy: 0.8776 - val_loss: 0.3614 - val_accuracy: 0.8668\n",
      "Epoch 8/10\n",
      "1719/1719 [==============================] - 5s 3ms/step - loss: 0.3292 - accuracy: 0.8810 - val_loss: 0.3558 - val_accuracy: 0.8684\n",
      "Epoch 9/10\n",
      "1719/1719 [==============================] - 5s 3ms/step - loss: 0.3211 - accuracy: 0.8842 - val_loss: 0.3664 - val_accuracy: 0.8638\n",
      "Epoch 10/10\n",
      "1719/1719 [==============================] - 5s 3ms/step - loss: 0.3112 - accuracy: 0.8864 - val_loss: 0.3694 - val_accuracy: 0.8662\n"
     ]
    }
   ],
   "source": [
    "optimizer = tf.keras.optimizers.SGD(learning_rate=0.001, momentum=0.9)\n",
    "history_momentum = build_and_train_model(optimizer) "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Nesterov Accelerated Gradient"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/10\n",
      "1719/1719 [==============================] - 5s 3ms/step - loss: 0.6915 - accuracy: 0.7673 - val_loss: 0.4964 - val_accuracy: 0.8226\n",
      "Epoch 2/10\n",
      "1719/1719 [==============================] - 5s 3ms/step - loss: 0.4652 - accuracy: 0.8385 - val_loss: 0.4451 - val_accuracy: 0.8400\n",
      "Epoch 3/10\n",
      "1719/1719 [==============================] - 5s 3ms/step - loss: 0.4223 - accuracy: 0.8512 - val_loss: 0.4051 - val_accuracy: 0.8548\n",
      "Epoch 4/10\n",
      "1719/1719 [==============================] - 5s 3ms/step - loss: 0.3963 - accuracy: 0.8609 - val_loss: 0.3880 - val_accuracy: 0.8610\n",
      "Epoch 5/10\n",
      "1719/1719 [==============================] - 5s 3ms/step - loss: 0.3753 - accuracy: 0.8673 - val_loss: 0.3772 - val_accuracy: 0.8648\n",
      "Epoch 6/10\n",
      "1719/1719 [==============================] - 5s 3ms/step - loss: 0.3611 - accuracy: 0.8723 - val_loss: 0.3913 - val_accuracy: 0.8598\n",
      "Epoch 7/10\n",
      "1719/1719 [==============================] - 5s 3ms/step - loss: 0.3460 - accuracy: 0.8769 - val_loss: 0.3812 - val_accuracy: 0.8646\n",
      "Epoch 8/10\n",
      "1719/1719 [==============================] - 5s 3ms/step - loss: 0.3357 - accuracy: 0.8795 - val_loss: 0.3714 - val_accuracy: 0.8624\n",
      "Epoch 9/10\n",
      "1719/1719 [==============================] - 5s 3ms/step - loss: 0.3258 - accuracy: 0.8834 - val_loss: 0.3658 - val_accuracy: 0.8664\n",
      "Epoch 10/10\n",
      "1719/1719 [==============================] - 5s 3ms/step - loss: 0.3151 - accuracy: 0.8866 - val_loss: 0.3627 - val_accuracy: 0.8728\n"
     ]
    }
   ],
   "source": [
    "\n",
    "optimizer = tf.keras.optimizers.SGD(learning_rate=0.001, momentum=0.9,\n",
    "                                    nesterov=True)\n",
    "history_nesterov = build_and_train_model(optimizer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
