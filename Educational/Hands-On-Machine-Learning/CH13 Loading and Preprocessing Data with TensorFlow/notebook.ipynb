{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# CH 13 Loading and Preprocessing Data with TensorFlow\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### The tf.data API\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Help on package tensorflow._api.v2.data in tensorflow._api.v2:\n",
      "\n",
      "NAME\n",
      "    tensorflow._api.v2.data - `tf.data.Dataset` API for input pipelines.\n",
      "\n",
      "DESCRIPTION\n",
      "    See [Importing Data](https://tensorflow.org/guide/data) for an overview.\n",
      "\n",
      "PACKAGE CONTENTS\n",
      "    experimental (package)\n",
      "\n",
      "DATA\n",
      "    AUTOTUNE = -1\n",
      "    INFINITE_CARDINALITY = -1\n",
      "    UNKNOWN_CARDINALITY = -2\n",
      "\n",
      "FILE\n",
      "    c:\\users\\sayed\\.conda\\envs\\homl3\\lib\\site-packages\\tensorflow\\_api\\v2\\data\\__init__.py\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "help(tf.data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<tf.Tensor: shape=(10,), dtype=int32, numpy=array([0, 1, 2, 3, 4, 5, 6, 7, 8, 9])>"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tf.range(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<_TensorSliceDataset element_spec=TensorSpec(shape=(), dtype=tf.int32, name=None)>"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dataset = tf.data.Dataset.from_tensor_slices(tf.range(10))\n",
    "dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "'_TensorSliceDataset' object is not subscriptable\n"
     ]
    }
   ],
   "source": [
    "try :\n",
    "    dataset[0]\n",
    "except Exception as ex:\n",
    "    print(ex)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "10"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(dataset)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tf.Tensor(0, shape=(), dtype=int32)\n",
      "tf.Tensor(1, shape=(), dtype=int32)\n",
      "tf.Tensor(2, shape=(), dtype=int32)\n"
     ]
    }
   ],
   "source": [
    "it = iter(dataset)\n",
    "print(next(it))\n",
    "print(next(it))\n",
    "print(next(it))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tf.Tensor(0, shape=(), dtype=int32)\n",
      "tf.Tensor(1, shape=(), dtype=int32)\n",
      "tf.Tensor(2, shape=(), dtype=int32)\n",
      "tf.Tensor(3, shape=(), dtype=int32)\n",
      "tf.Tensor(4, shape=(), dtype=int32)\n",
      "tf.Tensor(5, shape=(), dtype=int32)\n",
      "tf.Tensor(6, shape=(), dtype=int32)\n",
      "tf.Tensor(7, shape=(), dtype=int32)\n",
      "tf.Tensor(8, shape=(), dtype=int32)\n",
      "tf.Tensor(9, shape=(), dtype=int32)\n"
     ]
    }
   ],
   "source": [
    "for item in dataset:\n",
    "    print(item)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'a': (<tf.Tensor: shape=(), dtype=int32, numpy=1>, <tf.Tensor: shape=(), dtype=int32, numpy=4>), 'b': <tf.Tensor: shape=(), dtype=int32, numpy=7>}\n",
      "{'a': (<tf.Tensor: shape=(), dtype=int32, numpy=2>, <tf.Tensor: shape=(), dtype=int32, numpy=5>), 'b': <tf.Tensor: shape=(), dtype=int32, numpy=8>}\n",
      "{'a': (<tf.Tensor: shape=(), dtype=int32, numpy=3>, <tf.Tensor: shape=(), dtype=int32, numpy=6>), 'b': <tf.Tensor: shape=(), dtype=int32, numpy=9>}\n"
     ]
    }
   ],
   "source": [
    "X_nested = {\"a\": ([1, 2, 3], [4, 5, 6]), \"b\": [7, 8, 9]}\n",
    "dataset = tf.data.Dataset.from_tensor_slices(X_nested)\n",
    "\n",
    "for item in dataset:\n",
    "    print(item)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ERROR:  Dimensions 3 and 4 are not compatible\n"
     ]
    }
   ],
   "source": [
    "# let's try using unbalanced dict\n",
    "try :\n",
    "    X_nested = {\"a\": ([1, 2, 3], [4, 5, 6]), \"b\": [7, 8, 9,10]}\n",
    "    dataset = tf.data.Dataset.from_tensor_slices(X_nested)\n",
    "\n",
    "    for item in dataset:\n",
    "        print(item)\n",
    "except Exception as ex:\n",
    "    print('ERROR: ',ex)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Chaining Transformation\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 0, 1, 2, 3, 4, 5, 6, 7, 8, 9]\n"
     ]
    }
   ],
   "source": [
    "dataset = tf.data.Dataset.from_tensor_slices(tf.range(10))\n",
    "dataset = dataset.repeat(3)\n",
    "print(list(dataset.as_numpy_iterator()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[array([0, 1, 2, 3, 4, 5, 6, 7]),\n",
       " array([8, 9, 0, 1, 2, 3, 4, 5]),\n",
       " array([6, 7, 8, 9, 0, 1, 2, 3]),\n",
       " array([4, 5, 6, 7, 8, 9])]"
      ]
     },
     "execution_count": 38,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "list(dataset.batch(batch_size=8).as_numpy_iterator())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 0, 1, 2, 3, 4, 5, 6, 7, 8, 9]\n"
     ]
    }
   ],
   "source": [
    "dataset = tf.data.Dataset.from_tensor_slices(tf.range(10)).repeat(200)\n",
    "print(list(dataset.as_numpy_iterator())[:20])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "2000"
      ]
     },
     "execution_count": 80,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(dataset)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "metadata": {},
   "outputs": [],
   "source": [
    "batched_data = dataset.batch(batch_size=8,drop_remainder=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "num_parallel_calls\n",
    "(Optional.) A tf.int64 scalar tf.Tensor, representing the number of batches to compute asynchronously in parallel. If not specified, batches will be computed sequentially. If the value tf.data.AUTOTUNE is used, then the number of parallel calls is set dynamically based on available resources.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "metadata": {},
   "outputs": [],
   "source": [
    "parallel_batched_data = dataset.batch(batch_size=8,drop_remainder=True,\n",
    "                            num_parallel_calls=tf.data.AUTOTUNE)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[array([0, 1, 2, 3, 4, 5, 6, 7]),\n",
       " array([8, 9, 0, 1, 2, 3, 4, 5]),\n",
       " array([6, 7, 8, 9, 0, 1, 2, 3]),\n",
       " array([4, 5, 6, 7, 8, 9, 0, 1])]"
      ]
     },
     "execution_count": 85,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "list(parallel_batched_data.as_numpy_iterator())[:4]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### map\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 93,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[array([ 0,  3,  6,  9, 12, 15, 18, 21]),\n",
       " array([24, 27,  0,  3,  6,  9, 12, 15]),\n",
       " array([18, 21, 24, 27,  0,  3,  6,  9]),\n",
       " array([12, 15, 18, 21, 24, 27,  0,  3])]"
      ]
     },
     "execution_count": 93,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "mapped_data = batched_data.map(lambda x : x*3)\n",
    "list(mapped_data.as_numpy_iterator())[:4]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 95,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[array([1.0000000e+00, 2.7182817e+00, 7.3890562e+00, 2.0085537e+01,\n",
       "        5.4598148e+01, 1.4841316e+02, 4.0342877e+02, 1.0966332e+03],\n",
       "       dtype=float32),\n",
       " array([2.9809580e+03, 8.1030835e+03, 1.0000000e+00, 2.7182817e+00,\n",
       "        7.3890562e+00, 2.0085537e+01, 5.4598148e+01, 1.4841316e+02],\n",
       "       dtype=float32),\n",
       " array([4.0342877e+02, 1.0966332e+03, 2.9809580e+03, 8.1030835e+03,\n",
       "        1.0000000e+00, 2.7182817e+00, 7.3890562e+00, 2.0085537e+01],\n",
       "       dtype=float32),\n",
       " array([5.4598148e+01, 1.4841316e+02, 4.0342877e+02, 1.0966332e+03,\n",
       "        2.9809580e+03, 8.1030835e+03, 1.0000000e+00, 2.7182817e+00],\n",
       "       dtype=float32)]"
      ]
     },
     "execution_count": 95,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def exp_func(x):\n",
    "    return tf.exp(float(x))\n",
    "\n",
    "mapped_data = batched_data.map(exp_func)\n",
    "list(mapped_data.as_numpy_iterator())[:4]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### filter\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 122,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[6, 7, 8, 9, 6]"
      ]
     },
     "execution_count": 122,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "filtered_data = dataset.filter(lambda x : x>5)\n",
    "list(filtered_data.as_numpy_iterator())[:5]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 124,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Invalid `predicate`. `predicate` must return a `tf.bool` scalar tensor, but its return type is TensorSpec(shape=(8,), dtype=tf.bool, name=None).\n"
     ]
    }
   ],
   "source": [
    "try :\n",
    "    filtered_data = batched_data.filter(lambda x : x>5)\n",
    "    list(filtered_data.as_numpy_iterator())[:5]\n",
    "except Exception as ex:\n",
    "    print(\"ERROR\",ex)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 134,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[array([0, 1, 2, 3, 4, 5, 6, 7]),\n",
       " array([8, 9, 0, 1, 2, 3, 4, 5]),\n",
       " array([6, 7, 8, 9, 0, 1, 2, 3]),\n",
       " array([4, 5, 6, 7, 8, 9, 0, 1]),\n",
       " array([0, 1, 2, 3, 4, 5, 6, 7])]"
      ]
     },
     "execution_count": 134,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "filtered_data = batched_data.filter(lambda x : tf.reduce_min(x)==0)\n",
    "list(filtered_data.as_numpy_iterator())[:5]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 140,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[array([0, 1, 2, 3, 4, 5, 6, 7]),\n",
       " array([8, 9, 0, 1, 2, 3, 4, 5]),\n",
       " array([6, 7, 8, 9, 0, 1, 2, 3])]"
      ]
     },
     "execution_count": 140,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "list(filtered_data.take(3).as_numpy_iterator())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Shuffle\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "buffer_size\n",
    "A tf.int64 scalar tf.Tensor, representing the number of elements from this dataset from which the new dataset will sample. To uniformly shuffle the entire dataset, use buffer_size=dataset.cardinality().\n",
    "\n",
    "Randomly shuffles the elements of this dataset.\n",
    "\n",
    "This dataset fills a buffer with buffer_size elements, then randomly samples elements from this buffer, replacing the selected elements with new elements. For perfect shuffling, a buffer size greater than or equal to the full size of the dataset is required.\n",
    "\n",
    "For instance, if your dataset contains 10,000 elements but buffer_size is set to 1,000, then shuffle will initially select a random element from only the first 1,000 elements in the buffer. Once an element is selected, its space in the buffer is replaced by the next (i.e. 1,001-st) element, maintaining the 1,000 element buffer.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 157,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[array([0, 1, 3, 4, 5, 2, 6], dtype=int64),\n",
       " array([0, 9, 1, 7, 2, 3, 8], dtype=int64),\n",
       " array([5, 8, 7, 9, 4, 6], dtype=int64)]"
      ]
     },
     "execution_count": 157,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dataset = tf.data.Dataset.range(10).repeat(2).shuffle(buffer_size=4, seed=42, reshuffle_each_iteration=False).batch(7)\n",
    "list(dataset.as_numpy_iterator())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Interleaving Lines from Multiple Files\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "homl3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.14"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
