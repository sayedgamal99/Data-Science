{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## CH13 Exercises: Loading and Preprocessing Data With Tensorflow\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "1- Why would you want to use the tf.data API?\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "reading and preprocessing large datasets is complex, but using this api we can load data from various sources( such as text or binary files), reading data in parallel from different files, applying transformations on fly, interleaving records, shuffling the data, batching and prefetching it.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "2. What are the benefits of splitting a large dataset into multiple files?\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "splitting data into multiple files make it possible to shuffle it good before shuffling it with the internal buffer when calling. It also simpler to handle huge dataset that can't fit in a single machine. It is also easier to manipulate thousands files rather than one huge file. If the data spread across multiple servers, it is possible to download several files from multiple servers simultaneously, which improves the bandwidth usage.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "3. During training, how can you tell that your input pipeline is the bottleneck?\n",
    "   What can you do to fix it?\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "When utilization of GPU is low ( below 80% ). It could indicate that model waiting the data from the pipeline.\n",
    "\n",
    "When you notice high cpu utilization, this means that is take alot of time preprocessing the data and passing through the pipeline.\n",
    "\n",
    "we can fix it using prefetching to overlap the work of input pipeline with the model training. increase parallelism in data loading\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "4. Can you save any binary data to a TFRecord file, or only serialized protocol\n",
    "   buffers?\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The TFRecord format composed of a sequence of arbitrary binary records. we can store any binary data we want in each record. most of tfrecord files contains sequences of serialized protocol buffers, This make it possible to benefit from the advantages of protocol buffers, like the fact they can be read easily across multiple platforms and languages.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "5. Why would you go through the hassle of converting all your data to the Example\n",
    "   protobuf format? Why not use your own protobuf definition?\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The Example protobuf format has the advantages that tensorflow provides some function to parse it directly like (tf.io.parse\\*example()). It is sufficient to work with most cases. but if it doesn't, we can define our own format and compile it using protoc compiler, and this need to deploy the descriptor along with the model.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "6. When using TFRecords, when would you want to activate compression? Why\n",
    "   not do it systematically?\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "if we download it, it's prefered to activate the compression to save some time. but if it in the same machine. it's preferred not to, to avoid wasting cpu for this jop (decompression)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "7.  Data can be preprocessed directly when writing the data files, or within the\n",
    "    tf.data pipeline, or in preprocessing layers within your model. Can you list a few\n",
    "    pros and cons of each option?\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- 1- preprocessed directly:\n",
    "\n",
    "  - easy to apply preprocessing once,\n",
    "  - training run faster,\n",
    "  - preprocessed data would by smaller than original one,\n",
    "\n",
    "  - but can have data mismatch when production\n",
    "  - if we do data augmentation, we need to make alot of variants. (big space needed on the disk)\n",
    "\n",
    "- tf.data pipeline:\n",
    "\n",
    "  - easier to modify the preprocessing logic and apply data augmentation without making variants of the data.\n",
    "  - we can use the multi-threading and prefetching, improving the performance\n",
    "\n",
    "  - but slower training, each instance would be processed once per epoch.\n",
    "  - potential overwork and bottleneck risk.\n",
    "  - still need to add the same preprocessing the the production pipeline\n",
    "\n",
    "- preprocessing layers in the model:\n",
    "  - we need to write preprocessing code once inside the model, and don't care about the production preprocessing at all.\n",
    "  - slower training, each instance would be processed once per epoch\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "8. Name a few common ways you can encode categorical integer features. What\n",
    "   about text?\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "ordinal encoding ( for ordinal category data )\n",
    "one hot encoding, multi hot encoding, id encoding, embedding\n",
    "\n",
    "count encoding, tf-idf encoding, pertained language models ,embedding\n",
    "\n",
    "For Categorical Features: Use ordinal encoding for ordered categories, one-hot encoding for unordered categories, and embeddings for many categories.\n",
    "For Text Data: Start with the TextVectorization layer for simple tasks, or leverage TF Text for advanced processing. For complex tasks, consider using pretrained models from TF Hub or Hugging Face.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "homl3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.10.14"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
