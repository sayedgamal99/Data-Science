{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "1. Which linear regression training algorithm can you use if you have a training set\n",
    "   with millions of features?\n",
    "\n",
    "> I will choose Gradient Descent (mini-batch version) to benefit from vectorization and provide convergence.\n",
    "\n",
    "<br>\n",
    "<br>\n",
    "\n",
    "2. Suppose the features in your training set have very different scales. Which algo‐\n",
    "   rithms might suffer from this, and how? What can you do about it?\n",
    "\n",
    "> Gradient Descent will suffer from different scales for features. because if $\\theta{}_1$ is too small it will take to long to make an effective change in the cost function and versa-vise.\n",
    "> it will converge (reach global minima) but take too long to do that.\n",
    "> we can use Standard Scaler (Normalization)\n",
    "\n",
    "> Additional: Moreover, regularized models may converge to a suboptimal solution if the features are not scaled: since regularization penalizes large weights, features with smaller values will tend to be ignored compared to features with larger values.\n",
    "\n",
    "<br>\n",
    "<br>\n",
    "\n",
    "3. Can gradient descent get stuck in a local minimum when training a logistic\n",
    "   regression model?\n",
    "\n",
    "> The Answer is **NO** because the cost function for gradient descent is also **Convex** ( which have no local minima But Only one global minima)\n",
    "\n",
    "<br>\n",
    "<br>\n",
    "\n",
    "4. Do all gradient descent algorithms lead to the same model, provided you let them run long enough?\n",
    "\n",
    "> **NO** because SGD (Stochastic) & mini-batch will not converge to the global minima but Batch & will converge.\n",
    "\n",
    "<br>\n",
    "<br>\n",
    "\n",
    "5. Suppose you use batch gradient descent and you plot the validation error at every\n",
    "   epoch. If you notice that the validation error consistently goes up, what is likely\n",
    "   going on? How can you fix this\n",
    "\n",
    "> if training error also goes up after each iteration it must be the learning rate too large and the algorithm diverge, or it will be **OverFitting**, we can fix it by several methods, but in this case i would use early stopping with restoring best weights.\n",
    "> other methods such as using penalty for the weights such as $l2$ and $l1$ regularization.\n",
    "\n",
    "<br>\n",
    "<br>\n",
    "\n",
    "6. Is it a good idea to stop mini-batch gradient descent immediately when the\n",
    "   validation error goes up?\n",
    "\n",
    "> **NO**. Because it may goes down after it, it depends on how much the current batch is difficult for learning, and due to shuffling it happens.\n",
    "> so, we can save the best model with intervals and give the model some tolerance epochs to improve, otherwise return best model and stop training.\n",
    "\n",
    "<br>\n",
    "<br>\n",
    "\n",
    "7. Which gradient descent algorithm (among those we discussed) will reach the\n",
    "   vicinity of the optimal solution the fastest? Which will actually converge? How\n",
    "   can you make the others converge as well?\n",
    "\n",
    "> **Mini-Batch** will be the fastest, we can make SGD get convergence by using learning rate schedule.\n",
    "\n",
    "> Additional: Stochastic Gradient Descent has the fastest training iteration since it considers only one training instance at a time, so it is generally the first to reach the vicinity of the global optimum (or Mini-batch GD with a very small mini-batch size). However, only Batch Gradient Descent will actually converge, given enough training time. As mentioned, Stochastic GD and Mini-batch GD will bounce around the optimum, unless you gradually reduce the learning rate.\n",
    "\n",
    "<br>\n",
    "<br>\n",
    "\n",
    "8. Suppose you are using polynomial regression. You plot the learning curves and\n",
    "   you notice that there is a large gap between the training error and the validation\n",
    "   error. What is happening? What are three ways to solve this?\n",
    "\n",
    "> The is a problem of **Over-fitting** that happened because of complexity of the model, which not suit the data (model too complex than the data). we can simplify the model by reducing degree of polynomial or using regularization technique.\n",
    "\n",
    "<br>\n",
    "<br>\n",
    "\n",
    "9. Suppose you are using ridge regression and you notice that the training error\n",
    "   and the validation error are almost equal and fairly high. Would you say that\n",
    "   the model suffers from high bias or high variance? Should you increase the\n",
    "   regularization hyperparameter α or reduce it?\n",
    "\n",
    "> The model suffering from **High-Bias**. in this case we try to reduce the regularization hyperparameter $\\lambda$.\n",
    "\n",
    "<br>\n",
    "<br>\n",
    "\n",
    "10. Why would you want to use:\n",
    "    a. Ridge regression instead of plain linear regression (i.e., without any\n",
    "    regularization)?\n",
    "    b. Lasso instead of ridge regression?\n",
    "    c. Elastic net instead of lasso regression?\n",
    "\n",
    "> We use **Ridge** instead of Linear Regression because we need the model to generalize the output instead of getting in over-fitting trap easily.\n",
    "> And we choose **Lasso** Over Ridge if we need to perform Feature Selection automatically.\n",
    "> **ElasticNet** over Lasso because lasso behave erratically when some features are correlated.\n",
    "\n",
    "<br>\n",
    "<br>\n",
    "\n",
    "11. Suppose you want to classify pictures as outdoor/indoor and daytime/nighttime.\n",
    "    Should you implement two logistic regression classifiers or one softmax regression classifier?\n",
    "\n",
    "> I would choose to train two Logistic models, or try to implement some multi-output classifier to detect whatever in the picture.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "12. Implement batch gradient descent with early stopping for softmax regression\n",
    "    without using Scikit-Learn, only NumPy. Use it on a classification task such as\n",
    "    the iris dataset.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['data', 'target', 'frame', 'target_names', 'DESCR', 'feature_names', 'filename', 'data_module']\n"
     ]
    }
   ],
   "source": [
    "from sklearn.datasets import load_iris\n",
    "import numpy as np\n",
    "data = load_iris(as_frame=True)\n",
    "X, y = data['data'], data['target']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[1. , 5.1, 3.5, 1.4, 0.2],\n",
       "       [1. , 4.9, 3. , 1.4, 0.2]])"
      ]
     },
     "execution_count": 34,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_with_b = np.c_[np.ones(len(X)), X]\n",
    "X_with_b[:2]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_ratio = .2\n",
    "validation_ratio = .2\n",
    "total_size = len(X_with_b)\n",
    "\n",
    "test_size = int(test_ratio*total_size)\n",
    "validation_size = int(validation_ratio*total_size)\n",
    "train_size = total_size - validation_size - test_size\n",
    "\n",
    "np.random.seed(42)\n",
    "shuffled = np.random.permutation(total_size)\n",
    "\n",
    "X_train = X_with_b[shuffled[:train_size]]\n",
    "X_valid = X_with_b[shuffled[train_size:-test_size]]\n",
    "X_test = X_with_b[shuffled[-test_size:]]\n",
    "\n",
    "y_train = y[shuffled[:train_size]]\n",
    "y_valid = y[shuffled[train_size:-test_size]]\n",
    "y_test = y[shuffled[-test_size:]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [],
   "source": [
    "number_of_classes = 3\n",
    "\n",
    "\n",
    "def onehot(y):\n",
    "    return np.identity(3)[y]\n",
    "\n",
    "\n",
    "y_train_enc = onehot(y_train)\n",
    "y_valid_enc = onehot(y_valid)\n",
    "y_test_enc = onehot(y_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Scaling\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {},
   "outputs": [],
   "source": [
    "mean = X_train[:, 1:].mean(axis=0)\n",
    "std = X_train[:, 1:].std(axis=0)\n",
    "\n",
    "X_train[:, 1:] = (X_train[:, 1:]-mean)/std\n",
    "X_valid[:, 1:] = (X_valid[:, 1:]-mean)/std\n",
    "X_test[:, 1:] = (X_test[:, 1:]-mean)/std"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "metadata": {},
   "outputs": [],
   "source": [
    "def softmax(logits):\n",
    "    exps = np.exp(logits)\n",
    "    exp_sums = exps.sum(axis=1, keepdims=True)\n",
    "    return exps / exp_sums\n",
    "\n",
    "\n",
    "n_inputs = X_train.shape[1]\n",
    "n_outputs = number_of_classes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0 4.626481998690278\n",
      "1000 0.11433651664960792\n",
      "2000 0.09771828547727753\n",
      "3000 0.09482470188955566\n",
      "4000 0.09774000000102669\n",
      "5000 0.1031457416377819\n"
     ]
    }
   ],
   "source": [
    "learning_rate = .5\n",
    "n_epochs = 5001\n",
    "m = len(X_train)\n",
    "epsilon = 1e-5\n",
    "\n",
    "np.random.seed(42)\n",
    "Theta = np.random.randn(n_inputs, n_outputs)\n",
    "\n",
    "for epoch in range(n_epochs):\n",
    "    z = np.dot(X_train, Theta)\n",
    "    a = softmax(z)\n",
    "\n",
    "    if epoch % 1000 == 0:\n",
    "        Y_proba_valid = softmax(X_valid @ Theta)\n",
    "        xentropy_losses = -(y_valid_enc * np.log(Y_proba_valid + epsilon))\n",
    "        print(epoch, xentropy_losses.sum(axis=1).mean())\n",
    "\n",
    "    error = a - y_train_enc\n",
    "    grads = (1/m)*(X_train.T@error)\n",
    "    Theta = Theta - learning_rate*grads"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[ 1.23389181,  6.24071307, -6.46846648],\n",
       "       [-2.52703783,  2.47299879,  1.10877856],\n",
       "       [ 3.64053358,  0.04103479, -1.80439521],\n",
       "       [-5.10552678, -2.6481553 ,  7.36709468],\n",
       "       [-5.3114502 , -2.65232032,  4.56753472]])"
      ]
     },
     "execution_count": 75,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "Theta"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.9666666666666667"
      ]
     },
     "execution_count": 83,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "z_preds = X_valid@Theta\n",
    "a_preds = softmax(z_preds)\n",
    "preds = np.argmax(a_preds, axis=1)\n",
    "accuracy_score = (preds == y_valid).mean()\n",
    "accuracy_score"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Regularization\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0 4.6922\n",
      "1000 0.2835\n",
      "2000 0.2834\n",
      "3000 0.2834\n",
      "4000 0.2834\n",
      "5000 0.2834\n"
     ]
    }
   ],
   "source": [
    "learning_rate = .5\n",
    "n_epochs = 5001\n",
    "m = len(X_train)\n",
    "epsilon = 1e-5\n",
    "alpha = .01\n",
    "\n",
    "np.random.seed(42)\n",
    "Theta = np.random.randn(n_inputs, n_outputs)\n",
    "\n",
    "for epoch in range(n_epochs):\n",
    "    z = np.dot(X_train, Theta)\n",
    "    a = softmax(z)\n",
    "\n",
    "    if epoch % 1000 == 0:\n",
    "        Y_proba_valid = softmax(X_valid @ Theta)\n",
    "        xentropy_losses = -(y_valid_enc * np.log(Y_proba_valid + epsilon))\n",
    "        l2_loss = 1 / 2 * (Theta[1:] ** 2).sum()\n",
    "        total_loss = xentropy_losses.sum(axis=1).mean() + alpha * l2_loss\n",
    "        print(epoch, total_loss.round(4))\n",
    "\n",
    "    error = a - y_train_enc\n",
    "    grads = (1/m)*(X_train.T@error)\n",
    "    # penalty for only W not b (this is the reason for whole concatenation)\n",
    "    grads += np.r_[np.zeros([1, n_outputs]), .01 * Theta[1:]]\n",
    "    Theta = Theta - learning_rate*grads"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 89,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.9333333333333333"
      ]
     },
     "execution_count": 89,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "z_preds = X_valid@Theta\n",
    "a_preds = softmax(z_preds)\n",
    "preds = np.argmax(a_preds, axis=1)\n",
    "accuracy_score = (preds == y_valid).mean()\n",
    "accuracy_score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 93,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0 4.6922\n",
      "1000 0.2835\n",
      "2000 0.2834\n",
      "3000 0.2834\n",
      "4000 0.2834\n",
      "5000 0.2834\n",
      "5514 0.2834\n",
      "5515 0.2834 early stopping!\n"
     ]
    }
   ],
   "source": [
    "eta = 0.5\n",
    "n_epochs = 50_001\n",
    "m = len(X_train)\n",
    "epsilon = 1e-5\n",
    "C = 100  # regularization hyperparameter\n",
    "best_loss = np.infty\n",
    "\n",
    "np.random.seed(42)\n",
    "Theta = np.random.randn(n_inputs, n_outputs)\n",
    "tolerance = 5  # 5 epochs of not getting better (patience)\n",
    "for epoch in range(n_epochs):\n",
    "    logits = X_train @ Theta\n",
    "    Y_proba = softmax(logits)\n",
    "    Y_proba_valid = softmax(X_valid @ Theta)\n",
    "    xentropy_losses = -(y_valid_enc * np.log(Y_proba_valid + epsilon))\n",
    "    l2_loss = 1 / 2 * (Theta[1:] ** 2).sum()\n",
    "    total_loss = xentropy_losses.sum(axis=1).mean() + 1 / C * l2_loss\n",
    "    if epoch % 1000 == 0:\n",
    "        print(epoch, total_loss.round(4))\n",
    "    if total_loss < best_loss:\n",
    "        best_loss = total_loss\n",
    "    else:\n",
    "        tolerance -= 1\n",
    "        if tolerance == 0:\n",
    "            print(epoch - 1, best_loss.round(4))\n",
    "            print(epoch, total_loss.round(4), \"early stopping!\")\n",
    "            break\n",
    "    error = Y_proba - y_train_enc\n",
    "    gradients = 1 / m * X_train.T @ error\n",
    "    gradients += np.r_[np.zeros([1, n_outputs]), 1 / C * Theta[1:]]\n",
    "    Theta = Theta - eta * gradients"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 94,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.9333333333333333"
      ]
     },
     "execution_count": 94,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "z_preds = X_valid@Theta\n",
    "a_preds = softmax(z_preds)\n",
    "preds = np.argmax(a_preds, axis=1)\n",
    "accuracy_score = (preds == y_valid).mean()\n",
    "accuracy_score"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
