{"metadata":{"kernelspec":{"name":"python3","display_name":"Python 3","language":"python"},"language_info":{"name":"python","version":"3.10.12","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"colab":{"provenance":[],"gpuType":"T4"},"accelerator":"GPU","kaggle":{"accelerator":"gpu","dataSources":[],"dockerImageVersionId":30823,"isInternetEnabled":true,"language":"python","sourceType":"notebook","isGpuEnabled":true}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"markdown","source":"#  Chapter 16: Natural Language Processing with RNNs and Attention","metadata":{"id":"ggVICmR_lDmx"}},{"cell_type":"code","source":"import os\nos.environ[\"TF_USE_LEGACY_KERAS\"] = \"1\" \n\nimport tensorflow as tf\nimport numpy as np\nfrom pathlib import Path\nimport tf_keras","metadata":{"id":"kGPHeBvwlDmz","trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"\nshakespeare_url = \"https://homl.info/shakespeare\"  # shortcut URL\nfilepath = tf.keras.utils.get_file(\"shakespeare.txt\", shakespeare_url)\nwith open(filepath) as f:\n    shakespeare_text = f.read()","metadata":{"id":"IIdD6DJIlDm0","outputId":"688cc633-fb31-4ef3-c339-0097afcff21e","trusted":true,"execution":{"iopub.status.busy":"2024-12-30T17:37:14.880120Z","iopub.execute_input":"2024-12-30T17:37:14.880549Z","iopub.status.idle":"2024-12-30T17:37:15.884028Z","shell.execute_reply.started":"2024-12-30T17:37:14.880528Z","shell.execute_reply":"2024-12-30T17:37:15.883136Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"print(shakespeare_text[:80])","metadata":{"id":"D6bLc0i3nwns","outputId":"ab6e1b8e-72b3-48ba-807c-8aa00921e9f2","trusted":true,"execution":{"iopub.status.busy":"2024-12-30T17:37:15.885891Z","iopub.execute_input":"2024-12-30T17:37:15.886204Z","iopub.status.idle":"2024-12-30T17:37:15.890544Z","shell.execute_reply.started":"2024-12-30T17:37:15.886168Z","shell.execute_reply":"2024-12-30T17:37:15.889690Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"all_chars  = \"\".join(sorted(set(shakespeare_text.lower())))\nprint(all_chars)\nprint(len(all_chars))","metadata":{"id":"uN6lMOKSn-ex","outputId":"c934481e-1d9c-44ba-f4e9-b56f5e25063e","trusted":true,"execution":{"iopub.status.busy":"2024-12-30T17:37:15.891733Z","iopub.execute_input":"2024-12-30T17:37:15.891929Z","iopub.status.idle":"2024-12-30T17:37:15.922048Z","shell.execute_reply.started":"2024-12-30T17:37:15.891911Z","shell.execute_reply":"2024-12-30T17:37:15.921242Z"}},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"## Generating Shakespearean Text Using a Character RNN","metadata":{"id":"R7JKQcEOoTZa"}},{"cell_type":"markdown","source":"### Preparing Dataset for a char level rnn model","metadata":{"id":"yNXX2EUpomcj"}},{"cell_type":"markdown","source":"#### Text Vectorization","metadata":{"id":"VsEzAzNpoQ2X"}},{"cell_type":"code","source":"text_vec_layer = tf.keras.layers.TextVectorization(split='character', standardize='lower')\ntext_vec_layer.adapt([shakespeare_text])\nencoded = text_vec_layer([shakespeare_text])[0]\nencoded","metadata":{"id":"IwRa58oQoJqV","outputId":"814a5d49-050b-42c6-94b5-cf2738617efc","trusted":true,"execution":{"iopub.status.busy":"2024-12-30T17:37:15.922760Z","iopub.execute_input":"2024-12-30T17:37:15.922971Z","iopub.status.idle":"2024-12-30T17:37:18.586121Z","shell.execute_reply.started":"2024-12-30T17:37:15.922942Z","shell.execute_reply":"2024-12-30T17:37:18.585369Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"encoded -= 2 # drop 0 for padding and 1 for unkown tokens\nn_tokens = text_vec_layer.vocabulary_size()-2\ndataset_size = len(encoded)\nprint(\"n_tokens:\", n_tokens)\nprint(\"dataset_size:\", dataset_size)","metadata":{"id":"22HqryeMo9m-","outputId":"b74e3a9c-4cd4-4646-b226-3955d2f841d4","trusted":true,"execution":{"iopub.status.busy":"2024-12-30T17:37:18.586836Z","iopub.execute_input":"2024-12-30T17:37:18.587072Z","iopub.status.idle":"2024-12-30T17:37:18.595769Z","shell.execute_reply.started":"2024-12-30T17:37:18.587052Z","shell.execute_reply":"2024-12-30T17:37:18.594950Z"}},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"it is seq2seq model","metadata":{"id":"b_y5WRc6p2NB"}},{"cell_type":"code","source":"def to_dataset(sequence, length, shuffle=False, seed=None, batch_size=32):\n    dataset = tf.data.Dataset.from_tensor_slices(sequence)\n    dataset = dataset.window(length+1, shift=1, drop_remainder =True)\n    dataset = dataset.flat_map(lambda window: window.batch(length+1))\n    if shuffle:\n      dataset = dataset.shuffle(buffer_size=100_000, seed=seed)\n    dataset = dataset.batch(batch_size)\n    return dataset.map(lambda window: (window[:,:-1], window[:,1:])).prefetch(1)","metadata":{"id":"IV4ry7kspnTt","trusted":true,"execution":{"iopub.status.busy":"2024-12-30T17:37:18.596545Z","iopub.execute_input":"2024-12-30T17:37:18.596778Z","iopub.status.idle":"2024-12-30T17:37:18.609710Z","shell.execute_reply.started":"2024-12-30T17:37:18.596759Z","shell.execute_reply":"2024-12-30T17:37:18.608900Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# There's just one sample in this dataset: the input represents \"to b\" and the\n# output represents \"o be\"\nlist(to_dataset(text_vec_layer([\"To be\"])[0], length=4))","metadata":{"id":"Bx0wgQbEvrXh","trusted":true,"execution":{"iopub.status.busy":"2024-12-30T17:37:18.612029Z","iopub.execute_input":"2024-12-30T17:37:18.612298Z","iopub.status.idle":"2024-12-30T17:37:18.703580Z","shell.execute_reply.started":"2024-12-30T17:37:18.612278Z","shell.execute_reply":"2024-12-30T17:37:18.702955Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"length = 100\ntf.random.set_seed(42)\n\ntrain_set = to_dataset(encoded[:1_000_000], length=100, shuffle=True, seed=42)\nvalid_set = to_dataset(encoded[1_000_000:1_060_000], length=100)\ntest_set = to_dataset(encoded[1_060_000:], length=100)\n","metadata":{"id":"QXiSTEIlrMy4","trusted":true,"execution":{"iopub.status.busy":"2024-12-30T17:37:18.704718Z","iopub.execute_input":"2024-12-30T17:37:18.705010Z","iopub.status.idle":"2024-12-30T17:37:19.211982Z","shell.execute_reply.started":"2024-12-30T17:37:18.704981Z","shell.execute_reply":"2024-12-30T17:37:19.211052Z"}},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"### Building and Training the Char-RNN Model\n","metadata":{"id":"gk63mwP6r4-c"}},{"cell_type":"code","source":"model = tf.keras.Sequential([\n    tf.keras.layers.Embedding(input_dim=n_tokens, output_dim=16),\n    tf.keras.layers.GRU(128, return_sequences=True),\n    tf.keras.layers.Dense(n_tokens, activation='softmax'),\n])\n\n\nmodel.compile(loss='sparse_categorical_crossentropy', optimizer='nadam',\n              metrics=['accuracy'])\n\nmodel_ckpt = tf.keras.callbacks.ModelCheckpoint(\n \"my_shakespeare_model.keras\", monitor=\"val_accuracy\", save_best_only=True)","metadata":{"id":"kY2pmL0JrvaP","trusted":true,"execution":{"iopub.status.busy":"2024-12-30T17:37:19.212960Z","iopub.execute_input":"2024-12-30T17:37:19.213201Z","iopub.status.idle":"2024-12-30T17:37:19.583128Z","shell.execute_reply.started":"2024-12-30T17:37:19.213181Z","shell.execute_reply":"2024-12-30T17:37:19.582213Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"model.summary()","metadata":{"id":"X6bVn23_u1BF","outputId":"94714040-f6d2-4cd2-e803-0a2d349fe2f6","trusted":true,"execution":{"iopub.status.busy":"2024-12-30T17:37:19.583973Z","iopub.execute_input":"2024-12-30T17:37:19.584274Z","iopub.status.idle":"2024-12-30T17:37:19.602784Z","shell.execute_reply.started":"2024-12-30T17:37:19.584238Z","shell.execute_reply":"2024-12-30T17:37:19.601963Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"history = model.fit(train_set, validation_data=valid_set, epochs=1,\n callbacks=[model_ckpt])\n","metadata":{"id":"sRLAF770uRUs","outputId":"bda73fdd-b3a4-4192-9828-aa3af0f02100","trusted":true,"execution":{"iopub.status.busy":"2024-12-30T17:37:19.603725Z","iopub.execute_input":"2024-12-30T17:37:19.604002Z","iopub.status.idle":"2024-12-30T17:42:04.888880Z","shell.execute_reply.started":"2024-12-30T17:37:19.603976Z","shell.execute_reply":"2024-12-30T17:42:04.888056Z"}},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"#### shakespeare model","metadata":{"id":"KJ9F9K3zubMQ"}},{"cell_type":"code","source":"shakespeare_model = tf.keras.Sequential([\n    text_vec_layer,\n    tf.keras.layers.Lambda(lambda X:X-2),\n    model\n])","metadata":{"id":"zsJbgOituYIJ","trusted":true,"execution":{"iopub.status.busy":"2024-12-30T17:42:04.889683Z","iopub.execute_input":"2024-12-30T17:42:04.889895Z","iopub.status.idle":"2024-12-30T17:42:05.108865Z","shell.execute_reply.started":"2024-12-30T17:42:04.889874Z","shell.execute_reply":"2024-12-30T17:42:05.108150Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# or we can the pretrained model\nurl = \"https://github.com/ageron/data/raw/main/shakespeare_model.tgz\"\npath = tf.keras.utils.get_file(\"shakespeare_model.tgz\", url, extract=True)\nmodel_path = Path(path).with_name(\"shakespeare_model\")\nshakespeare_model = tf.keras.models.load_model(model_path)","metadata":{"id":"9lowGE-YxtwJ","outputId":"6171cba6-800a-40ba-bcbf-356f0a552ed2","trusted":true,"execution":{"iopub.status.busy":"2024-12-30T17:42:05.109709Z","iopub.execute_input":"2024-12-30T17:42:05.110013Z","iopub.status.idle":"2024-12-30T17:42:07.704277Z","shell.execute_reply.started":"2024-12-30T17:42:05.109980Z","shell.execute_reply":"2024-12-30T17:42:07.703643Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"model_path","metadata":{"id":"xshpDtK20oEF","outputId":"f3d36e88-420d-436c-bf96-bc6e401cf2d2","trusted":true,"execution":{"iopub.status.busy":"2024-12-30T17:42:07.713655Z","iopub.execute_input":"2024-12-30T17:42:07.713995Z","iopub.status.idle":"2024-12-30T17:42:07.719110Z","shell.execute_reply.started":"2024-12-30T17:42:07.713964Z","shell.execute_reply":"2024-12-30T17:42:07.718271Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"shakespeare_model.summary()","metadata":{"id":"0zUAgHRFuxl5","outputId":"e67d4541-e9d2-4ed8-a969-3461342b5824","trusted":true,"execution":{"iopub.status.busy":"2024-12-30T17:42:07.719932Z","iopub.execute_input":"2024-12-30T17:42:07.720207Z","iopub.status.idle":"2024-12-30T17:42:07.749346Z","shell.execute_reply.started":"2024-12-30T17:42:07.720185Z","shell.execute_reply":"2024-12-30T17:42:07.748737Z"}},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"predicting next character:","metadata":{"id":"D84edM0eu8M_"}},{"cell_type":"code","source":"y_propas = shakespeare_model.predict(['To be or not to b'])[0]\nprint( y_propas[-1].shape,'\\n\\n', y_propas[-1],'\\n')\npredicted_char = tf.argmax(y_propas[-1])\nprint(predicted_char+2) # predicted character index + 2 to map the original char again\n\nprint(\"predicted character is: \", text_vec_layer.get_vocabulary()[predicted_char+2])","metadata":{"id":"SDkWyotsuxih","trusted":true,"execution":{"iopub.status.busy":"2024-12-30T17:42:07.750080Z","iopub.execute_input":"2024-12-30T17:42:07.750341Z","iopub.status.idle":"2024-12-30T17:42:08.196723Z","shell.execute_reply.started":"2024-12-30T17:42:07.750313Z","shell.execute_reply":"2024-12-30T17:42:08.196027Z"}},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"### Generating Shakespear 'FAKE' text","metadata":{}},{"cell_type":"markdown","source":"Instead of using greedy decoding (predict next character and add it to the current text and use all to predict next char and so on.)\n\nThe previous approach lead to repeated words.\n\nWe can use random sampling (with keeping the estimated propabilty of the model prediction when sampling)\n","metadata":{}},{"cell_type":"code","source":"tf.random.set_seed(412)\n\nlog_propas = tf.math.log([[.5,.4,.1]]) # simulate the logits\nprint(\"> logits: \", log_propas)\n\nprint(\"> Sampling results: \", tf.random.categorical(log_propas, num_samples=8))","metadata":{"id":"7HEbXQd_uxcn","trusted":true,"execution":{"iopub.status.busy":"2024-12-30T17:42:08.198654Z","iopub.execute_input":"2024-12-30T17:42:08.198891Z","iopub.status.idle":"2024-12-30T17:42:08.324829Z","shell.execute_reply.started":"2024-12-30T17:42:08.198871Z","shell.execute_reply":"2024-12-30T17:42:08.323920Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"tf.random.categorical(log_propas, num_samples=1).numpy()[0,0]","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2024-12-30T17:42:08.326160Z","iopub.execute_input":"2024-12-30T17:42:08.326525Z","iopub.status.idle":"2024-12-30T17:42:08.332960Z","shell.execute_reply.started":"2024-12-30T17:42:08.326494Z","shell.execute_reply":"2024-12-30T17:42:08.332224Z"}},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"we can take the control over the generated diversity of the text using `temperature`\n\n> high values indicates creativity\n> \n> low values indicates precision","metadata":{}},{"cell_type":"code","source":"def next_char(text, temperature=1):\n    y_propas = shakespeare_model.predict([text], verbose=0)[0,-1:]\n    y_propas = tf.math.log(y_propas) / temperature\n    predicted_ind = tf.random.categorical(y_propas, num_samples=1).numpy()[0,0]\n    return text_vec_layer.get_vocabulary()[predicted_ind+2]\n\ndef generate(text, n_char=50, temperature=1):\n    for _ in range(n_char):\n        text += next_char(text, temperature)\n    return text\n","metadata":{"id":"AAJY317euww4","trusted":true,"execution":{"iopub.status.busy":"2024-12-30T17:42:08.333687Z","iopub.execute_input":"2024-12-30T17:42:08.333930Z","iopub.status.idle":"2024-12-30T17:42:08.345952Z","shell.execute_reply.started":"2024-12-30T17:42:08.333910Z","shell.execute_reply":"2024-12-30T17:42:08.345080Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"tf.random.set_seed(42)\nprint(generate('to be or not to b', temperature=0.01),end= '\\n'+'='*50+'\\n')\nprint(generate('to be or not to b', temperature=1),end= '\\n'+'='*50+'\\n')\nprint(generate('to be or not to b', temperature=199),end= '\\n'+'='*50+'\\n')","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2024-12-30T17:42:08.346770Z","iopub.execute_input":"2024-12-30T17:42:08.347050Z","iopub.status.idle":"2024-12-30T17:42:18.716411Z","shell.execute_reply.started":"2024-12-30T17:42:08.347016Z","shell.execute_reply":"2024-12-30T17:42:18.715467Z"}},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"### Sampling from top k charcters","metadata":{}},{"cell_type":"code","source":"def next_char(text, model, text_vec_layer, k=5, temperature=1.0):\n    y_probs = model.predict([text], verbose=0)[0, -1:]\n    y_probs = tf.math.log(y_probs) / temperature\n    top_k_indices = tf.math.top_k(y_probs, k=k).indices\n    top_k_probs = tf.gather(y_probs, top_k_indices, axis=-1)\n    top_k_probs = tf.reshape(top_k_probs, (1, -1))\n    predicted_idx = tf.random.categorical(top_k_probs, num_samples=1)[0, 0]\n    char_idx = top_k_indices[0].numpy()[predicted_idx]+2\n    return text_vec_layer.get_vocabulary()[char_idx]\n\ndef generate(text, model, text_vec_layer, n_chars=50, k=5, temperature=1.0):\n    generated_text = text\n    for _ in range(n_chars):\n        generated_text += next_char(generated_text, model, text_vec_layer, k, temperature)\n    return generated_text","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2024-12-30T17:42:18.717382Z","iopub.execute_input":"2024-12-30T17:42:18.717736Z","iopub.status.idle":"2024-12-30T17:42:18.723589Z","shell.execute_reply.started":"2024-12-30T17:42:18.717701Z","shell.execute_reply":"2024-12-30T17:42:18.722785Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"tf.random.set_seed(42)\ngenerated_text = generate(\"To be or not to b\", shakespeare_model, text_vec_layer, n_chars=100, k=5, temperature=0.7)\nprint(generated_text)\nprint('-'*60,'\\n\\n')\n\ngenerated_text = generate(\"To be or not to b\", shakespeare_model, text_vec_layer, n_chars=100, k=5, temperature=0.001)\nprint(generated_text)\nprint('-'*60,'\\n\\n')\n\ngenerated_text = generate(\"To be or not to b\", shakespeare_model, text_vec_layer, n_chars=100, k=5, temperature=60)\nprint(generated_text)\nprint('-'*60,'\\n\\n')","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2024-12-30T17:42:18.724459Z","iopub.execute_input":"2024-12-30T17:42:18.724736Z","iopub.status.idle":"2024-12-30T17:42:40.214153Z","shell.execute_reply.started":"2024-12-30T17:42:18.724704Z","shell.execute_reply":"2024-12-30T17:42:40.213363Z"}},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"### Nucleus Sampling and Beam Search Generation","metadata":{}},{"cell_type":"code","source":"def generate_beam_search(text, max_length=50, beam_width=3, temperature=1.0):\n    beams = [(0.0, text)]\n    completed_beams = []\n    \n    for _ in range(max_length):\n        candidates = []\n        for score, beam_text in beams:\n            y_probs = shakespeare_model.predict([beam_text], verbose=0)[0, -1:]\n            logits = tf.math.log(y_probs) / temperature\n            top_k_logits, top_k_indices = tf.math.top_k(logits, k=beam_width)\n            \n            for logit, token_idx in zip(top_k_logits[0], top_k_indices[0]):\n                next_char = text_vec_layer.get_vocabulary()[token_idx.numpy() + 2]\n                new_text = beam_text + next_char\n                new_score = score - float(logit)\n                candidates.append((new_score, new_text))\n        \n        beams = sorted(candidates, key=lambda x: x[0])[:beam_width] # cut off top candidates after each generation step.\n    \n    return beams[0][1]\n\ndef generate_nucleus(text, max_length=50, p=0.9, temperature=1.0):\n    result = text\n    \n    for _ in range(max_length):\n        y_probs = shakespeare_model.predict([result], verbose=0)[0, -1:]\n        logits = tf.math.log(y_probs) / temperature\n        probs = tf.nn.softmax(logits, axis=-1)[0]\n        sorted_indices = tf.argsort(probs, direction='DESCENDING')\n        sorted_probs = tf.gather(probs, sorted_indices)\n        cumulative_probs = tf.cumsum(sorted_probs)\n        nucleus_mask = cumulative_probs <= p\n        filtered_probs = sorted_probs * tf.cast(nucleus_mask, tf.float32)\n        filtered_probs = filtered_probs / tf.reduce_sum(filtered_probs)\n        sample_idx = tf.random.categorical(tf.math.log(filtered_probs[None, :]), num_samples=1)[0, 0]\n        char_idx = sorted_indices[sample_idx]\n        next_char = text_vec_layer.get_vocabulary()[char_idx.numpy() + 2]\n        result += next_char\n    \n    return result","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2024-12-30T17:59:12.862042Z","iopub.execute_input":"2024-12-30T17:59:12.862348Z","iopub.status.idle":"2024-12-30T17:59:12.870995Z","shell.execute_reply.started":"2024-12-30T17:59:12.862325Z","shell.execute_reply":"2024-12-30T17:59:12.869997Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"print(\"-------------------Beam Search (conservative)-------------------\")\nprint(generate_beam_search(\"to be or not to b\", max_length=50, beam_width=3, temperature=0.7))\nprint(\"=\"*64)\n\nprint(\"\\n---------------------Beam Search (standard)---------------------\")\nprint(generate_beam_search(\"to be or not to b\", max_length=50, beam_width=5, temperature=1.0))\nprint(\"=\"*64)\n\nprint(\"\\n-------------------Nucleus Sampling (focused)-------------------\")\nprint(generate_nucleus(\"to be or not to b\", max_length=50, p=0.9, temperature=0.7))\nprint(\"=\"*64)\n\nprint(\"\\n-------------------Nucleus Sampling (creative)------------------\")\nprint(generate_nucleus(\"to be or not to b\", max_length=50, p=0.95, temperature=1.3))","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2024-12-30T18:00:30.033536Z","iopub.execute_input":"2024-12-30T18:00:30.033873Z","iopub.status.idle":"2024-12-30T18:01:06.348556Z","shell.execute_reply.started":"2024-12-30T18:00:30.033844Z","shell.execute_reply":"2024-12-30T18:01:06.347781Z"}},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"###  Stateful RNN","metadata":{}},{"cell_type":"markdown","source":"Preparing the dataset for statefull rnn, it must takes sequential and non-overlaping dataset rather than shuffled and overlapped dataset for stateless rnn.","metadata":{}},{"cell_type":"code","source":"def to_dataset_for_stateful_rnn(sequence, length):\n    ds = tf.data.Dataset.from_tensor_slices(sequence)\n    ds = ds.window(length+1, shift = length, drop_remainder=True)\n    ds = ds.flat_map(lambda window: window.batch(length+1)).batch(1)\n    return ds.map(lambda window: (window[:,:-1], window[:,1:])).prefetch(1)\n\nstateful_train_set = to_dataset_for_stateful_rnn(encoded[:1_000_000], length)\nstateful_valid_set = to_dataset_for_stateful_rnn(encoded[1_000_000:1_060_000],length)\nstateful_test_set = to_dataset_for_stateful_rnn(encoded[1_060_000:], length)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2024-12-30T18:23:29.088867Z","iopub.execute_input":"2024-12-30T18:23:29.089146Z","iopub.status.idle":"2024-12-30T18:23:29.189206Z","shell.execute_reply.started":"2024-12-30T18:23:29.089126Z","shell.execute_reply":"2024-12-30T18:23:29.188338Z"}},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"### Using Batching with Statful RNN","metadata":{}},{"cell_type":"code","source":"def to_non_overlapping_windows(sequence, length):\n    ds = tf.data.Dataset.from_tensor_slices(sequence)\n    ds = ds.window(length + 1, shift=length, drop_remainder=True)\n    return ds.flat_map(lambda window: window.batch(length + 1))\n\ndef to_batched_dataset_for_stateful_rnn(sequence, length, batch_size=32):\n    parts = np.array_split(sequence, batch_size)\n    datasets = tuple(to_non_overlapping_windows(part, length) for part in parts) \n    ds = tf.data.Dataset.zip(datasets).map(lambda *windows: tf.stack(windows))\n    return ds.map(lambda window: (window[:, :-1], window[:, 1:])).prefetch(1)\n\nstateful_train_set = to_batched_dataset_for_stateful_rnn(encoded[:1_000_000], length)\nstateful_valid_set = to_batched_dataset_for_stateful_rnn(encoded[1_000_000:1_060_000],length)\nstateful_test_set = to_batched_dataset_for_stateful_rnn(encoded[1_060_000:], length)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2024-12-30T18:40:04.153342Z","iopub.execute_input":"2024-12-30T18:40:04.153681Z","iopub.status.idle":"2024-12-30T18:40:05.059115Z","shell.execute_reply.started":"2024-12-30T18:40:04.153656Z","shell.execute_reply":"2024-12-30T18:40:05.058494Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"for idx, (seq, tar) in enumerate(to_batched_dataset_for_stateful_rnn(tf.range(50), length=3, batch_size=4)):\n    print('Sequence: \\n', seq, '\\nTarget: \\n', tar, '\\n\\n')\n    if idx>0: break","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2024-12-30T18:42:04.726802Z","iopub.execute_input":"2024-12-30T18:42:04.727125Z","iopub.status.idle":"2024-12-30T18:42:04.825656Z","shell.execute_reply.started":"2024-12-30T18:42:04.727098Z","shell.execute_reply":"2024-12-30T18:42:04.824789Z"}},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"#### Building the stateful model","metadata":{}},{"cell_type":"code","source":"model = tf.keras.Sequential([\n    tf.keras.layers.Embedding(input_dim=n_tokens, output_dim=16,\n                              batch_input_shape=[32, None]),\n    tf.keras.layers.GRU(128, return_sequences=True, stateful=True),\n    tf.keras.layers.Dense(n_tokens, activation=\"softmax\")\n])\n\nclass ResetStatesCallback(tf.keras.callbacks.Callback):\n    def on_epoch_begin(self, epoch, logs):\n        self.model.reset_states()\n\nmodel_ckpt = tf.keras.callbacks.ModelCheckpoint(\n    \"my_stateful_shakespeare_model.keras\",\n    monitor=\"val_accuracy\",\n    save_best_only=True)\n\nmodel.compile(loss=\"sparse_categorical_crossentropy\", optimizer=\"nadam\",\n              metrics=[\"accuracy\"])","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2024-12-30T18:48:55.845338Z","iopub.execute_input":"2024-12-30T18:48:55.845705Z","iopub.status.idle":"2024-12-30T18:48:56.064028Z","shell.execute_reply.started":"2024-12-30T18:48:55.845676Z","shell.execute_reply":"2024-12-30T18:48:56.063161Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"history = model.fit(stateful_train_set, validation_data=stateful_valid_set,\n                    epochs=20, callbacks=[ResetStatesCallback(), model_ckpt])","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2024-12-30T18:48:56.120265Z","iopub.execute_input":"2024-12-30T18:48:56.120498Z","iopub.status.idle":"2024-12-30T18:50:18.505923Z","shell.execute_reply.started":"2024-12-30T18:48:56.120477Z","shell.execute_reply":"2024-12-30T18:50:18.505266Z"}},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"To use the model with different batch sizes, we need to create a stateless copy:\n","metadata":{}},{"cell_type":"code","source":"stateless_model = tf.keras.Sequential([\n    tf.keras.layers.Embedding(input_dim=n_tokens, output_dim=16),\n    tf.keras.layers.GRU(128, return_sequences=True),\n    tf.keras.layers.Dense(n_tokens, activation=\"softmax\")\n])\n\nstateless_model.build(tf.TensorShape([None, None]))\nstateless_model.set_weights(model.get_weights())\n\nshakespeare_model = tf.keras.Sequential([\n    text_vec_layer,\n    tf.keras.layers.Lambda(lambda X: X - 2),  # no <PAD> or <UNK> tokens\n    stateless_model\n])\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2024-12-30T18:54:00.435844Z","iopub.execute_input":"2024-12-30T18:54:00.436158Z","iopub.status.idle":"2024-12-30T18:54:00.915894Z","shell.execute_reply.started":"2024-12-30T18:54:00.436135Z","shell.execute_reply":"2024-12-30T18:54:00.914908Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"generated_text = generate(\"To be or not to b\", shakespeare_model, text_vec_layer, n_chars=100, k=5, temperature=0.001)\nprint(generated_text)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2024-12-30T18:54:03.283624Z","iopub.execute_input":"2024-12-30T18:54:03.283923Z","iopub.status.idle":"2024-12-30T18:54:10.503201Z","shell.execute_reply.started":"2024-12-30T18:54:03.283900Z","shell.execute_reply":"2024-12-30T18:54:10.502471Z"}},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"## Sentiment Analysis","metadata":{}},{"cell_type":"markdown","source":"let's download the `imdb` dataset from tensorflow datasets","metadata":{}},{"cell_type":"code","source":"import tensorflow_datasets as tfds\n\nraw_train_set, raw_valid_set, raw_test_set = tfds.load('imdb_reviews', \n                                       split=['train[:90%]', 'train[90%:]', 'test'],\n                                       as_supervised=True)\ntf.random.set_seed(42)\ntrain_set = raw_train_set.shuffle(5000, seed=42).batch(32).prefetch(1)\nvalid_set = raw_valid_set.batch(32).prefetch(1)\ntest_set = raw_test_set.batch(32).prefetch(1)\n","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"for review, label in raw_train_set.take(4):\n    print(\">> \",review.numpy().decode('utf-8')[:100])\n    print(\"Label: \", label.numpy())","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-01-02T19:11:41.041756Z","iopub.execute_input":"2025-01-02T19:11:41.042125Z","iopub.status.idle":"2025-01-02T19:11:41.093349Z","shell.execute_reply.started":"2025-01-02T19:11:41.042097Z","shell.execute_reply":"2025-01-02T19:11:41.092598Z"}},"outputs":[{"name":"stdout","text":">>  This was an absolutely terrible movie. Don't be lured in by Christopher Walken or Michael Ironside. \nLabel:  0\n>>  I have been known to fall asleep during films, but this is usually due to a combination of things in\nLabel:  0\n>>  Mann photographs the Alberta Rocky Mountains in a superb fashion, and Jimmy Stewart and Walter Brenn\nLabel:  0\n>>  This is the kind of film for a snowy Sunday afternoon when the rest of the world can go ahead with i\nLabel:  1\n","output_type":"stream"}],"execution_count":16},{"cell_type":"markdown","source":"#### sentiment analysis model","metadata":{}},{"cell_type":"code","source":"vocab_size = 1000\ntext_vec_layer = tf.keras.layers.TextVectorization(max_tokens=vocab_size)\ntext_vec_layer.adapt(train_set.map(lambda review, label: review))\n\nembed_size =128\ntf.random.set_seed(42)\nmodel = tf.keras.Sequential([\n    text_vec_layer,\n    tf.keras.layers.Embedding(input_dim = vocab_size, output_dim=128),\n    tf.keras.layers.GRU(128),\n    tf.keras.layers.Dense(1, activation='sigmoid')\n\n])","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-01-02T19:52:43.787263Z","iopub.execute_input":"2025-01-02T19:52:43.787490Z","iopub.status.idle":"2025-01-02T19:52:46.761300Z","shell.execute_reply.started":"2025-01-02T19:52:43.787470Z","shell.execute_reply":"2025-01-02T19:52:46.760635Z"}},"outputs":[],"execution_count":35},{"cell_type":"code","source":"for review, label in raw_train_set.take(1):\n    print(text_vec_layer(review.numpy().decode('utf-8')))","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-01-02T19:52:46.762894Z","iopub.execute_input":"2025-01-02T19:52:46.763184Z","iopub.status.idle":"2025-01-02T19:52:46.825480Z","shell.execute_reply.started":"2025-01-02T19:52:46.763154Z","shell.execute_reply":"2025-01-02T19:52:46.824617Z"}},"outputs":[{"name":"stdout","text":"tf.Tensor(\n[ 11  14  34 410 383  18  90  28   1   8  33   1   1  41 491   1 192  24\n  86 152  19  11 218 315  28  65 241 217   8 487  54  65  86 113  95  22\n   1  11  93 644 729  11  18   7  34 396   1 171   1 404   2  88   1 137\n  67 144  52   2   1   1  67 245  65   1  16   1   1   1   1   1   1   3\n  40   1   1  17   1  14 158  19   4   1 874   1   8   4  18  12  14   1\n   5  98 146   1  10 237 688  12  48  24  93  39  11   1 152  39   1   1\n  50 403  10  95   1 863 140   9], shape=(116,), dtype=int64)\n","output_type":"stream"}],"execution_count":36},{"cell_type":"markdown","source":"note that tekens 0,1 are for unknown and padding","metadata":{}},{"cell_type":"code","source":"model.summary()","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-01-02T19:52:46.826580Z","iopub.execute_input":"2025-01-02T19:52:46.826909Z","iopub.status.idle":"2025-01-02T19:52:46.844667Z","shell.execute_reply.started":"2025-01-02T19:52:46.826877Z","shell.execute_reply":"2025-01-02T19:52:46.843752Z"}},"outputs":[{"name":"stdout","text":"Model: \"sequential_4\"\n_________________________________________________________________\n Layer (type)                Output Shape              Param #   \n=================================================================\n text_vectorization_2 (Text  (None, None)              0         \n Vectorization)                                                  \n                                                                 \n embedding_5 (Embedding)     (None, None, 128)         128000    \n                                                                 \n gru_4 (GRU)                 (None, 128)               99072     \n                                                                 \n dense_4 (Dense)             (None, 1)                 129       \n                                                                 \n=================================================================\nTotal params: 227201 (887.50 KB)\nTrainable params: 227201 (887.50 KB)\nNon-trainable params: 0 (0.00 Byte)\n_________________________________________________________________\n","output_type":"stream"}],"execution_count":37},{"cell_type":"code","source":"model.compile(loss='binary_crossentropy', optimizer='adam',metrics=['accuracy'])\nhistory1 = model.fit(train_set, validation_data=valid_set, epochs=5)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-01-02T19:52:46.845554Z","iopub.execute_input":"2025-01-02T19:52:46.845861Z","iopub.status.idle":"2025-01-02T19:55:20.815755Z","shell.execute_reply.started":"2025-01-02T19:52:46.845829Z","shell.execute_reply":"2025-01-02T19:55:20.814911Z"}},"outputs":[{"name":"stdout","text":"Epoch 1/5\n704/704 [==============================] - 38s 51ms/step - loss: 0.6935 - accuracy: 0.5011 - val_loss: 0.6932 - val_accuracy: 0.5012\nEpoch 2/5\n704/704 [==============================] - 30s 42ms/step - loss: 0.6930 - accuracy: 0.4947 - val_loss: 0.6925 - val_accuracy: 0.5052\nEpoch 3/5\n704/704 [==============================] - 29s 41ms/step - loss: 0.6927 - accuracy: 0.5008 - val_loss: 0.6943 - val_accuracy: 0.5008\nEpoch 4/5\n704/704 [==============================] - 29s 41ms/step - loss: 0.6917 - accuracy: 0.5047 - val_loss: 0.6949 - val_accuracy: 0.4996\nEpoch 5/5\n704/704 [==============================] - 29s 41ms/step - loss: 0.6900 - accuracy: 0.5062 - val_loss: 0.6964 - val_accuracy: 0.4996\n","output_type":"stream"}],"execution_count":38},{"cell_type":"markdown","source":"we notice that the model performance is very poor and this is because there's many padding tokens in the seqeuences fed to the model. which make the RNN forget about what it learned.","metadata":{}},{"cell_type":"markdown","source":"<details>\n<summary><h3>RNNs and Padding Issues Illustration (click for details)</h3></summary>\n\nLet’s break this down step by step with a simple RNN example. We’ll demonstrate how zeros (padding tokens) lead to forgetting or losing information in the sequence `\"What a goal, wow.\"`.\n\n### Initial Setup\n- **Input Sequence**: `\"What a goal, wow.\"`\n- After padding: `[\"What\", \"a\", \"goal,\", \"wow.\", 0, 0, 0, ..., 0]`\n- Assume each token is represented as an embedding vector for the RNN:\n  ```plaintext\n  [\"What\" → [1, 0.5],\n   \"a\" → [0.2, 0.1],\n   \"goal,\" → [0.9, 0.7],\n   \"wow.\" → [1.2, 0.8],\n   0 → [0, 0],  # Padding token mapped to [0, 0]\n   0 → [0, 0], ..., 0 → [0, 0]]\n  ```\n\n### RNN Computation\nFor simplicity, assume:\n- Hidden state size = 2\n- Initial hidden state: `h_0 = [0, 0]`\n- Weight matrices: `W_x`, `W_h`, and bias `b` (omitted explicit values for clarity)\n\nThe RNN computes at each timestep:\n\\[\nh_t = \\tanh(W_x \\cdot x_t + W_h \\cdot h_{t-1} + b)\n\\]\n\n#### Step-by-Step\n1. **First Token: `\"What\"`**\n   - \\( x_1 = [1, 0.5] \\)\n   - \\( h_1 = \\tanh(W_x \\cdot [1, 0.5] + W_h \\cdot [0, 0] + b) \\)\n   - Result: \\( h_1 = [0.8, 0.6] \\) (example value)\n\n2. **Second Token: `\"a\"`**\n   - \\( x_2 = [0.2, 0.1] \\)\n   - \\( h_2 = \\tanh(W_x \\cdot [0.2, 0.1] + W_h \\cdot [0.8, 0.6] + b) \\)\n   - Result: \\( h_2 = [0.7, 0.5] \\)\n\n3. **After `\"goal,\"** and `\"wow.\"**\n   - Gradually builds up meaningful context:\n     - \\( h_3 = [0.9, 0.7] \\), \\( h_4 = [1.0, 0.8] \\)\n\n4. **Padding Tokens: `0`**\n   - \\( x_5 = [0, 0] \\), \\( x_6 = [0, 0] \\), etc.\n   - For these, \\( h_t = \\tanh(W_x \\cdot [0, 0] + W_h \\cdot h_{t-1} + b) \\).\n   - Since \\( x_t = [0, 0] \\), only \\( W_h \\cdot h_{t-1} \\) contributes. However, over multiple padding steps, the hidden state \\( h_t \\) starts to decay:\n     - \\( h_5 \\approx [0.6, 0.4] \\)\n     - \\( h_6 \\approx [0.3, 0.2] \\)\n     - Eventually, \\( h_t \\approx [0, 0] \\).\n\n### Key Observations\n- **Information Loss**: The meaningful context \\( h_4 = [1.0, 0.8] \\) (derived from `\"What a goal, wow.\"`) decays to near-zero as padding dominates.\n- **Learning Challenges**: During training, the RNN might learn to ignore later timesteps entirely, assuming they don’t contain useful information.\n\n</details>","metadata":{}},{"cell_type":"markdown","source":"We can use a mask to ignore zeros during computation and training. This helps the RNN focus only on the meaningful parts of the sequence.\nThis is done by setting `mask_zero` equal to true in the embedding layer, and it propagates the mask downstream to all layers that accept it.","metadata":{}},{"cell_type":"code","source":"model = tf.keras.Sequential([\n    text_vec_layer,\n    tf.keras.layers.Embedding(input_dim = vocab_size, output_dim=128, mask_zero=True),\n    tf.keras.layers.GRU(128),\n    tf.keras.layers.Dense(1, activation='sigmoid')\n\n])\n\nmodel.compile(loss='binary_crossentropy', optimizer='adam',metrics=['accuracy'])\nhistory2 = model.fit(train_set, validation_data=valid_set, epochs=5)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-01-02T19:56:31.361572Z","iopub.execute_input":"2025-01-02T19:56:31.361900Z","iopub.status.idle":"2025-01-02T19:58:32.262597Z","shell.execute_reply.started":"2025-01-02T19:56:31.361875Z","shell.execute_reply":"2025-01-02T19:58:32.261897Z"}},"outputs":[{"name":"stdout","text":"Epoch 1/5\n704/704 [==============================] - 33s 42ms/step - loss: 0.5776 - accuracy: 0.6844 - val_loss: 1.3721 - val_accuracy: 0.5880\nEpoch 2/5\n704/704 [==============================] - 22s 32ms/step - loss: 0.4597 - accuracy: 0.7832 - val_loss: 0.3539 - val_accuracy: 0.8432\nEpoch 3/5\n704/704 [==============================] - 22s 31ms/step - loss: 0.4007 - accuracy: 0.8165 - val_loss: 0.4034 - val_accuracy: 0.8268\nEpoch 4/5\n704/704 [==============================] - 22s 30ms/step - loss: 0.3403 - accuracy: 0.8525 - val_loss: 0.3530 - val_accuracy: 0.8440\nEpoch 5/5\n704/704 [==============================] - 21s 30ms/step - loss: 0.3201 - accuracy: 0.8642 - val_loss: 0.3264 - val_accuracy: 0.8540\n","output_type":"stream"}],"execution_count":39},{"cell_type":"markdown","source":"The model now is capable to learn and performing well!","metadata":{}},{"cell_type":"markdown","source":"<details>\n<summary><h3>Why is masking needed in internal layers, and how does padding affect the input layer?</h3></summary>\n\n### Question:\n**Why do we need masking in the internal layers (not just the first one), and how does padding zeros only in the input layer affect the sequence processing?**\n\n### Answer:\n1. Propagation of Padding Effect\nEven though the first layer (e.g., an embedding or RNN) processes the padded input and replaces explicit zeros with meaningful values (e.g., the previous timestep's hidden state), those padding steps still represent \"invalid\" parts of the sequence.\nWithout a mask, internal layers may treat these invalid timesteps as meaningful, which can corrupt the learned representations.\nFor example:\n\nSuppose the input sequence is [word1, word2, 0, 0], and the RNN replaces the padding steps with the hidden state of word2.\nIf an internal RNN or dense layer operates on these outputs without masking, it may treat the repeated values from word2 as meaningful information, skewing the results.\n\n</details>","metadata":{}},{"cell_type":"code","source":"tf.random.set_seed(42)\ninputs = tf.keras.layers.Input(shape=[], dtype=tf.string)\ntoken_ids = text_vec_layer(inputs)\nmask = tf.math.not_equal(token_ids,0)\nZ = tf.keras.layers.Embedding(input_dim = vocab_size, output_dim=128)(token_ids)\nZ = tf.keras.layers.GRU(128, dropout=.2)(Z, mask=mask)\noutputs = tf.keras.layers.Dense(1, activation='sigmoid')(Z)\nmodel = tf.keras.Model(inputs=[inputs], outputs=[outputs])\n\nmodel.compile(loss=\"binary_crossentropy\", optimizer=\"nadam\",\n              metrics=[\"accuracy\"])\nhistory3 = model.fit(train_set, validation_data=valid_set, epochs=5)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-01-02T20:37:41.435364Z","iopub.execute_input":"2025-01-02T20:37:41.435698Z","iopub.status.idle":"2025-01-02T20:39:50.309697Z","shell.execute_reply.started":"2025-01-02T20:37:41.435669Z","shell.execute_reply":"2025-01-02T20:39:50.308741Z"}},"outputs":[{"name":"stdout","text":"Epoch 1/5\n704/704 [==============================] - 38s 47ms/step - loss: 0.4781 - accuracy: 0.7601 - val_loss: 0.3498 - val_accuracy: 0.8560\nEpoch 2/5\n704/704 [==============================] - 23s 32ms/step - loss: 0.3379 - accuracy: 0.8600 - val_loss: 0.3331 - val_accuracy: 0.8616\nEpoch 3/5\n704/704 [==============================] - 23s 32ms/step - loss: 0.2903 - accuracy: 0.8810 - val_loss: 0.3047 - val_accuracy: 0.8732\nEpoch 4/5\n704/704 [==============================] - 22s 31ms/step - loss: 0.2696 - accuracy: 0.8910 - val_loss: 0.3210 - val_accuracy: 0.8728\nEpoch 5/5\n704/704 [==============================] - 22s 32ms/step - loss: 0.2546 - accuracy: 0.8962 - val_loss: 0.3149 - val_accuracy: 0.8612\n","output_type":"stream"}],"execution_count":43},{"cell_type":"markdown","source":"#### Last approach using ragged tensors","metadata":{}},{"cell_type":"code","source":"text_vec_layer_ragged = tf.keras.layers.TextVectorization(max_tokens=vocab_size, ragged=True)\ntext_vec_layer_ragged.adapt(train_set.map(lambda review, label: review))\ntext_vec_layer_ragged([\"Great movie!\", \"This is DiCaprio's best role.\"])","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-01-02T20:40:05.859487Z","iopub.execute_input":"2025-01-02T20:40:05.859801Z","iopub.status.idle":"2025-01-02T20:40:08.555019Z","shell.execute_reply.started":"2025-01-02T20:40:05.859776Z","shell.execute_reply":"2025-01-02T20:40:08.554277Z"}},"outputs":[{"execution_count":45,"output_type":"execute_result","data":{"text/plain":"<tf.RaggedTensor [[86, 18], [11, 7, 1, 116, 217]]>"},"metadata":{}}],"execution_count":45},{"cell_type":"code","source":"text_vec_layer([\"Great movie!\", \"This is DiCaprio's best role.\"])","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-01-02T20:40:12.037005Z","iopub.execute_input":"2025-01-02T20:40:12.037320Z","iopub.status.idle":"2025-01-02T20:40:12.051451Z","shell.execute_reply.started":"2025-01-02T20:40:12.037294Z","shell.execute_reply":"2025-01-02T20:40:12.050620Z"}},"outputs":[{"execution_count":46,"output_type":"execute_result","data":{"text/plain":"<tf.Tensor: shape=(2, 5), dtype=int64, numpy=\narray([[ 86,  18,   0,   0,   0],\n       [ 11,   7,   1, 116, 217]])>"},"metadata":{}}],"execution_count":46},{"cell_type":"code","source":"embed_size = 128\ntf.random.set_seed(42)\nmodel = tf.keras.Sequential([\n    text_vec_layer_ragged,\n    tf.keras.layers.Embedding(vocab_size, embed_size),\n    tf.keras.layers.GRU(128),\n    tf.keras.layers.Dense(1, activation=\"sigmoid\")\n])\nmodel.compile(loss=\"binary_crossentropy\", optimizer=\"nadam\",\n              metrics=[\"accuracy\"])\nhistory4 = model.fit(train_set, validation_data=valid_set, epochs=5)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-01-02T20:40:16.760579Z","iopub.execute_input":"2025-01-02T20:40:16.760895Z","iopub.status.idle":"2025-01-02T20:42:27.885279Z","shell.execute_reply.started":"2025-01-02T20:40:16.760869Z","shell.execute_reply":"2025-01-02T20:42:27.884570Z"}},"outputs":[{"name":"stdout","text":"Epoch 1/5\n704/704 [==============================] - 36s 48ms/step - loss: 0.5011 - accuracy: 0.7548 - val_loss: 0.3581 - val_accuracy: 0.8520\nEpoch 2/5\n704/704 [==============================] - 24s 34ms/step - loss: 0.3428 - accuracy: 0.8580 - val_loss: 0.3508 - val_accuracy: 0.8492\nEpoch 3/5\n704/704 [==============================] - 24s 34ms/step - loss: 0.2950 - accuracy: 0.8796 - val_loss: 0.3231 - val_accuracy: 0.8648\nEpoch 4/5\n704/704 [==============================] - 23s 33ms/step - loss: 0.2715 - accuracy: 0.8904 - val_loss: 0.3718 - val_accuracy: 0.8380\nEpoch 5/5\n704/704 [==============================] - 23s 33ms/step - loss: 0.2543 - accuracy: 0.8970 - val_loss: 0.3069 - val_accuracy: 0.8672\n","output_type":"stream"}],"execution_count":47},{"cell_type":"markdown","source":"#### Using TensorBoard for Embedding Visualization","metadata":{}},{"cell_type":"code","source":"embed_size = 128\ntf.random.set_seed(42)\nmodel = tf.keras.Sequential([\n    text_vec_layer_ragged,\n    tf.keras.layers.Embedding(vocab_size, embed_size),\n    tf.keras.layers.GRU(128),\n    tf.keras.layers.Dense(1, activation=\"sigmoid\")\n])\nmodel.compile(loss=\"binary_crossentropy\", optimizer=\"nadam\",\n              metrics=[\"accuracy\"])\n\n\n\n# -----------------modify TensorBoard callback to include more metadata------------\nvocab = text_vec_layer_ragged.get_vocabulary()\n\n# Create a metadata file for your words\nmetadata_file = \"metadata.tsv\"\nwith open(metadata_file, 'w') as f:\n    for word in vocab:\n        f.write(f\"{word}\\n\")\n\n# Modified TensorBoard callback\ntensorboard_callback = tf.keras.callbacks.TensorBoard(\n    log_dir=\"./logs\",\n    embeddings_freq=1,\n    embeddings_layer_names=['embedding'], # Make sure this matches your embedding layer name\n    embeddings_metadata=metadata_file,\n    update_freq='epoch'\n)\n\n#---------------------------------------------------------------------------------------\n\nhistory5 = model.fit(\n    train_set,  \n    validation_data=valid_set,  \n    epochs=5,\n    callbacks=[tensorboard_callback]\n)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-01-02T20:49:41.091842Z","iopub.execute_input":"2025-01-02T20:49:41.092171Z","iopub.status.idle":"2025-01-02T20:51:53.563184Z","shell.execute_reply.started":"2025-01-02T20:49:41.092147Z","shell.execute_reply":"2025-01-02T20:51:53.562318Z"}},"outputs":[{"name":"stdout","text":"Epoch 1/5\n704/704 [==============================] - 38s 49ms/step - loss: 0.5465 - accuracy: 0.7116 - val_loss: 0.3455 - val_accuracy: 0.8468\nEpoch 2/5\n704/704 [==============================] - 24s 34ms/step - loss: 0.3342 - accuracy: 0.8551 - val_loss: 0.3242 - val_accuracy: 0.8572\nEpoch 3/5\n704/704 [==============================] - 23s 33ms/step - loss: 0.2935 - accuracy: 0.8760 - val_loss: 0.3158 - val_accuracy: 0.8732\nEpoch 4/5\n704/704 [==============================] - 23s 33ms/step - loss: 0.2745 - accuracy: 0.8860 - val_loss: 0.3007 - val_accuracy: 0.8744\nEpoch 5/5\n704/704 [==============================] - 23s 33ms/step - loss: 0.2605 - accuracy: 0.8936 - val_loss: 0.3021 - val_accuracy: 0.8728\n","output_type":"stream"}],"execution_count":48},{"cell_type":"code","source":"from tensorboard import program\nimport webbrowser\nfrom pathlib import Path\n\n# Setup the TensorBoard notebook extension\n%load_ext tensorboard\n\n# Launch TensorBoard\n# In Kaggle, we need to use a specific port\n%tensorboard --logdir ./logs --port 6006","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"import tensorflow as tf\nimport tensorboard as tb\n\n# Get the embedding layer\nembedding_layer = model.layers[1]  # Adjust index based on your model structure\nweights = embedding_layer.get_weights()[0]\n\n# Create a summary writer\nwriter = tf.summary.create_file_writer(\"./logs/embedding\")\n\n# Write the embeddings\nwith writer.as_default():\n    tf.summary.text(\"vocab\", tf.constant(vocab), step=0)\n    tf.summary.embedding(\n        \"embedding\",\n        weights,\n        metadata=vocab,\n        step=0\n    )","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"import tensorboard as tb\n\n# Get the embedding layer weights\nembedding_layer = model.get_layer('embedding')\nweights = embedding_layer.get_weights()[0]\n\n# Create a summary writer for embeddings\nwriter = tf.summary.create_file_writer(str(log_dir / \"embedding\"))\n\n# Write the embeddings\nwith writer.as_default():\n    tf.summary.text(\"vocab\", tf.constant(vocab), step=0)\n    tf.summary.embedding(\n        \"embedding\",\n        weights,\n        metadata=vocab,\n        step=0\n    )\n\n# 9. Helper function to check nearest neighbors in embedding space\ndef find_nearest_words(word, n=5):\n    \"\"\"Find n nearest neighbors for a given word in the embedding space.\"\"\"\n    if word not in vocab:\n        return \"Word not in vocabulary\"\n    \n    word_idx = vocab.index(word)\n    word_embedding = weights[word_idx]\n    \n    # Calculate distances to all other words\n    distances = np.linalg.norm(weights - word_embedding, axis=1)\n    \n    # Get indices of nearest neighbors\n    nearest_indices = np.argsort(distances)[1:n+1]\n    \n    return [(vocab[idx], distances[idx]) for idx in nearest_indices]\n\n# Example usage:\nprint(find_nearest_words(\"good\", n=5))","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-01-02T20:52:01.843986Z","iopub.execute_input":"2025-01-02T20:52:01.844286Z","execution_failed":"2025-01-02T21:02:04.317Z"}},"outputs":[{"name":"stderr","text":"/usr/lib/python3.10/pty.py:89: RuntimeWarning: os.fork() was called. os.fork() is incompatible with multithreaded code, and JAX is multithreaded, so this will likely lead to a deadlock.\n  pid, fd = os.forkpty()\n","output_type":"stream"},{"name":"stdout","text":"2025-01-02 20:52:03.096473: E external/local_xla/xla/stream_executor/cuda/cuda_fft.cc:485] Unable to register cuFFT factory: Attempting to register factory for plugin cuFFT when one has already been registered\n2025-01-02 20:52:03.118680: E external/local_xla/xla/stream_executor/cuda/cuda_dnn.cc:8454] Unable to register cuDNN factory: Attempting to register factory for plugin cuDNN when one has already been registered\n2025-01-02 20:52:03.125158: E external/local_xla/xla/stream_executor/cuda/cuda_blas.cc:1452] Unable to register cuBLAS factory: Attempting to register factory for plugin cuBLAS when one has already been registered\nWARNING: All log messages before absl::InitializeLog() is called are written to STDERR\nI0000 00:00:1735851125.706429    8117 cuda_executor.cc:1015] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero. See more at https://github.com/torvalds/linux/blob/v6.0/Documentation/ABI/testing/sysfs-bus-pci#L344-L355\nI0000 00:00:1735851125.771418    8117 cuda_executor.cc:1015] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero. See more at https://github.com/torvalds/linux/blob/v6.0/Documentation/ABI/testing/sysfs-bus-pci#L344-L355\nI0000 00:00:1735851125.771705    8117 cuda_executor.cc:1015] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero. See more at https://github.com/torvalds/linux/blob/v6.0/Documentation/ABI/testing/sysfs-bus-pci#L344-L355\n\nNOTE: Using experimental fast data loading logic. To disable, pass\n    \"--load_fast=false\" and report issues on GitHub. More details:\n    https://github.com/tensorflow/tensorboard/issues/4784\n\nServing TensorBoard on localhost; to expose to the network, use a proxy or pass --bind_all\nTensorBoard 2.17.0 at http://localhost:6006/ (Press CTRL+C to quit)\n","output_type":"stream"}],"execution_count":null}]}