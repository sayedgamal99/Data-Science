{"metadata":{"kernelspec":{"name":"python3","display_name":"Python 3","language":"python"},"language_info":{"name":"python","version":"3.10.12","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"colab":{"provenance":[],"gpuType":"T4"},"accelerator":"GPU","kaggle":{"accelerator":"gpu","dataSources":[],"dockerImageVersionId":30823,"isInternetEnabled":true,"language":"python","sourceType":"notebook","isGpuEnabled":true}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"markdown","source":"#  Chapter 16: Natural Language Processing with RNNs and Attention","metadata":{"id":"ggVICmR_lDmx"}},{"cell_type":"code","source":"import os\nos.environ[\"TF_USE_LEGACY_KERAS\"] = \"1\" \n\nimport tensorflow as tf\nimport numpy as np\nfrom pathlib import Path\nimport tf_keras","metadata":{"id":"kGPHeBvwlDmz","trusted":true,"execution":{"iopub.status.busy":"2025-01-18T19:12:23.027642Z","iopub.execute_input":"2025-01-18T19:12:23.027847Z","iopub.status.idle":"2025-01-18T19:12:23.031729Z","shell.execute_reply.started":"2025-01-18T19:12:23.027823Z","shell.execute_reply":"2025-01-18T19:12:23.030987Z"}},"outputs":[],"execution_count":2},{"cell_type":"code","source":"\nshakespeare_url = \"https://homl.info/shakespeare\"  # shortcut URL\nfilepath = tf.keras.utils.get_file(\"shakespeare.txt\", shakespeare_url)\nwith open(filepath) as f:\n    shakespeare_text = f.read()","metadata":{"id":"IIdD6DJIlDm0","outputId":"688cc633-fb31-4ef3-c339-0097afcff21e","trusted":true,"execution":{"iopub.status.busy":"2025-01-17T14:38:46.837238Z","iopub.execute_input":"2025-01-17T14:38:46.837748Z","iopub.status.idle":"2025-01-17T14:38:48.101445Z","shell.execute_reply.started":"2025-01-17T14:38:46.837705Z","shell.execute_reply":"2025-01-17T14:38:48.100623Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"print(shakespeare_text[:80])","metadata":{"id":"D6bLc0i3nwns","outputId":"ab6e1b8e-72b3-48ba-807c-8aa00921e9f2","trusted":true,"execution":{"iopub.status.busy":"2025-01-17T14:38:48.102901Z","iopub.execute_input":"2025-01-17T14:38:48.103154Z","iopub.status.idle":"2025-01-17T14:38:48.106736Z","shell.execute_reply.started":"2025-01-17T14:38:48.103133Z","shell.execute_reply":"2025-01-17T14:38:48.105864Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"all_chars  = \"\".join(sorted(set(shakespeare_text.lower())))\nprint(all_chars)\nprint(len(all_chars))","metadata":{"id":"uN6lMOKSn-ex","outputId":"c934481e-1d9c-44ba-f4e9-b56f5e25063e","trusted":true,"execution":{"iopub.status.busy":"2025-01-17T14:38:48.108137Z","iopub.execute_input":"2025-01-17T14:38:48.108363Z","iopub.status.idle":"2025-01-17T14:38:48.141555Z","shell.execute_reply.started":"2025-01-17T14:38:48.108345Z","shell.execute_reply":"2025-01-17T14:38:48.140786Z"}},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"## Generating Shakespearean Text Using a Character RNN","metadata":{"id":"R7JKQcEOoTZa"}},{"cell_type":"markdown","source":"### Preparing Dataset for a char level rnn model","metadata":{"id":"yNXX2EUpomcj"}},{"cell_type":"markdown","source":"#### Text Vectorization","metadata":{"id":"VsEzAzNpoQ2X"}},{"cell_type":"code","source":"text_vec_layer = tf.keras.layers.TextVectorization(split='character', standardize='lower')\ntext_vec_layer.adapt([shakespeare_text])\nencoded = text_vec_layer([shakespeare_text])[0]\nencoded","metadata":{"id":"IwRa58oQoJqV","outputId":"814a5d49-050b-42c6-94b5-cf2738617efc","trusted":true,"execution":{"iopub.status.busy":"2025-01-17T14:38:48.142329Z","iopub.execute_input":"2025-01-17T14:38:48.142618Z","iopub.status.idle":"2025-01-17T14:38:50.852088Z","shell.execute_reply.started":"2025-01-17T14:38:48.142588Z","shell.execute_reply":"2025-01-17T14:38:50.851114Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"encoded -= 2 # drop 0 for padding and 1 for unkown tokens\nn_tokens = text_vec_layer.vocabulary_size()-2\ndataset_size = len(encoded)\nprint(\"n_tokens:\", n_tokens)\nprint(\"dataset_size:\", dataset_size)","metadata":{"id":"22HqryeMo9m-","outputId":"b74e3a9c-4cd4-4646-b226-3955d2f841d4","trusted":true,"execution":{"iopub.status.busy":"2025-01-17T14:38:50.853037Z","iopub.execute_input":"2025-01-17T14:38:50.853379Z","iopub.status.idle":"2025-01-17T14:38:50.861608Z","shell.execute_reply.started":"2025-01-17T14:38:50.853347Z","shell.execute_reply":"2025-01-17T14:38:50.860936Z"}},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"it is seq2seq model","metadata":{"id":"b_y5WRc6p2NB"}},{"cell_type":"code","source":"def to_dataset(sequence, length, shuffle=False, seed=None, batch_size=32):\n    dataset = tf.data.Dataset.from_tensor_slices(sequence)\n    dataset = dataset.window(length+1, shift=1, drop_remainder =True)\n    dataset = dataset.flat_map(lambda window: window.batch(length+1))\n    if shuffle:\n      dataset = dataset.shuffle(buffer_size=100_000, seed=seed)\n    dataset = dataset.batch(batch_size)\n    return dataset.map(lambda window: (window[:,:-1], window[:,1:])).prefetch(1)","metadata":{"id":"IV4ry7kspnTt","trusted":true,"execution":{"iopub.status.busy":"2025-01-17T14:38:50.862430Z","iopub.execute_input":"2025-01-17T14:38:50.862780Z","iopub.status.idle":"2025-01-17T14:38:50.877617Z","shell.execute_reply.started":"2025-01-17T14:38:50.862745Z","shell.execute_reply":"2025-01-17T14:38:50.876852Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# There's just one sample in this dataset: the input represents \"to b\" and the\n# output represents \"o be\"\nlist(to_dataset(text_vec_layer([\"To be\"])[0], length=4))","metadata":{"id":"Bx0wgQbEvrXh","trusted":true,"execution":{"iopub.status.busy":"2025-01-17T14:38:50.879367Z","iopub.execute_input":"2025-01-17T14:38:50.879639Z","iopub.status.idle":"2025-01-17T14:38:50.979403Z","shell.execute_reply.started":"2025-01-17T14:38:50.879604Z","shell.execute_reply":"2025-01-17T14:38:50.978643Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"length = 100\ntf.random.set_seed(42)\n\ntrain_set = to_dataset(encoded[:1_000_000], length=100, shuffle=True, seed=42)\nvalid_set = to_dataset(encoded[1_000_000:1_060_000], length=100)\ntest_set = to_dataset(encoded[1_060_000:], length=100)\n","metadata":{"id":"QXiSTEIlrMy4","trusted":true,"execution":{"iopub.status.busy":"2025-01-17T14:38:50.980210Z","iopub.execute_input":"2025-01-17T14:38:50.980427Z","iopub.status.idle":"2025-01-17T14:38:51.495431Z","shell.execute_reply.started":"2025-01-17T14:38:50.980409Z","shell.execute_reply":"2025-01-17T14:38:51.494788Z"}},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"### Building and Training the Char-RNN Model\n","metadata":{"id":"gk63mwP6r4-c"}},{"cell_type":"code","source":"model = tf.keras.Sequential([\n    tf.keras.layers.Embedding(input_dim=n_tokens, output_dim=16),\n    tf.keras.layers.GRU(128, return_sequences=True),\n    tf.keras.layers.Dense(n_tokens, activation='softmax'),\n])\n\n\nmodel.compile(loss='sparse_categorical_crossentropy', optimizer='nadam',\n              metrics=['accuracy'])\n\nmodel_ckpt = tf.keras.callbacks.ModelCheckpoint(\n \"my_shakespeare_model.keras\", monitor=\"val_accuracy\", save_best_only=True)","metadata":{"id":"kY2pmL0JrvaP","trusted":true,"execution":{"iopub.status.busy":"2024-12-30T17:37:19.212960Z","iopub.execute_input":"2024-12-30T17:37:19.213201Z","iopub.status.idle":"2024-12-30T17:37:19.583128Z","shell.execute_reply.started":"2024-12-30T17:37:19.213181Z","shell.execute_reply":"2024-12-30T17:37:19.582213Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"model.summary()","metadata":{"id":"X6bVn23_u1BF","outputId":"94714040-f6d2-4cd2-e803-0a2d349fe2f6","trusted":true,"execution":{"iopub.status.busy":"2024-12-30T17:37:19.583973Z","iopub.execute_input":"2024-12-30T17:37:19.584274Z","iopub.status.idle":"2024-12-30T17:37:19.602784Z","shell.execute_reply.started":"2024-12-30T17:37:19.584238Z","shell.execute_reply":"2024-12-30T17:37:19.601963Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"history = model.fit(train_set, validation_data=valid_set, epochs=1,\n callbacks=[model_ckpt])\n","metadata":{"id":"sRLAF770uRUs","outputId":"bda73fdd-b3a4-4192-9828-aa3af0f02100","trusted":true,"execution":{"iopub.status.busy":"2024-12-30T17:37:19.603725Z","iopub.execute_input":"2024-12-30T17:37:19.604002Z","iopub.status.idle":"2024-12-30T17:42:04.888880Z","shell.execute_reply.started":"2024-12-30T17:37:19.603976Z","shell.execute_reply":"2024-12-30T17:42:04.888056Z"}},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"#### shakespeare model","metadata":{"id":"KJ9F9K3zubMQ"}},{"cell_type":"code","source":"shakespeare_model = tf.keras.Sequential([\n    text_vec_layer,\n    tf.keras.layers.Lambda(lambda X:X-2),\n    model\n])","metadata":{"id":"zsJbgOituYIJ","trusted":true,"execution":{"iopub.status.busy":"2024-12-30T17:42:04.889683Z","iopub.execute_input":"2024-12-30T17:42:04.889895Z","iopub.status.idle":"2024-12-30T17:42:05.108865Z","shell.execute_reply.started":"2024-12-30T17:42:04.889874Z","shell.execute_reply":"2024-12-30T17:42:05.108150Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# or we can the pretrained model\nurl = \"https://github.com/ageron/data/raw/main/shakespeare_model.tgz\"\npath = tf.keras.utils.get_file(\"shakespeare_model.tgz\", url, extract=True)\nmodel_path = Path(path).with_name(\"shakespeare_model\")\nshakespeare_model = tf.keras.models.load_model(model_path)","metadata":{"id":"9lowGE-YxtwJ","outputId":"6171cba6-800a-40ba-bcbf-356f0a552ed2","trusted":true,"execution":{"iopub.status.busy":"2024-12-30T17:42:05.109709Z","iopub.execute_input":"2024-12-30T17:42:05.110013Z","iopub.status.idle":"2024-12-30T17:42:07.704277Z","shell.execute_reply.started":"2024-12-30T17:42:05.109980Z","shell.execute_reply":"2024-12-30T17:42:07.703643Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"model_path","metadata":{"id":"xshpDtK20oEF","outputId":"f3d36e88-420d-436c-bf96-bc6e401cf2d2","trusted":true,"execution":{"iopub.status.busy":"2024-12-30T17:42:07.713655Z","iopub.execute_input":"2024-12-30T17:42:07.713995Z","iopub.status.idle":"2024-12-30T17:42:07.719110Z","shell.execute_reply.started":"2024-12-30T17:42:07.713964Z","shell.execute_reply":"2024-12-30T17:42:07.718271Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"shakespeare_model.summary()","metadata":{"id":"0zUAgHRFuxl5","outputId":"e67d4541-e9d2-4ed8-a969-3461342b5824","trusted":true,"execution":{"iopub.status.busy":"2024-12-30T17:42:07.719932Z","iopub.execute_input":"2024-12-30T17:42:07.720207Z","iopub.status.idle":"2024-12-30T17:42:07.749346Z","shell.execute_reply.started":"2024-12-30T17:42:07.720185Z","shell.execute_reply":"2024-12-30T17:42:07.748737Z"}},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"predicting next character:","metadata":{"id":"D84edM0eu8M_"}},{"cell_type":"code","source":"y_propas = shakespeare_model.predict(['To be or not to b'])[0]\nprint( y_propas[-1].shape,'\\n\\n', y_propas[-1],'\\n')\npredicted_char = tf.argmax(y_propas[-1])\nprint(predicted_char+2) # predicted character index + 2 to map the original char again\n\nprint(\"predicted character is: \", text_vec_layer.get_vocabulary()[predicted_char+2])","metadata":{"id":"SDkWyotsuxih","trusted":true,"execution":{"iopub.status.busy":"2024-12-30T17:42:07.750080Z","iopub.execute_input":"2024-12-30T17:42:07.750341Z","iopub.status.idle":"2024-12-30T17:42:08.196723Z","shell.execute_reply.started":"2024-12-30T17:42:07.750313Z","shell.execute_reply":"2024-12-30T17:42:08.196027Z"}},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"### Generating Shakespear 'FAKE' text","metadata":{}},{"cell_type":"markdown","source":"Instead of using greedy decoding (predict next character and add it to the current text and use all to predict next char and so on.)\n\nThe previous approach lead to repeated words.\n\nWe can use random sampling (with keeping the estimated propabilty of the model prediction when sampling)\n","metadata":{}},{"cell_type":"code","source":"tf.random.set_seed(412)\n\nlog_propas = tf.math.log([[.5,.4,.1]]) # simulate the logits\nprint(\"> logits: \", log_propas)\n\nprint(\"> Sampling results: \", tf.random.categorical(log_propas, num_samples=8))","metadata":{"id":"7HEbXQd_uxcn","trusted":true,"execution":{"iopub.status.busy":"2024-12-30T17:42:08.198654Z","iopub.execute_input":"2024-12-30T17:42:08.198891Z","iopub.status.idle":"2024-12-30T17:42:08.324829Z","shell.execute_reply.started":"2024-12-30T17:42:08.198871Z","shell.execute_reply":"2024-12-30T17:42:08.323920Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"tf.random.categorical(log_propas, num_samples=1).numpy()[0,0]","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2024-12-30T17:42:08.326160Z","iopub.execute_input":"2024-12-30T17:42:08.326525Z","iopub.status.idle":"2024-12-30T17:42:08.332960Z","shell.execute_reply.started":"2024-12-30T17:42:08.326494Z","shell.execute_reply":"2024-12-30T17:42:08.332224Z"}},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"we can take the control over the generated diversity of the text using `temperature`\n\n> high values indicates creativity\n> \n> low values indicates precision","metadata":{}},{"cell_type":"code","source":"def next_char(text, temperature=1):\n    y_propas = shakespeare_model.predict([text], verbose=0)[0,-1:]\n    y_propas = tf.math.log(y_propas) / temperature\n    predicted_ind = tf.random.categorical(y_propas, num_samples=1).numpy()[0,0]\n    return text_vec_layer.get_vocabulary()[predicted_ind+2]\n\ndef generate(text, n_char=50, temperature=1):\n    for _ in range(n_char):\n        text += next_char(text, temperature)\n    return text\n","metadata":{"id":"AAJY317euww4","trusted":true,"execution":{"iopub.status.busy":"2024-12-30T17:42:08.333687Z","iopub.execute_input":"2024-12-30T17:42:08.333930Z","iopub.status.idle":"2024-12-30T17:42:08.345952Z","shell.execute_reply.started":"2024-12-30T17:42:08.333910Z","shell.execute_reply":"2024-12-30T17:42:08.345080Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"tf.random.set_seed(42)\nprint(generate('to be or not to b', temperature=0.01),end= '\\n'+'='*50+'\\n')\nprint(generate('to be or not to b', temperature=1),end= '\\n'+'='*50+'\\n')\nprint(generate('to be or not to b', temperature=199),end= '\\n'+'='*50+'\\n')","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2024-12-30T17:42:08.346770Z","iopub.execute_input":"2024-12-30T17:42:08.347050Z","iopub.status.idle":"2024-12-30T17:42:18.716411Z","shell.execute_reply.started":"2024-12-30T17:42:08.347016Z","shell.execute_reply":"2024-12-30T17:42:18.715467Z"}},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"### Sampling from top k charcters","metadata":{}},{"cell_type":"code","source":"def next_char(text, model, text_vec_layer, k=5, temperature=1.0):\n    y_probs = model.predict([text], verbose=0)[0, -1:]\n    y_probs = tf.math.log(y_probs) / temperature\n    top_k_indices = tf.math.top_k(y_probs, k=k).indices\n    top_k_probs = tf.gather(y_probs, top_k_indices, axis=-1)\n    top_k_probs = tf.reshape(top_k_probs, (1, -1))\n    predicted_idx = tf.random.categorical(top_k_probs, num_samples=1)[0, 0]\n    char_idx = top_k_indices[0].numpy()[predicted_idx]+2\n    return text_vec_layer.get_vocabulary()[char_idx]\n\ndef generate(text, model, text_vec_layer, n_chars=50, k=5, temperature=1.0):\n    generated_text = text\n    for _ in range(n_chars):\n        generated_text += next_char(generated_text, model, text_vec_layer, k, temperature)\n    return generated_text","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2024-12-30T17:42:18.717382Z","iopub.execute_input":"2024-12-30T17:42:18.717736Z","iopub.status.idle":"2024-12-30T17:42:18.723589Z","shell.execute_reply.started":"2024-12-30T17:42:18.717701Z","shell.execute_reply":"2024-12-30T17:42:18.722785Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"tf.random.set_seed(42)\ngenerated_text = generate(\"To be or not to b\", shakespeare_model, text_vec_layer, n_chars=100, k=5, temperature=0.7)\nprint(generated_text)\nprint('-'*60,'\\n\\n')\n\ngenerated_text = generate(\"To be or not to b\", shakespeare_model, text_vec_layer, n_chars=100, k=5, temperature=0.001)\nprint(generated_text)\nprint('-'*60,'\\n\\n')\n\ngenerated_text = generate(\"To be or not to b\", shakespeare_model, text_vec_layer, n_chars=100, k=5, temperature=60)\nprint(generated_text)\nprint('-'*60,'\\n\\n')","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2024-12-30T17:42:18.724459Z","iopub.execute_input":"2024-12-30T17:42:18.724736Z","iopub.status.idle":"2024-12-30T17:42:40.214153Z","shell.execute_reply.started":"2024-12-30T17:42:18.724704Z","shell.execute_reply":"2024-12-30T17:42:40.213363Z"}},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"### Nucleus Sampling and Beam Search Generation","metadata":{}},{"cell_type":"code","source":"def generate_beam_search(text, max_length=50, beam_width=3, temperature=1.0):\n    beams = [(0.0, text)]\n    completed_beams = []\n    \n    for _ in range(max_length):\n        candidates = []\n        for score, beam_text in beams:\n            y_probs = shakespeare_model.predict([beam_text], verbose=0)[0, -1:]\n            logits = tf.math.log(y_probs) / temperature\n            top_k_logits, top_k_indices = tf.math.top_k(logits, k=beam_width)\n            \n            for logit, token_idx in zip(top_k_logits[0], top_k_indices[0]):\n                next_char = text_vec_layer.get_vocabulary()[token_idx.numpy() + 2]\n                new_text = beam_text + next_char\n                new_score = score - float(logit)\n                candidates.append((new_score, new_text))\n        \n        beams = sorted(candidates, key=lambda x: x[0])[:beam_width] # cut off top candidates after each generation step.\n    \n    return beams[0][1]\n\ndef generate_nucleus(text, max_length=50, p=0.9, temperature=1.0):\n    result = text\n    \n    for _ in range(max_length):\n        y_probs = shakespeare_model.predict([result], verbose=0)[0, -1:]\n        logits = tf.math.log(y_probs) / temperature\n        probs = tf.nn.softmax(logits, axis=-1)[0]\n        sorted_indices = tf.argsort(probs, direction='DESCENDING')\n        sorted_probs = tf.gather(probs, sorted_indices)\n        cumulative_probs = tf.cumsum(sorted_probs)\n        nucleus_mask = cumulative_probs <= p\n        filtered_probs = sorted_probs * tf.cast(nucleus_mask, tf.float32)\n        filtered_probs = filtered_probs / tf.reduce_sum(filtered_probs)\n        sample_idx = tf.random.categorical(tf.math.log(filtered_probs[None, :]), num_samples=1)[0, 0]\n        char_idx = sorted_indices[sample_idx]\n        next_char = text_vec_layer.get_vocabulary()[char_idx.numpy() + 2]\n        result += next_char\n    \n    return result","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2024-12-30T17:59:12.862042Z","iopub.execute_input":"2024-12-30T17:59:12.862348Z","iopub.status.idle":"2024-12-30T17:59:12.870995Z","shell.execute_reply.started":"2024-12-30T17:59:12.862325Z","shell.execute_reply":"2024-12-30T17:59:12.869997Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"print(\"-------------------Beam Search (conservative)-------------------\")\nprint(generate_beam_search(\"to be or not to b\", max_length=50, beam_width=3, temperature=0.7))\nprint(\"=\"*64)\n\nprint(\"\\n---------------------Beam Search (standard)---------------------\")\nprint(generate_beam_search(\"to be or not to b\", max_length=50, beam_width=5, temperature=1.0))\nprint(\"=\"*64)\n\nprint(\"\\n-------------------Nucleus Sampling (focused)-------------------\")\nprint(generate_nucleus(\"to be or not to b\", max_length=50, p=0.9, temperature=0.7))\nprint(\"=\"*64)\n\nprint(\"\\n-------------------Nucleus Sampling (creative)------------------\")\nprint(generate_nucleus(\"to be or not to b\", max_length=50, p=0.95, temperature=1.3))","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2024-12-30T18:00:30.033536Z","iopub.execute_input":"2024-12-30T18:00:30.033873Z","iopub.status.idle":"2024-12-30T18:01:06.348556Z","shell.execute_reply.started":"2024-12-30T18:00:30.033844Z","shell.execute_reply":"2024-12-30T18:01:06.347781Z"}},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"###  Stateful RNN","metadata":{}},{"cell_type":"markdown","source":"Preparing the dataset for statefull rnn, it must takes sequential and non-overlaping dataset rather than shuffled and overlapped dataset for stateless rnn.","metadata":{}},{"cell_type":"code","source":"def to_dataset_for_stateful_rnn(sequence, length):\n    ds = tf.data.Dataset.from_tensor_slices(sequence)\n    ds = ds.window(length+1, shift = length, drop_remainder=True)\n    ds = ds.flat_map(lambda window: window.batch(length+1)).batch(1)\n    return ds.map(lambda window: (window[:,:-1], window[:,1:])).prefetch(1)\n\nstateful_train_set = to_dataset_for_stateful_rnn(encoded[:1_000_000], length)\nstateful_valid_set = to_dataset_for_stateful_rnn(encoded[1_000_000:1_060_000],length)\nstateful_test_set = to_dataset_for_stateful_rnn(encoded[1_060_000:], length)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2024-12-30T18:23:29.088867Z","iopub.execute_input":"2024-12-30T18:23:29.089146Z","iopub.status.idle":"2024-12-30T18:23:29.189206Z","shell.execute_reply.started":"2024-12-30T18:23:29.089126Z","shell.execute_reply":"2024-12-30T18:23:29.188338Z"}},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"### Using Batching with Statful RNN","metadata":{}},{"cell_type":"code","source":"def to_non_overlapping_windows(sequence, length):\n    ds = tf.data.Dataset.from_tensor_slices(sequence)\n    ds = ds.window(length + 1, shift=length, drop_remainder=True)\n    return ds.flat_map(lambda window: window.batch(length + 1))\n\ndef to_batched_dataset_for_stateful_rnn(sequence, length, batch_size=32):\n    parts = np.array_split(sequence, batch_size)\n    datasets = tuple(to_non_overlapping_windows(part, length) for part in parts) \n    ds = tf.data.Dataset.zip(datasets).map(lambda *windows: tf.stack(windows))\n    return ds.map(lambda window: (window[:, :-1], window[:, 1:])).prefetch(1)\n\nstateful_train_set = to_batched_dataset_for_stateful_rnn(encoded[:1_000_000], length)\nstateful_valid_set = to_batched_dataset_for_stateful_rnn(encoded[1_000_000:1_060_000],length)\nstateful_test_set = to_batched_dataset_for_stateful_rnn(encoded[1_060_000:], length)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2024-12-30T18:40:04.153342Z","iopub.execute_input":"2024-12-30T18:40:04.153681Z","iopub.status.idle":"2024-12-30T18:40:05.059115Z","shell.execute_reply.started":"2024-12-30T18:40:04.153656Z","shell.execute_reply":"2024-12-30T18:40:05.058494Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"for idx, (seq, tar) in enumerate(to_batched_dataset_for_stateful_rnn(tf.range(50), length=3, batch_size=4)):\n    print('Sequence: \\n', seq, '\\nTarget: \\n', tar, '\\n\\n')\n    if idx>0: break","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2024-12-30T18:42:04.726802Z","iopub.execute_input":"2024-12-30T18:42:04.727125Z","iopub.status.idle":"2024-12-30T18:42:04.825656Z","shell.execute_reply.started":"2024-12-30T18:42:04.727098Z","shell.execute_reply":"2024-12-30T18:42:04.824789Z"}},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"#### Building the stateful model","metadata":{}},{"cell_type":"code","source":"model = tf.keras.Sequential([\n    tf.keras.layers.Embedding(input_dim=n_tokens, output_dim=16,\n                              batch_input_shape=[32, None]),\n    tf.keras.layers.GRU(128, return_sequences=True, stateful=True),\n    tf.keras.layers.Dense(n_tokens, activation=\"softmax\")\n])\n\nclass ResetStatesCallback(tf.keras.callbacks.Callback):\n    def on_epoch_begin(self, epoch, logs):\n        self.model.reset_states()\n\nmodel_ckpt = tf.keras.callbacks.ModelCheckpoint(\n    \"my_stateful_shakespeare_model.keras\",\n    monitor=\"val_accuracy\",\n    save_best_only=True)\n\nmodel.compile(loss=\"sparse_categorical_crossentropy\", optimizer=\"nadam\",\n              metrics=[\"accuracy\"])","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2024-12-30T18:48:55.845338Z","iopub.execute_input":"2024-12-30T18:48:55.845705Z","iopub.status.idle":"2024-12-30T18:48:56.064028Z","shell.execute_reply.started":"2024-12-30T18:48:55.845676Z","shell.execute_reply":"2024-12-30T18:48:56.063161Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"history = model.fit(stateful_train_set, validation_data=stateful_valid_set,\n                    epochs=20, callbacks=[ResetStatesCallback(), model_ckpt])","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2024-12-30T18:48:56.120265Z","iopub.execute_input":"2024-12-30T18:48:56.120498Z","iopub.status.idle":"2024-12-30T18:50:18.505923Z","shell.execute_reply.started":"2024-12-30T18:48:56.120477Z","shell.execute_reply":"2024-12-30T18:50:18.505266Z"}},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"To use the model with different batch sizes, we need to create a stateless copy:\n","metadata":{}},{"cell_type":"code","source":"stateless_model = tf.keras.Sequential([\n    tf.keras.layers.Embedding(input_dim=n_tokens, output_dim=16),\n    tf.keras.layers.GRU(128, return_sequences=True),\n    tf.keras.layers.Dense(n_tokens, activation=\"softmax\")\n])\n\nstateless_model.build(tf.TensorShape([None, None]))\nstateless_model.set_weights(model.get_weights())\n\nshakespeare_model = tf.keras.Sequential([\n    text_vec_layer,\n    tf.keras.layers.Lambda(lambda X: X - 2),  # no <PAD> or <UNK> tokens\n    stateless_model\n])\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2024-12-30T18:54:00.435844Z","iopub.execute_input":"2024-12-30T18:54:00.436158Z","iopub.status.idle":"2024-12-30T18:54:00.915894Z","shell.execute_reply.started":"2024-12-30T18:54:00.436135Z","shell.execute_reply":"2024-12-30T18:54:00.914908Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"generated_text = generate(\"To be or not to b\", shakespeare_model, text_vec_layer, n_chars=100, k=5, temperature=0.001)\nprint(generated_text)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2024-12-30T18:54:03.283624Z","iopub.execute_input":"2024-12-30T18:54:03.283923Z","iopub.status.idle":"2024-12-30T18:54:10.503201Z","shell.execute_reply.started":"2024-12-30T18:54:03.283900Z","shell.execute_reply":"2024-12-30T18:54:10.502471Z"}},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"## Sentiment Analysis","metadata":{}},{"cell_type":"markdown","source":"let's download the `imdb` dataset from tensorflow datasets","metadata":{}},{"cell_type":"code","source":"import tensorflow_datasets as tfds\n\nraw_train_set, raw_valid_set, raw_test_set = tfds.load('imdb_reviews', \n                                       split=['train[:90%]', 'train[90%:]', 'test'],\n                                       as_supervised=True)\ntf.random.set_seed(42)\ntrain_set = raw_train_set.shuffle(5000, seed=42).batch(32).prefetch(1)\nvalid_set = raw_valid_set.batch(32).prefetch(1)\ntest_set = raw_test_set.batch(32).prefetch(1)\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-01-17T14:41:17.245973Z","iopub.execute_input":"2025-01-17T14:41:17.246283Z","iopub.status.idle":"2025-01-17T14:41:50.808730Z","shell.execute_reply.started":"2025-01-17T14:41:17.246260Z","shell.execute_reply":"2025-01-17T14:41:50.808002Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"for review, label in raw_train_set.take(4):\n    print(\">> \",review.numpy().decode('utf-8')[:100])\n    print(\"Label: \", label.numpy())","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-01-17T14:41:50.809695Z","iopub.execute_input":"2025-01-17T14:41:50.810023Z","iopub.status.idle":"2025-01-17T14:41:50.865767Z","shell.execute_reply.started":"2025-01-17T14:41:50.810000Z","shell.execute_reply":"2025-01-17T14:41:50.864922Z"}},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"#### sentiment analysis model","metadata":{}},{"cell_type":"code","source":"vocab_size = 1000\ntext_vec_layer = tf.keras.layers.TextVectorization(max_tokens=vocab_size)\ntext_vec_layer.adapt(train_set.map(lambda review, label: review))\n\nembed_size =128\ntf.random.set_seed(42)\nmodel = tf.keras.Sequential([\n    text_vec_layer,\n    tf.keras.layers.Embedding(input_dim = vocab_size, output_dim=128),\n    tf.keras.layers.GRU(128),\n    tf.keras.layers.Dense(1, activation='sigmoid')\n\n])","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-01-17T14:41:50.867430Z","iopub.execute_input":"2025-01-17T14:41:50.867668Z","iopub.status.idle":"2025-01-17T14:41:54.350932Z","shell.execute_reply.started":"2025-01-17T14:41:50.867649Z","shell.execute_reply":"2025-01-17T14:41:54.349931Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"for review, label in raw_train_set.take(1):\n    print(text_vec_layer(review.numpy().decode('utf-8')))","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-01-17T14:41:54.352003Z","iopub.execute_input":"2025-01-17T14:41:54.352215Z","iopub.status.idle":"2025-01-17T14:41:54.436084Z","shell.execute_reply.started":"2025-01-17T14:41:54.352195Z","shell.execute_reply":"2025-01-17T14:41:54.435210Z"}},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"note that tekens 0,1 are for unknown and padding","metadata":{}},{"cell_type":"code","source":"model.summary()","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-01-17T14:41:54.437092Z","iopub.execute_input":"2025-01-17T14:41:54.437426Z","iopub.status.idle":"2025-01-17T14:41:54.456672Z","shell.execute_reply.started":"2025-01-17T14:41:54.437393Z","shell.execute_reply":"2025-01-17T14:41:54.456026Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"model.compile(loss='binary_crossentropy', optimizer='adam',metrics=['accuracy'])\nhistory1 = model.fit(train_set, validation_data=valid_set, epochs=5)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-01-17T14:41:54.457601Z","iopub.execute_input":"2025-01-17T14:41:54.457901Z","iopub.status.idle":"2025-01-17T14:43:10.244667Z","shell.execute_reply.started":"2025-01-17T14:41:54.457872Z","shell.execute_reply":"2025-01-17T14:43:10.243543Z"}},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"we notice that the model performance is very poor and this is because there's many padding tokens in the seqeuences fed to the model. which make the RNN forget about what it learned.","metadata":{}},{"cell_type":"markdown","source":"<details>\n<summary><h3>RNNs and Padding Issues Illustration (click for details)</h3></summary>\n\nLet’s break this down step by step with a simple RNN example. We’ll demonstrate how zeros (padding tokens) lead to forgetting or losing information in the sequence `\"What a goal, wow.\"`.\n\n### Initial Setup\n- **Input Sequence**: `\"What a goal, wow.\"`\n- After padding: `[\"What\", \"a\", \"goal,\", \"wow.\", 0, 0, 0, ..., 0]`\n- Assume each token is represented as an embedding vector for the RNN:\n  ```plaintext\n  [\"What\" → [1, 0.5],\n   \"a\" → [0.2, 0.1],\n   \"goal,\" → [0.9, 0.7],\n   \"wow.\" → [1.2, 0.8],\n   0 → [0, 0],  # Padding token mapped to [0, 0]\n   0 → [0, 0], ..., 0 → [0, 0]]\n  ```\n\n### RNN Computation\nFor simplicity, assume:\n- Hidden state size = 2\n- Initial hidden state: `h_0 = [0, 0]`\n- Weight matrices: `W_x`, `W_h`, and bias `b` (omitted explicit values for clarity)\n\nThe RNN computes at each timestep:\n\\[\nh_t = \\tanh(W_x \\cdot x_t + W_h \\cdot h_{t-1} + b)\n\\]\n\n#### Step-by-Step\n1. **First Token: `\"What\"`**\n   - \\( x_1 = [1, 0.5] \\)\n   - \\( h_1 = \\tanh(W_x \\cdot [1, 0.5] + W_h \\cdot [0, 0] + b) \\)\n   - Result: \\( h_1 = [0.8, 0.6] \\) (example value)\n\n2. **Second Token: `\"a\"`**\n   - \\( x_2 = [0.2, 0.1] \\)\n   - \\( h_2 = \\tanh(W_x \\cdot [0.2, 0.1] + W_h \\cdot [0.8, 0.6] + b) \\)\n   - Result: \\( h_2 = [0.7, 0.5] \\)\n\n3. **After `\"goal,\"** and `\"wow.\"**\n   - Gradually builds up meaningful context:\n     - \\( h_3 = [0.9, 0.7] \\), \\( h_4 = [1.0, 0.8] \\)\n\n4. **Padding Tokens: `0`**\n   - \\( x_5 = [0, 0] \\), \\( x_6 = [0, 0] \\), etc.\n   - For these, \\( h_t = \\tanh(W_x \\cdot [0, 0] + W_h \\cdot h_{t-1} + b) \\).\n   - Since \\( x_t = [0, 0] \\), only \\( W_h \\cdot h_{t-1} \\) contributes. However, over multiple padding steps, the hidden state \\( h_t \\) starts to decay:\n     - \\( h_5 \\approx [0.6, 0.4] \\)\n     - \\( h_6 \\approx [0.3, 0.2] \\)\n     - Eventually, \\( h_t \\approx [0, 0] \\).\n\n### Key Observations\n- **Information Loss**: The meaningful context \\( h_4 = [1.0, 0.8] \\) (derived from `\"What a goal, wow.\"`) decays to near-zero as padding dominates.\n- **Learning Challenges**: During training, the RNN might learn to ignore later timesteps entirely, assuming they don’t contain useful information.\n\n</details>","metadata":{}},{"cell_type":"markdown","source":"We can use a mask to ignore zeros during computation and training. This helps the RNN focus only on the meaningful parts of the sequence.\nThis is done by setting `mask_zero` equal to true in the embedding layer, and it propagates the mask downstream to all layers that accept it.","metadata":{}},{"cell_type":"code","source":"model = tf.keras.Sequential([\n    text_vec_layer,\n    tf.keras.layers.Embedding(input_dim = vocab_size, output_dim=128, mask_zero=True),\n    tf.keras.layers.GRU(128),\n    tf.keras.layers.Dense(1, activation='sigmoid')\n\n])\n\nmodel.compile(loss='binary_crossentropy', optimizer='adam',metrics=['accuracy'])\nhistory2 = model.fit(train_set, validation_data=valid_set, epochs=5)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-01-17T14:43:18.051006Z","iopub.execute_input":"2025-01-17T14:43:18.051340Z","iopub.status.idle":"2025-01-17T14:45:21.143301Z","shell.execute_reply.started":"2025-01-17T14:43:18.051314Z","shell.execute_reply":"2025-01-17T14:45:21.142567Z"}},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"The model now is capable to learn and performing well!","metadata":{}},{"cell_type":"markdown","source":"<details>\n<summary><h3>Why is masking needed in internal layers, and how does padding affect the input layer?</h3></summary>\n\n### Question:\n**Why do we need masking in the internal layers (not just the first one), and how does padding zeros only in the input layer affect the sequence processing?**\n\n### Answer:\n1. Propagation of Padding Effect\nEven though the first layer (e.g., an embedding or RNN) processes the padded input and replaces explicit zeros with meaningful values (e.g., the previous timestep's hidden state), those padding steps still represent \"invalid\" parts of the sequence.\nWithout a mask, internal layers may treat these invalid timesteps as meaningful, which can corrupt the learned representations.\nFor example:\n\nSuppose the input sequence is [word1, word2, 0, 0], and the RNN replaces the padding steps with the hidden state of word2.\nIf an internal RNN or dense layer operates on these outputs without masking, it may treat the repeated values from word2 as meaningful information, skewing the results.\n\n</details>","metadata":{}},{"cell_type":"code","source":"tf.random.set_seed(42)\ninputs = tf.keras.layers.Input(shape=[], dtype=tf.string)\ntoken_ids = text_vec_layer(inputs)\nmask = tf.math.not_equal(token_ids,0)\nZ = tf.keras.layers.Embedding(input_dim = vocab_size, output_dim=128)(token_ids)\nZ = tf.keras.layers.GRU(128, dropout=.2)(Z, mask=mask)\noutputs = tf.keras.layers.Dense(1, activation='sigmoid')(Z)\nmodel = tf.keras.Model(inputs=[inputs], outputs=[outputs])\n\nmodel.compile(loss=\"binary_crossentropy\", optimizer=\"nadam\",\n              metrics=[\"accuracy\"])\nhistory3 = model.fit(train_set, validation_data=valid_set, epochs=5)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-01-02T20:37:41.435364Z","iopub.execute_input":"2025-01-02T20:37:41.435698Z","iopub.status.idle":"2025-01-02T20:39:50.309697Z","shell.execute_reply.started":"2025-01-02T20:37:41.435669Z","shell.execute_reply":"2025-01-02T20:39:50.308741Z"}},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"#### Last approach using ragged tensors","metadata":{}},{"cell_type":"code","source":"text_vec_layer_ragged = tf.keras.layers.TextVectorization(max_tokens=vocab_size, ragged=True)\ntext_vec_layer_ragged.adapt(train_set.map(lambda review, label: review))\ntext_vec_layer_ragged([\"Great movie!\", \"This is DiCaprio's best role.\"])","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-01-02T21:38:46.951654Z","iopub.execute_input":"2025-01-02T21:38:46.951946Z","iopub.status.idle":"2025-01-02T21:38:49.661941Z","shell.execute_reply.started":"2025-01-02T21:38:46.951925Z","shell.execute_reply":"2025-01-02T21:38:49.661241Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"text_vec_layer([\"Great movie!\", \"This is DiCaprio's best role.\"])","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-01-02T21:38:49.662900Z","iopub.execute_input":"2025-01-02T21:38:49.663214Z","iopub.status.idle":"2025-01-02T21:38:49.676423Z","shell.execute_reply.started":"2025-01-02T21:38:49.663190Z","shell.execute_reply":"2025-01-02T21:38:49.675521Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"embed_size = 128\ntf.random.set_seed(42)\nmodel = tf.keras.Sequential([\n    text_vec_layer_ragged,\n    tf.keras.layers.Embedding(vocab_size, embed_size),\n    tf.keras.layers.GRU(128),\n    tf.keras.layers.Dense(1, activation=\"sigmoid\")\n])\nmodel.compile(loss=\"binary_crossentropy\", optimizer=\"nadam\",\n              metrics=[\"accuracy\"])\nhistory4 = model.fit(train_set, validation_data=valid_set, epochs=5)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-01-02T20:40:16.760579Z","iopub.execute_input":"2025-01-02T20:40:16.760895Z","iopub.status.idle":"2025-01-02T20:42:27.885279Z","shell.execute_reply.started":"2025-01-02T20:40:16.760869Z","shell.execute_reply":"2025-01-02T20:42:27.884570Z"}},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"#### Using TensorBoard for Embedding Visualization","metadata":{}},{"cell_type":"code","source":"embed_size = 128\ntf.random.set_seed(42)\nmodel = tf.keras.Sequential([\n    text_vec_layer_ragged,\n    tf.keras.layers.Embedding(vocab_size, embed_size),\n    tf.keras.layers.GRU(128),\n    tf.keras.layers.Dense(1, activation=\"sigmoid\")\n])\nmodel.compile(loss=\"binary_crossentropy\", optimizer=\"nadam\",\n              metrics=[\"accuracy\"])\n\nmodel.summary()","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-01-02T21:38:52.873980Z","iopub.execute_input":"2025-01-02T21:38:52.874344Z","iopub.status.idle":"2025-01-02T21:38:53.152697Z","shell.execute_reply.started":"2025-01-02T21:38:52.874313Z","shell.execute_reply":"2025-01-02T21:38:53.151782Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"\nvocab = text_vec_layer_ragged.get_vocabulary()\n\n# Create a metadata file for your words\nmetadata_file = \"metadata.tsv\"\nwith open(metadata_file, 'w') as f:\n    for word in vocab:\n        f.write(f\"{word}\\n\")\n\ntensorboard_callback = tf.keras.callbacks.TensorBoard(\n    log_dir=\"./logs\",\n    embeddings_freq=1,\n    embeddings_layer_names=['embedding'], \n    embeddings_metadata=metadata_file,\n    update_freq='epoch'\n)\n\n\nhistory5 = model.fit(\n    train_set,  \n    validation_data=valid_set,  \n    epochs=5,\n    callbacks=[tensorboard_callback]\n)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-01-02T21:39:09.995549Z","iopub.execute_input":"2025-01-02T21:39:09.995891Z","iopub.status.idle":"2025-01-02T21:41:25.833572Z","shell.execute_reply.started":"2025-01-02T21:39:09.995868Z","shell.execute_reply":"2025-01-02T21:41:25.832839Z"}},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"##  Reusing Pretrained Embeddings and Language Models\n","metadata":{}},{"cell_type":"code","source":"import os\nimport tensorflow_hub as hub\n\nos.environ['TFHUB_CACHE_DIR'] = \"my_tfhub_cache\"\ntf.random.set_seed(42)\n\nmodel = tf.keras.Sequential([\n    hub.KerasLayer(\"https://tfhub.dev/google/universal-sentence-encoder/4\",\n                   trainable=True, dtype=tf.string, input_shape=[]),\n    tf.keras.layers.Dense(64, activation=\"relu\"),\n    tf.keras.layers.Dense(1, activation=\"sigmoid\")\n])","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-01-17T14:46:54.257915Z","iopub.execute_input":"2025-01-17T14:46:54.258226Z","iopub.status.idle":"2025-01-17T14:47:00.314746Z","shell.execute_reply.started":"2025-01-17T14:46:54.258199Z","shell.execute_reply":"2025-01-17T14:47:00.314037Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"model.compile(loss=\"binary_crossentropy\", optimizer=\"nadam\",\n              metrics=[\"accuracy\"])\nhistory_with_pretrained = model.fit(train_set,\n                                    validation_data=valid_set, epochs=10)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-01-17T14:47:08.967218Z","iopub.execute_input":"2025-01-17T14:47:08.967529Z","iopub.status.idle":"2025-01-17T15:13:17.936638Z","shell.execute_reply.started":"2025-01-17T14:47:08.967504Z","shell.execute_reply":"2025-01-17T15:13:17.935686Z"}},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"## An Encoder–Decoder Network for Neural Machine Translation","metadata":{}},{"cell_type":"code","source":"url = \"https://storage.googleapis.com/download.tensorflow.org/data/spa-eng.zip\"\npath = tf.keras.utils.get_file('spa-eng.zip', origin=url, cache_dir='datasets', extract=True)\ntext = (Path(path).with_name(\"spa-eng\")/'spa.txt').read_text()","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-01-18T20:03:13.381969Z","iopub.execute_input":"2025-01-18T20:03:13.382334Z","iopub.status.idle":"2025-01-18T20:03:13.491419Z","shell.execute_reply.started":"2025-01-18T20:03:13.382310Z","shell.execute_reply":"2025-01-18T20:03:13.490513Z"}},"outputs":[],"execution_count":41},{"cell_type":"code","source":"import numpy as np\n\ntext = text.replace(\"¡\", \"\").replace(\"¿\", \"\")\npairs = [line.split(\"\\t\") for line in text.splitlines()]\nnp.random.seed(42)  # extra code – ensures reproducibility on CPU\nnp.random.shuffle(pairs)\nsentences_en, sentences_es = zip(*pairs)  # separates the pairs into 2 lists","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-01-18T20:03:15.142658Z","iopub.execute_input":"2025-01-18T20:03:15.142969Z","iopub.status.idle":"2025-01-18T20:03:15.888859Z","shell.execute_reply.started":"2025-01-18T20:03:15.142941Z","shell.execute_reply":"2025-01-18T20:03:15.888153Z"}},"outputs":[],"execution_count":42},{"cell_type":"code","source":"for i in range(3):\n    print(sentences_en[i], \"=>\", sentences_es[i])","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-01-18T20:03:16.927523Z","iopub.execute_input":"2025-01-18T20:03:16.927859Z","iopub.status.idle":"2025-01-18T20:03:16.932699Z","shell.execute_reply.started":"2025-01-18T20:03:16.927831Z","shell.execute_reply":"2025-01-18T20:03:16.932023Z"}},"outputs":[{"name":"stdout","text":"How boring! => Qué aburrimiento!\nI love sports. => Adoro el deporte.\nWould you like to swap jobs? => Te gustaría que intercambiemos los trabajos?\n","output_type":"stream"}],"execution_count":43},{"cell_type":"markdown","source":"#### Tokenization","metadata":{}},{"cell_type":"code","source":"vocab_size = 1000\nmax_length = 50\n\ntext_vec_layer_en = tf.keras.layers.TextVectorization(\n    vocab_size, output_sequence_length=max_length)\ntext_vec_layer_es = tf.keras.layers.TextVectorization(\n    vocab_size, output_sequence_length=max_length)\n\ntext_vec_layer_en.adapt(sentences_en)\ntext_vec_layer_es.adapt([f\"startofseq {s} endofseq\" for s in sentences_es])","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-01-18T20:03:17.511330Z","iopub.execute_input":"2025-01-18T20:03:17.511634Z","iopub.status.idle":"2025-01-18T20:03:31.580038Z","shell.execute_reply.started":"2025-01-18T20:03:17.511612Z","shell.execute_reply":"2025-01-18T20:03:31.579319Z"}},"outputs":[],"execution_count":44},{"cell_type":"code","source":"print(\"English Vocab samples: \", text_vec_layer_en.get_vocabulary()[:10])\nprint(\"Spanish Vocab samples: \", text_vec_layer_es.get_vocabulary()[:10])\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-01-18T20:03:31.581095Z","iopub.execute_input":"2025-01-18T20:03:31.581353Z","iopub.status.idle":"2025-01-18T20:03:31.590538Z","shell.execute_reply.started":"2025-01-18T20:03:31.581332Z","shell.execute_reply":"2025-01-18T20:03:31.589918Z"}},"outputs":[{"name":"stdout","text":"English Vocab samples:  ['', '[UNK]', 'the', 'i', 'to', 'you', 'tom', 'a', 'is', 'he']\nSpanish Vocab samples:  ['', '[UNK]', 'startofseq', 'endofseq', 'de', 'que', 'a', 'no', 'tom', 'la']\n","output_type":"stream"}],"execution_count":45},{"cell_type":"markdown","source":"#### Preparing Dataset ","metadata":{}},{"cell_type":"code","source":"# encoder inputs\nX_train = tf.constant(sentences_en[:100_000])\nX_valid = tf.constant(sentences_en[100_000:])\n\n# decoder inputs\nX_train_dec = tf.constant([f\"startofseq {s}\" for s in sentences_es[:100_000]])\nX_valid_dec = tf.constant([f\"startofseq {s}\" for s in sentences_es[100_000:]])\n\n# decoder outputs (targets)\nY_train = text_vec_layer_es([f\"{s} endofseq\" for s in sentences_es[:100_000]])\nY_valid = text_vec_layer_es([f\"{s} endofseq\" for s in sentences_es[100_000:]])","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-01-18T20:03:31.592220Z","iopub.execute_input":"2025-01-18T20:03:31.592449Z","iopub.status.idle":"2025-01-18T20:03:32.303633Z","shell.execute_reply.started":"2025-01-18T20:03:31.592430Z","shell.execute_reply":"2025-01-18T20:03:32.302940Z"}},"outputs":[],"execution_count":46},{"cell_type":"markdown","source":"#### Model Construction","metadata":{}},{"cell_type":"code","source":"tf.random.set_seed(42)\n\n# input layer\nencoder_inputs = tf.keras.layers.Input(shape=[], dtype=tf.string)\ndecoder_inputs = tf.keras.layers.Input(shape=[], dtype=tf.string)\n\n# embedding\nembed_size = 128\nencoder_input_ids = text_vec_layer_en(encoder_inputs)\ndecoder_input_ids = text_vec_layer_es(decoder_inputs)\n\nencoder_embedding_layer = tf.keras.layers.Embedding(vocab_size, embed_size,mask_zero=True)\ndecoder_embedding_layer = tf.keras.layers.Embedding(vocab_size, embed_size,\n                                                    mask_zero=True)\n\nencoder_embeddings = encoder_embedding_layer(encoder_input_ids)\ndecoder_embeddings = decoder_embedding_layer(decoder_input_ids)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-01-18T20:03:32.304965Z","iopub.execute_input":"2025-01-18T20:03:32.305265Z","iopub.status.idle":"2025-01-18T20:03:32.432453Z","shell.execute_reply.started":"2025-01-18T20:03:32.305236Z","shell.execute_reply":"2025-01-18T20:03:32.431732Z"}},"outputs":[],"execution_count":47},{"cell_type":"markdown","source":"#### Encoder Decoder","metadata":{}},{"cell_type":"code","source":"# encoder\nencoder = tf.keras.layers.LSTM(512, return_state=True)\nencoder_outputs, *encoder_state = encoder(encoder_embeddings)\n\n# decoder\ndecoder = tf.keras.layers.LSTM(512, return_sequences=True)\ndecoder_outputs = decoder(decoder_embeddings, initial_state=encoder_state)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-01-18T20:03:32.433332Z","iopub.execute_input":"2025-01-18T20:03:32.433601Z","iopub.status.idle":"2025-01-18T20:03:34.113080Z","shell.execute_reply.started":"2025-01-18T20:03:32.433579Z","shell.execute_reply":"2025-01-18T20:03:34.112158Z"}},"outputs":[],"execution_count":48},{"cell_type":"markdown","source":"<details>\n    <summary><h3>What is return state and it's uses?</h3></summary>\n\n\n### Answer:\n\n#### It is the link between the encoder and decoder.\n\n1. **Capturing Context**: The internal states of the LSTM (hidden state `h` and cell state `c`) capture the context and information from the input sequence. These states are essentially a summary of the input sequence.\n2. **Passing Information**: In the encoder-decoder architecture, the encoder processes the input sequence and generates the final states. These states are then passed to the decoder as initial states. This allows the decoder to start generating the output sequence with the context of the input sequence.\n\n### How It Works:\n- **Encoder**: When `return_state=True`, the LSTM layer returns three outputs: the output sequence, the hidden state `h`, and the cell state `c`.\n- **Decoder**: The decoder LSTM uses the hidden state `h` and cell state `c` from the encoder as its initial states. This helps the decoder generate the target sequence with the context of the input sequence.\n\n### Example:\nIn the code snippet I provided earlier, the encoder LSTM is defined as follows:\n```python\nencoder_lstm = LSTM(latent_dim, return_state=True, name='encoder_lstm')\nencoder_outputs, state_h, state_c = encoder_lstm(encoder_embedding)\nencoder_states = [state_h, state_c]\n```\nHere, `return_state=True` ensures that the LSTM layer returns the hidden state `state_h` and cell state `state_c` along with the output sequence `encoder_outputs`. These states are then passed to the decoder:\n```python\ndecoder_lstm = LSTM(latent_dim, return_sequences=True, return_state=True, name='decoder_lstm')\ndecoder_outputs, _, _ = decoder_lstm(decoder_embedding, initial_state=encoder_states)\n```\nThe `initial_state=encoder_states` parameter initializes the decoder LSTM with the states from the encoder, allowing the decoder to generate the output sequence with the context of the input sequence.\n\nI hope this clarifies the use of `return_state`! Feel free to ask more questions or dive deeper into any specific aspect.\n\n</details>","metadata":{}},{"cell_type":"code","source":"# output layer ( dense + softmax ) \noutput_layer = tf.keras.layers.Dense(vocab_size, activation=\"softmax\")\nY_proba = output_layer(decoder_outputs)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-01-18T20:03:34.114130Z","iopub.execute_input":"2025-01-18T20:03:34.114382Z","iopub.status.idle":"2025-01-18T20:03:34.140233Z","shell.execute_reply.started":"2025-01-18T20:03:34.114361Z","shell.execute_reply":"2025-01-18T20:03:34.139614Z"}},"outputs":[],"execution_count":49},{"cell_type":"code","source":"model = tf.keras.Model(inputs=[encoder_inputs, decoder_inputs],\n                       outputs=[Y_proba])\nmodel.compile(loss=\"sparse_categorical_crossentropy\", optimizer=\"nadam\",\n              metrics=[\"accuracy\"])\nhistory_enc_dec = model.fit((X_train, X_train_dec), Y_train, epochs=10,\n          validation_data=((X_valid, X_valid_dec), Y_valid))","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-01-18T20:03:34.141113Z","iopub.execute_input":"2025-01-18T20:03:34.141425Z","iopub.status.idle":"2025-01-18T20:12:11.854326Z","shell.execute_reply.started":"2025-01-18T20:03:34.141388Z","shell.execute_reply":"2025-01-18T20:12:11.853371Z"}},"outputs":[{"name":"stdout","text":"Epoch 1/10\n3125/3125 [==============================] - 70s 20ms/step - loss: 2.9041 - accuracy: 0.4292 - val_loss: 2.1485 - val_accuracy: 0.5283\nEpoch 2/10\n3125/3125 [==============================] - 51s 16ms/step - loss: 1.8211 - accuracy: 0.5805 - val_loss: 1.6337 - val_accuracy: 0.6144\nEpoch 3/10\n3125/3125 [==============================] - 49s 16ms/step - loss: 1.4121 - accuracy: 0.6542 - val_loss: 1.4096 - val_accuracy: 0.6588\nEpoch 4/10\n3125/3125 [==============================] - 49s 16ms/step - loss: 1.1840 - accuracy: 0.6991 - val_loss: 1.3243 - val_accuracy: 0.6747\nEpoch 5/10\n3125/3125 [==============================] - 50s 16ms/step - loss: 1.0224 - accuracy: 0.7313 - val_loss: 1.2800 - val_accuracy: 0.6852\nEpoch 6/10\n3125/3125 [==============================] - 50s 16ms/step - loss: 0.8906 - accuracy: 0.7596 - val_loss: 1.2759 - val_accuracy: 0.6873\nEpoch 7/10\n3125/3125 [==============================] - 50s 16ms/step - loss: 0.7781 - accuracy: 0.7848 - val_loss: 1.2922 - val_accuracy: 0.6880\nEpoch 8/10\n3125/3125 [==============================] - 49s 16ms/step - loss: 0.6791 - accuracy: 0.8082 - val_loss: 1.3222 - val_accuracy: 0.6867\nEpoch 9/10\n3125/3125 [==============================] - 50s 16ms/step - loss: 0.5936 - accuracy: 0.8290 - val_loss: 1.3605 - val_accuracy: 0.6845\nEpoch 10/10\n3125/3125 [==============================] - 49s 16ms/step - loss: 0.5207 - accuracy: 0.8476 - val_loss: 1.4045 - val_accuracy: 0.6821\n","output_type":"stream"}],"execution_count":50},{"cell_type":"markdown","source":"#### Translation","metadata":{}},{"cell_type":"code","source":"def translate(sentence_en):\n    translation = \"\"\n    for word_idx in range(max_length):\n        # Prepare encoder input\n        X = np.array([sentence_en])  # encoder input\n        print(f\"Step {word_idx+1}: Encoder input X = {X}\")\n        \n        # Prepare decoder input\n        X_dec = np.array([\"startofseq \" + translation])  # decoder input\n        print(f\"Step {word_idx+1}: Decoder input X_dec = {X_dec}\")\n        \n        # Predict the next word's probabilities\n        y_proba = model.predict((X, X_dec))[0, word_idx]  # last token's probas\n        print(f\"Step {word_idx+1}: Model Outputs = {y_proba.shape}\")\n        # Get the predicted word ID\n        predicted_word_id = np.argmax(y_proba)\n        print(f\"Step {word_idx+1}: Predicted word ID = {predicted_word_id}\")\n        \n        # Get the predicted word from the vocabulary\n        vocabulary = text_vec_layer_es.get_vocabulary()\n        if predicted_word_id >= len(vocabulary):\n            print(f\"Step {word_idx+1}: Predicted word ID {predicted_word_id} is out of vocabulary range\")\n            break  # Handle out-of-vocabulary predictions\n        predicted_word = vocabulary[predicted_word_id]\n        print(f\"Step {word_idx+1}: Predicted word = {predicted_word}\")\n        \n        # Check for end-of-sequence token\n        if predicted_word == \"endofseq\":\n            print(f\"Step {word_idx+1}: End-of-sequence token encountered\")\n            break\n        \n        # Append the predicted word to the translation\n        translation += \" \" + predicted_word\n        print(f\"Step {word_idx+1}: Updated translation = {translation}\")\n    \n    return translation.strip()","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-01-18T20:13:11.628571Z","iopub.execute_input":"2025-01-18T20:13:11.628900Z","iopub.status.idle":"2025-01-18T20:13:11.634393Z","shell.execute_reply.started":"2025-01-18T20:13:11.628870Z","shell.execute_reply":"2025-01-18T20:13:11.633671Z"}},"outputs":[],"execution_count":52},{"cell_type":"code","source":"translate(\"I like soccer\")","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-01-18T20:13:14.462505Z","iopub.execute_input":"2025-01-18T20:13:14.462808Z","iopub.status.idle":"2025-01-18T20:13:16.842349Z","shell.execute_reply.started":"2025-01-18T20:13:14.462784Z","shell.execute_reply":"2025-01-18T20:13:16.841625Z"}},"outputs":[{"name":"stdout","text":"Step 1: Encoder input X = ['I like soccer']\nStep 1: Decoder input X_dec = ['startofseq ']\n1/1 [==============================] - 2s 2s/step\nStep 1: Model Outputs = (1000,)\nStep 1: Predicted word ID = 14\nStep 1: Predicted word = me\nStep 1: Updated translation =  me\nStep 2: Encoder input X = ['I like soccer']\nStep 2: Decoder input X_dec = ['startofseq  me']\n1/1 [==============================] - 0s 19ms/step\nStep 2: Model Outputs = (1000,)\nStep 2: Predicted word ID = 61\nStep 2: Predicted word = gusta\nStep 2: Updated translation =  me gusta\nStep 3: Encoder input X = ['I like soccer']\nStep 3: Decoder input X_dec = ['startofseq  me gusta']\n1/1 [==============================] - 0s 20ms/step\nStep 3: Model Outputs = (1000,)\nStep 3: Predicted word ID = 10\nStep 3: Predicted word = el\nStep 3: Updated translation =  me gusta el\nStep 4: Encoder input X = ['I like soccer']\nStep 4: Decoder input X_dec = ['startofseq  me gusta el']\n1/1 [==============================] - 0s 19ms/step\nStep 4: Model Outputs = (1000,)\nStep 4: Predicted word ID = 663\nStep 4: Predicted word = fútbol\nStep 4: Updated translation =  me gusta el fútbol\nStep 5: Encoder input X = ['I like soccer']\nStep 5: Decoder input X_dec = ['startofseq  me gusta el fútbol']\n1/1 [==============================] - 0s 19ms/step\nStep 5: Model Outputs = (1000,)\nStep 5: Predicted word ID = 3\nStep 5: Predicted word = endofseq\nStep 5: End-of-sequence token encountered\n","output_type":"stream"},{"execution_count":53,"output_type":"execute_result","data":{"text/plain":"'me gusta el fútbol'"},"metadata":{}}],"execution_count":53},{"cell_type":"code","source":"translate(\"I like soccer and also going to the beach\")","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-01-18T20:13:21.192574Z","iopub.execute_input":"2025-01-18T20:13:21.192866Z","iopub.status.idle":"2025-01-18T20:13:21.960859Z","shell.execute_reply.started":"2025-01-18T20:13:21.192844Z","shell.execute_reply":"2025-01-18T20:13:21.959941Z"}},"outputs":[{"name":"stdout","text":"Step 1: Encoder input X = ['I like soccer and also going to the beach']\nStep 1: Decoder input X_dec = ['startofseq ']\n1/1 [==============================] - 0s 20ms/step\nStep 1: Model Outputs = (1000,)\nStep 1: Predicted word ID = 14\nStep 1: Predicted word = me\nStep 1: Updated translation =  me\nStep 2: Encoder input X = ['I like soccer and also going to the beach']\nStep 2: Decoder input X_dec = ['startofseq  me']\n1/1 [==============================] - 0s 21ms/step\nStep 2: Model Outputs = (1000,)\nStep 2: Predicted word ID = 61\nStep 2: Predicted word = gusta\nStep 2: Updated translation =  me gusta\nStep 3: Encoder input X = ['I like soccer and also going to the beach']\nStep 3: Decoder input X_dec = ['startofseq  me gusta']\n1/1 [==============================] - 0s 20ms/step\nStep 3: Model Outputs = (1000,)\nStep 3: Predicted word ID = 31\nStep 3: Predicted word = y\nStep 3: Updated translation =  me gusta y\nStep 4: Encoder input X = ['I like soccer and also going to the beach']\nStep 4: Decoder input X_dec = ['startofseq  me gusta y']\n1/1 [==============================] - 0s 21ms/step\nStep 4: Model Outputs = (1000,)\nStep 4: Predicted word ID = 6\nStep 4: Predicted word = a\nStep 4: Updated translation =  me gusta y a\nStep 5: Encoder input X = ['I like soccer and also going to the beach']\nStep 5: Decoder input X_dec = ['startofseq  me gusta y a']\n1/1 [==============================] - 0s 20ms/step\nStep 5: Model Outputs = (1000,)\nStep 5: Predicted word ID = 9\nStep 5: Predicted word = la\nStep 5: Updated translation =  me gusta y a la\nStep 6: Encoder input X = ['I like soccer and also going to the beach']\nStep 6: Decoder input X_dec = ['startofseq  me gusta y a la']\n1/1 [==============================] - 0s 20ms/step\nStep 6: Model Outputs = (1000,)\nStep 6: Predicted word ID = 184\nStep 6: Predicted word = escuela\nStep 6: Updated translation =  me gusta y a la escuela\nStep 7: Encoder input X = ['I like soccer and also going to the beach']\nStep 7: Decoder input X_dec = ['startofseq  me gusta y a la escuela']\n1/1 [==============================] - 0s 19ms/step\nStep 7: Model Outputs = (1000,)\nStep 7: Predicted word ID = 17\nStep 7: Predicted word = lo\nStep 7: Updated translation =  me gusta y a la escuela lo\nStep 8: Encoder input X = ['I like soccer and also going to the beach']\nStep 8: Decoder input X_dec = ['startofseq  me gusta y a la escuela lo']\n1/1 [==============================] - 0s 21ms/step\nStep 8: Model Outputs = (1000,)\nStep 8: Predicted word ID = 5\nStep 8: Predicted word = que\nStep 8: Updated translation =  me gusta y a la escuela lo que\nStep 9: Encoder input X = ['I like soccer and also going to the beach']\nStep 9: Decoder input X_dec = ['startofseq  me gusta y a la escuela lo que']\n1/1 [==============================] - 0s 19ms/step\nStep 9: Model Outputs = (1000,)\nStep 9: Predicted word ID = 1\nStep 9: Predicted word = [UNK]\nStep 9: Updated translation =  me gusta y a la escuela lo que [UNK]\nStep 10: Encoder input X = ['I like soccer and also going to the beach']\nStep 10: Decoder input X_dec = ['startofseq  me gusta y a la escuela lo que [UNK]']\n1/1 [==============================] - 0s 19ms/step\nStep 10: Model Outputs = (1000,)\nStep 10: Predicted word ID = 11\nStep 10: Predicted word = en\nStep 10: Updated translation =  me gusta y a la escuela lo que [UNK] en\nStep 11: Encoder input X = ['I like soccer and also going to the beach']\nStep 11: Decoder input X_dec = ['startofseq  me gusta y a la escuela lo que [UNK] en']\n1/1 [==============================] - 0s 20ms/step\nStep 11: Model Outputs = (1000,)\nStep 11: Predicted word ID = 146\nStep 11: Predicted word = mí\nStep 11: Updated translation =  me gusta y a la escuela lo que [UNK] en mí\nStep 12: Encoder input X = ['I like soccer and also going to the beach']\nStep 12: Decoder input X_dec = ['startofseq  me gusta y a la escuela lo que [UNK] en mí']\n1/1 [==============================] - 0s 19ms/step\nStep 12: Model Outputs = (1000,)\nStep 12: Predicted word ID = 3\nStep 12: Predicted word = endofseq\nStep 12: End-of-sequence token encountered\n","output_type":"stream"},{"execution_count":54,"output_type":"execute_result","data":{"text/plain":"'me gusta y a la escuela lo que [UNK] en mí'"},"metadata":{}}],"execution_count":54},{"cell_type":"markdown","source":"The translation says “I like and go to school what [unk] in me”. So how can you\n improve it? One way is to increase the training set size and add more LSTM layers in\n both the encoder and the decoder. But this will only get you so far, so let’s look at\n more sophisticated techniques, starting with bidirectional recurrent layers.","metadata":{}},{"cell_type":"markdown","source":"## Bidirectional RNNs","metadata":{}},{"cell_type":"markdown","source":" At each time step, a regular recurrent layer only looks at past and present inputs\n before generating its output. In other words, it is causal, meaning it cannot look\n into the future. This type of RNN makes sense when forecasting time series, or\n in the decoder of a sequence-to-sequence (seq2seq) model. But for tasks like text\n classification, or in the encoder of a seq2seq model, it is often preferable to look\n ahead at the next words before encoding a given word.","metadata":{}},{"cell_type":"markdown","source":"To create a bidirectional recurrent layer, just wrap a regular recurrent layer in a Bidirectional layer:","metadata":{}},{"cell_type":"code","source":"encoder = tf.keras.layers.Bidirectional(\n    tf.keras.layers.LSTM(256, return_state=True))\n\n\n# we must concat the states from the encoder:\nencoder_outputs, *encoder_state = encoder(encoder_embeddings)\nencoder_state = [tf.concat(encoder_state[::2], axis=-1),  # short-term (0 & 2)\n                 tf.concat(encoder_state[1::2], axis=-1)]  # long-term (1 & 3)\n\n# same decoder and training:\ndecoder = tf.keras.layers.LSTM(512, return_sequences=True)\ndecoder_outputs = decoder(decoder_embeddings, initial_state=encoder_state)\noutput_layer = tf.keras.layers.Dense(vocab_size, activation=\"softmax\")\nY_proba = output_layer(decoder_outputs)\nmodel = tf.keras.Model(inputs=[encoder_inputs, decoder_inputs],\n                       outputs=[Y_proba])\n\nmodel.compile(loss=\"sparse_categorical_crossentropy\", optimizer=\"nadam\",\n              metrics=[\"accuracy\"])\nmodel.fit((X_train, X_train_dec), Y_train, epochs=10,\n          validation_data=((X_valid, X_valid_dec), Y_valid))","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-01-18T20:46:51.922756Z","iopub.execute_input":"2025-01-18T20:46:51.923182Z","iopub.status.idle":"2025-01-18T20:56:11.976075Z","shell.execute_reply.started":"2025-01-18T20:46:51.923151Z","shell.execute_reply":"2025-01-18T20:56:11.975380Z"}},"outputs":[{"name":"stdout","text":"Epoch 1/10\n3125/3125 [==============================] - 76s 21ms/step - loss: 2.1679 - accuracy: 0.5388 - val_loss: 1.5265 - val_accuracy: 0.6354\nEpoch 2/10\n3125/3125 [==============================] - 54s 17ms/step - loss: 1.3223 - accuracy: 0.6730 - val_loss: 1.2999 - val_accuracy: 0.6764\nEpoch 3/10\n3125/3125 [==============================] - 53s 17ms/step - loss: 1.1000 - accuracy: 0.7170 - val_loss: 1.2181 - val_accuracy: 0.6955\nEpoch 4/10\n3125/3125 [==============================] - 54s 17ms/step - loss: 0.9520 - accuracy: 0.7475 - val_loss: 1.1891 - val_accuracy: 0.7009\nEpoch 5/10\n3125/3125 [==============================] - 53s 17ms/step - loss: 0.8297 - accuracy: 0.7738 - val_loss: 1.1920 - val_accuracy: 0.7022\nEpoch 6/10\n3125/3125 [==============================] - 53s 17ms/step - loss: 0.7236 - accuracy: 0.7978 - val_loss: 1.2077 - val_accuracy: 0.7028\nEpoch 7/10\n3125/3125 [==============================] - 54s 17ms/step - loss: 0.6322 - accuracy: 0.8195 - val_loss: 1.2419 - val_accuracy: 0.7014\nEpoch 8/10\n3125/3125 [==============================] - 53s 17ms/step - loss: 0.5529 - accuracy: 0.8389 - val_loss: 1.2820 - val_accuracy: 0.7000\nEpoch 9/10\n3125/3125 [==============================] - 54s 17ms/step - loss: 0.4865 - accuracy: 0.8559 - val_loss: 1.3270 - val_accuracy: 0.6979\nEpoch 10/10\n3125/3125 [==============================] - 53s 17ms/step - loss: 0.4305 - accuracy: 0.8703 - val_loss: 1.3740 - val_accuracy: 0.6949\n","output_type":"stream"},{"execution_count":55,"output_type":"execute_result","data":{"text/plain":"<tf_keras.src.callbacks.History at 0x7e575960bcd0>"},"metadata":{}}],"execution_count":55},{"cell_type":"code","source":"translate(\"I like soccer and also going to the beach\")","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-01-18T20:57:16.989881Z","iopub.execute_input":"2025-01-18T20:57:16.990187Z","iopub.status.idle":"2025-01-18T20:57:20.705196Z","shell.execute_reply.started":"2025-01-18T20:57:16.990164Z","shell.execute_reply":"2025-01-18T20:57:20.704375Z"}},"outputs":[{"name":"stdout","text":"Step 1: Encoder input X = ['I like soccer and also going to the beach']\nStep 1: Decoder input X_dec = ['startofseq ']\n1/1 [==============================] - 3s 3s/step\nStep 1: Model Outputs = (1000,)\nStep 1: Predicted word ID = 14\nStep 1: Predicted word = me\nStep 1: Updated translation =  me\nStep 2: Encoder input X = ['I like soccer and also going to the beach']\nStep 2: Decoder input X_dec = ['startofseq  me']\n1/1 [==============================] - 0s 21ms/step\nStep 2: Model Outputs = (1000,)\nStep 2: Predicted word ID = 61\nStep 2: Predicted word = gusta\nStep 2: Updated translation =  me gusta\nStep 3: Encoder input X = ['I like soccer and also going to the beach']\nStep 3: Decoder input X_dec = ['startofseq  me gusta']\n1/1 [==============================] - 0s 20ms/step\nStep 3: Model Outputs = (1000,)\nStep 3: Predicted word ID = 52\nStep 3: Predicted word = hacer\nStep 3: Updated translation =  me gusta hacer\nStep 4: Encoder input X = ['I like soccer and also going to the beach']\nStep 4: Decoder input X_dec = ['startofseq  me gusta hacer']\n1/1 [==============================] - 0s 20ms/step\nStep 4: Model Outputs = (1000,)\nStep 4: Predicted word ID = 1\nStep 4: Predicted word = [UNK]\nStep 4: Updated translation =  me gusta hacer [UNK]\nStep 5: Encoder input X = ['I like soccer and also going to the beach']\nStep 5: Decoder input X_dec = ['startofseq  me gusta hacer [UNK]']\n1/1 [==============================] - 0s 20ms/step\nStep 5: Model Outputs = (1000,)\nStep 5: Predicted word ID = 6\nStep 5: Predicted word = a\nStep 5: Updated translation =  me gusta hacer [UNK] a\nStep 6: Encoder input X = ['I like soccer and also going to the beach']\nStep 6: Decoder input X_dec = ['startofseq  me gusta hacer [UNK] a']\n1/1 [==============================] - 0s 20ms/step\nStep 6: Model Outputs = (1000,)\nStep 6: Predicted word ID = 9\nStep 6: Predicted word = la\nStep 6: Updated translation =  me gusta hacer [UNK] a la\nStep 7: Encoder input X = ['I like soccer and also going to the beach']\nStep 7: Decoder input X_dec = ['startofseq  me gusta hacer [UNK] a la']\n1/1 [==============================] - 0s 20ms/step\nStep 7: Model Outputs = (1000,)\nStep 7: Predicted word ID = 777\nStep 7: Predicted word = playa\nStep 7: Updated translation =  me gusta hacer [UNK] a la playa\nStep 8: Encoder input X = ['I like soccer and also going to the beach']\nStep 8: Decoder input X_dec = ['startofseq  me gusta hacer [UNK] a la playa']\n1/1 [==============================] - 0s 21ms/step\nStep 8: Model Outputs = (1000,)\nStep 8: Predicted word ID = 16\nStep 8: Predicted word = por\nStep 8: Updated translation =  me gusta hacer [UNK] a la playa por\nStep 9: Encoder input X = ['I like soccer and also going to the beach']\nStep 9: Decoder input X_dec = ['startofseq  me gusta hacer [UNK] a la playa por']\n1/1 [==============================] - 0s 20ms/step\nStep 9: Model Outputs = (1000,)\nStep 9: Predicted word ID = 9\nStep 9: Predicted word = la\nStep 9: Updated translation =  me gusta hacer [UNK] a la playa por la\nStep 10: Encoder input X = ['I like soccer and also going to the beach']\nStep 10: Decoder input X_dec = ['startofseq  me gusta hacer [UNK] a la playa por la']\n1/1 [==============================] - 0s 20ms/step\nStep 10: Model Outputs = (1000,)\nStep 10: Predicted word ID = 777\nStep 10: Predicted word = playa\nStep 10: Updated translation =  me gusta hacer [UNK] a la playa por la playa\nStep 11: Encoder input X = ['I like soccer and also going to the beach']\nStep 11: Decoder input X_dec = ['startofseq  me gusta hacer [UNK] a la playa por la playa']\n1/1 [==============================] - 0s 20ms/step\nStep 11: Model Outputs = (1000,)\nStep 11: Predicted word ID = 3\nStep 11: Predicted word = endofseq\nStep 11: End-of-sequence token encountered\n","output_type":"stream"},{"execution_count":56,"output_type":"execute_result","data":{"text/plain":"'me gusta hacer [UNK] a la playa por la playa'"},"metadata":{}}],"execution_count":56},{"cell_type":"markdown","source":"## Beam Search","metadata":{}},{"cell_type":"markdown","source":"<font size=3>We boosting our encoder–decoder model’s performance without any extra training, simply by using it more wisely</font>","metadata":{}},{"cell_type":"code","source":"def beam_search(sentence_en, beam_width, verbose=False):\n    X = np.array([sentence_en])  # encoder input\n    X_dec = np.array([\"startofseq\"])  # decoder input\n    y_proba = model.predict((X, X_dec))[0, 0]  # first token's probas\n    top_k = tf.math.top_k(y_proba, k=beam_width)\n    top_translations = [  # list of best (log_proba, translation)\n        (np.log(word_proba), text_vec_layer_es.get_vocabulary()[word_id])\n        for word_proba, word_id in zip(top_k.values, top_k.indices)\n    ]\n    \n    # extra code – displays the top first words in verbose mode\n    if verbose:\n        print(\"Top first words:\", top_translations)\n\n    for idx in range(1, max_length):\n        candidates = []\n        for log_proba, translation in top_translations:\n            if translation.endswith(\"endofseq\"):\n                candidates.append((log_proba, translation))\n                continue  # translation is finished, so don't try to extend it\n            X = np.array([sentence_en])  # encoder input\n            X_dec = np.array([\"startofseq \" + translation])  # decoder input\n            y_proba = model.predict((X, X_dec))[0, idx]  # last token's proba\n            for word_id, word_proba in enumerate(y_proba):\n                word = text_vec_layer_es.get_vocabulary()[word_id]\n                candidates.append((log_proba + np.log(word_proba),\n                                   f\"{translation} {word}\"))\n        top_translations = sorted(candidates, reverse=True)[:beam_width]\n\n        # extra code – displays the top translation so far in verbose mode\n        if verbose:\n            print(\"Top translations so far:\", top_translations)\n\n        if all([tr.endswith(\"endofseq\") for _, tr in top_translations]):\n            return top_translations[0][1].replace(\"endofseq\", \"\").strip()","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-01-18T21:08:43.183187Z","iopub.execute_input":"2025-01-18T21:08:43.183585Z","iopub.status.idle":"2025-01-18T21:08:43.190804Z","shell.execute_reply.started":"2025-01-18T21:08:43.183556Z","shell.execute_reply":"2025-01-18T21:08:43.189993Z"}},"outputs":[],"execution_count":57},{"cell_type":"code","source":"sentence_en = \"I love cats and dogs\"\ntranslate(sentence_en)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-01-18T21:08:45.553749Z","iopub.execute_input":"2025-01-18T21:08:45.554067Z","iopub.status.idle":"2025-01-18T21:08:46.164575Z","shell.execute_reply.started":"2025-01-18T21:08:45.554041Z","shell.execute_reply":"2025-01-18T21:08:46.163705Z"}},"outputs":[{"name":"stdout","text":"Step 1: Encoder input X = ['I love cats and dogs']\nStep 1: Decoder input X_dec = ['startofseq ']\n1/1 [==============================] - 0s 23ms/step\nStep 1: Model Outputs = (1000,)\nStep 1: Predicted word ID = 14\nStep 1: Predicted word = me\nStep 1: Updated translation =  me\nStep 2: Encoder input X = ['I love cats and dogs']\nStep 2: Decoder input X_dec = ['startofseq  me']\n1/1 [==============================] - 0s 20ms/step\nStep 2: Model Outputs = (1000,)\nStep 2: Predicted word ID = 1\nStep 2: Predicted word = [UNK]\nStep 2: Updated translation =  me [UNK]\nStep 3: Encoder input X = ['I love cats and dogs']\nStep 3: Decoder input X_dec = ['startofseq  me [UNK]']\n1/1 [==============================] - 0s 20ms/step\nStep 3: Model Outputs = (1000,)\nStep 3: Predicted word ID = 21\nStep 3: Predicted word = los\nStep 3: Updated translation =  me [UNK] los\nStep 4: Encoder input X = ['I love cats and dogs']\nStep 4: Decoder input X_dec = ['startofseq  me [UNK] los']\n1/1 [==============================] - 0s 20ms/step\nStep 4: Model Outputs = (1000,)\nStep 4: Predicted word ID = 556\nStep 4: Predicted word = perros\nStep 4: Updated translation =  me [UNK] los perros\nStep 5: Encoder input X = ['I love cats and dogs']\nStep 5: Decoder input X_dec = ['startofseq  me [UNK] los perros']\n1/1 [==============================] - 0s 20ms/step\nStep 5: Model Outputs = (1000,)\nStep 5: Predicted word ID = 31\nStep 5: Predicted word = y\nStep 5: Updated translation =  me [UNK] los perros y\nStep 6: Encoder input X = ['I love cats and dogs']\nStep 6: Decoder input X_dec = ['startofseq  me [UNK] los perros y']\n1/1 [==============================] - 0s 21ms/step\nStep 6: Model Outputs = (1000,)\nStep 6: Predicted word ID = 7\nStep 6: Predicted word = no\nStep 6: Updated translation =  me [UNK] los perros y no\nStep 7: Encoder input X = ['I love cats and dogs']\nStep 7: Decoder input X_dec = ['startofseq  me [UNK] los perros y no']\n1/1 [==============================] - 0s 21ms/step\nStep 7: Model Outputs = (1000,)\nStep 7: Predicted word ID = 21\nStep 7: Predicted word = los\nStep 7: Updated translation =  me [UNK] los perros y no los\nStep 8: Encoder input X = ['I love cats and dogs']\nStep 8: Decoder input X_dec = ['startofseq  me [UNK] los perros y no los']\n1/1 [==============================] - 0s 21ms/step\nStep 8: Model Outputs = (1000,)\nStep 8: Predicted word ID = 654\nStep 8: Predicted word = gatos\nStep 8: Updated translation =  me [UNK] los perros y no los gatos\nStep 9: Encoder input X = ['I love cats and dogs']\nStep 9: Decoder input X_dec = ['startofseq  me [UNK] los perros y no los gatos']\n1/1 [==============================] - 0s 20ms/step\nStep 9: Model Outputs = (1000,)\nStep 9: Predicted word ID = 3\nStep 9: Predicted word = endofseq\nStep 9: End-of-sequence token encountered\n","output_type":"stream"},{"execution_count":58,"output_type":"execute_result","data":{"text/plain":"'me [UNK] los perros y no los gatos'"},"metadata":{}}],"execution_count":58},{"cell_type":"code","source":"beam_search(sentence_en, beam_width=3, verbose=True)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-01-18T21:08:46.165849Z","iopub.execute_input":"2025-01-18T21:08:46.166163Z","iopub.status.idle":"2025-01-18T21:09:30.604088Z","shell.execute_reply.started":"2025-01-18T21:08:46.166134Z","shell.execute_reply":"2025-01-18T21:09:30.603167Z"}},"outputs":[{"name":"stdout","text":"1/1 [==============================] - 0s 20ms/step\nTop first words: [(-0.07913118, 'me'), (-2.8435392, 'yo'), (-4.2571306, 'odio')]\n1/1 [==============================] - 0s 20ms/step\n1/1 [==============================] - 0s 21ms/step\n1/1 [==============================] - 0s 21ms/step\nTop translations so far: [(-0.40813845, 'me [UNK]'), (-2.2316828, 'me encanta'), (-2.4425611, 'me odio')]\n1/1 [==============================] - 0s 20ms/step\n1/1 [==============================] - 0s 22ms/step\n1/1 [==============================] - 0s 21ms/step\nTop translations so far: [(-1.0272044, 'me [UNK] los'), (-2.302462, 'me [UNK] y'), (-2.4974494, 'me [UNK] a')]\n1/1 [==============================] - 0s 21ms/step\n1/1 [==============================] - 0s 21ms/step\n1/1 [==============================] - 0s 20ms/step\nTop translations so far: [(-1.0715384, 'me [UNK] los perros'), (-2.5088043, 'me [UNK] a los'), (-2.734133, 'me [UNK] y a')]\n1/1 [==============================] - 0s 21ms/step\n1/1 [==============================] - 0s 22ms/step\n1/1 [==============================] - 0s 21ms/step\nTop translations so far: [(-1.2074786, 'me [UNK] los perros y'), (-2.5113072, 'me [UNK] a los perros'), (-2.9482303, 'me [UNK] y a los')]\n1/1 [==============================] - 0s 21ms/step\n1/1 [==============================] - 0s 22ms/step\n1/1 [==============================] - 0s 21ms/step\nTop translations so far: [(-2.0225499, 'me [UNK] los perros y no'), (-2.2251234, 'me [UNK] los perros y los'), (-2.959288, 'me [UNK] y a los perros')]\n1/1 [==============================] - 0s 20ms/step\n1/1 [==============================] - 0s 21ms/step\n1/1 [==============================] - 0s 21ms/step\nTop translations so far: [(-2.333189, 'me [UNK] los perros y no los'), (-2.415937, 'me [UNK] los perros y los gatos'), (-3.593313, 'me [UNK] y a los perros [UNK]')]\n1/1 [==============================] - 0s 20ms/step\n1/1 [==============================] - 0s 21ms/step\n1/1 [==============================] - 0s 21ms/step\nTop translations so far: [(-2.415949, 'me [UNK] los perros y los gatos endofseq'), (-3.2233038, 'me [UNK] los perros y no los gatos'), (-3.593593, 'me [UNK] y a los perros [UNK] endofseq')]\n1/1 [==============================] - 0s 21ms/step\nTop translations so far: [(-2.415949, 'me [UNK] los perros y los gatos endofseq'), (-3.2233052, 'me [UNK] los perros y no los gatos endofseq'), (-3.593593, 'me [UNK] y a los perros [UNK] endofseq')]\n","output_type":"stream"},{"execution_count":59,"output_type":"execute_result","data":{"text/plain":"'me [UNK] los perros y los gatos'"},"metadata":{}}],"execution_count":59},{"cell_type":"markdown","source":"Penalizing UNK token to discourage the model using it.","metadata":{}},{"cell_type":"code","source":"def beam_search(sentence_en, beam_width, verbose=False):\n    X = np.array([sentence_en])  # encoder input\n    X_dec = np.array([\"startofseq\"])  # decoder input\n    y_proba = model.predict((X, X_dec))[0, 0]  # first token's probas\n    top_k = tf.math.top_k(y_proba, k=beam_width)\n    top_translations = [  # list of best (log_proba, translation)\n        (np.log(word_proba), text_vec_layer_es.get_vocabulary()[word_id])\n        for word_proba, word_id in zip(top_k.values, top_k.indices)\n        if text_vec_layer_es.get_vocabulary()[word_id] != \"[UNK]\"\n    ]\n    \n    # extra code – displays the top first words in verbose mode\n    if verbose:\n        print(\"Top first words:\", top_translations)\n\n    for idx in range(1, max_length):\n        candidates = []\n        for log_proba, translation in top_translations:\n            if translation.endswith(\"endofseq\"):\n                candidates.append((log_proba, translation))\n                continue  # translation is finished, so don't try to extend it\n            X = np.array([sentence_en])  # encoder input\n            X_dec = np.array([\"startofseq \" + translation])  # decoder input\n            y_proba = model.predict((X, X_dec))[0, idx]  # last token's proba\n            for word_id, word_proba in enumerate(y_proba):\n                word = text_vec_layer_es.get_vocabulary()[word_id]\n                if word == \"[UNK]\":\n                    word_proba /= 5  # penalize [UNK] token\n                candidates.append((log_proba + np.log(word_proba),\n                                   f\"{translation} {word}\"))\n        top_translations = sorted(candidates, reverse=True)[:beam_width]\n\n        # extra code – displays the top translation so far in verbose mode\n        if verbose:\n            print(\"Top translations so far:\", top_translations)\n\n        if all([tr.endswith(\"endofseq\") for _, tr in top_translations]):\n            return top_translations[0][1].replace(\"endofseq\", \"\").strip()\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-01-18T21:19:40.510719Z","iopub.execute_input":"2025-01-18T21:19:40.511049Z","iopub.status.idle":"2025-01-18T21:19:40.518561Z","shell.execute_reply.started":"2025-01-18T21:19:40.511020Z","shell.execute_reply":"2025-01-18T21:19:40.517720Z"}},"outputs":[],"execution_count":68},{"cell_type":"code","source":"beam_search(sentence_en, beam_width=3)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-01-18T21:19:41.308882Z","iopub.execute_input":"2025-01-18T21:19:41.309165Z","iopub.status.idle":"2025-01-18T21:20:25.063295Z","shell.execute_reply.started":"2025-01-18T21:19:41.309140Z","shell.execute_reply":"2025-01-18T21:20:25.062567Z"}},"outputs":[{"name":"stdout","text":"1/1 [==============================] - 0s 21ms/step\n1/1 [==============================] - 0s 20ms/step\n1/1 [==============================] - 0s 20ms/step\n1/1 [==============================] - 0s 24ms/step\n1/1 [==============================] - 0s 20ms/step\n1/1 [==============================] - 0s 22ms/step\n1/1 [==============================] - 0s 21ms/step\n1/1 [==============================] - 0s 22ms/step\n1/1 [==============================] - 0s 21ms/step\n1/1 [==============================] - 0s 21ms/step\n1/1 [==============================] - 0s 21ms/step\n1/1 [==============================] - 0s 20ms/step\n1/1 [==============================] - 0s 20ms/step\n1/1 [==============================] - 0s 21ms/step\n1/1 [==============================] - 0s 22ms/step\n1/1 [==============================] - 0s 21ms/step\n1/1 [==============================] - 0s 21ms/step\n1/1 [==============================] - 0s 21ms/step\n1/1 [==============================] - 0s 20ms/step\n1/1 [==============================] - 0s 20ms/step\n1/1 [==============================] - 0s 21ms/step\n1/1 [==============================] - 0s 21ms/step\n1/1 [==============================] - 0s 21ms/step\n","output_type":"stream"},{"execution_count":69,"output_type":"execute_result","data":{"text/plain":"'me odio los perros y los gatos'"},"metadata":{}}],"execution_count":69},{"cell_type":"code","source":"","metadata":{"trusted":true},"outputs":[],"execution_count":null}]}