{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# CH7 Exercises"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "1. If you have trained five different models on the exact same training data, and\n",
    "they all achieve 95% precision, is there any chance that you can combine these\n",
    "models to get better results? If so, how? If not, why?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "YES! but it depends on what if these models are dependent or not.\n",
    "if not it will work, and even doing better if they are trained using different subsets of training data."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "2. What is the difference between hard and soft voting classifiers?\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "`hard voting` is to take majority predicted class over all predictors, it give equal weight to all predictors, while `soft voting` take the average probabilities over all predictors, giving a lot of weight to very confident prediction."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "3. Is it possible to speed up training of a bagging ensemble by distributing it across\n",
    "multiple servers? What about pasting ensembles, boosting ensembles, random\n",
    "forests, or stacking ensembles?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "It's Ok to do that for bagging, pasting and random forests as each estimator supposed to be independent. But not valid for boosting as it work sequentially and estimators depends on previous ones and it will not worth the effort of training across several servers.\n",
    "stacking can be distributed for each layer predictors, However, the predictors in one layer can only be trained after the predictors in the previous layer have all been trained."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "4. What is the benefit of out-of-bag evaluation?\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "out-of-bag evaluation works as validation set to validate predictors on data that doesn't used on training.. with this advantage you can use more data for training."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "5. What makes extra-trees ensembles more random than regular random forests?\n",
    "How can this extra randomness help? Are extra-trees classifiers slower or faster\n",
    "than regular random forests?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "in theory, sure it will be faster than random forests.\n",
    "rather than searching at each node for best splitter (features) thresholds, extra trees pick `random` values and split on it, extra randomness can help eliminating variance for a little high bias.\n",
    "\n",
    "for a more details here's some  <a href='https://chat.openai.com/share/5c991a3c-1af5-4d34-8dd0-1088def837d3'><font style=\"text-decoration: underline;\">Explanations</font></a>\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "6. If your AdaBoost ensemble underfits the training data, which hyperparameters\n",
    "should you tweak, and how?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "we may try to tweak (increase)learning rate hyperparameter.\n",
    "or increasing the number of estimators.\n",
    "or reducing regularization hyperparameters for base estimator."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "7. If your gradient boosting ensemble overfits the training set, should you increase\n",
    "or decrease the learning rate?\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "we can set it to low value, this technique called `shrinkage`, this scale the contribution of each tree prediction.\n",
    "or use `early stopping` to find right number of predictors."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "----"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
